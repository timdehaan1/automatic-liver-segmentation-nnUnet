
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 
2025-10-09 10:39:31.408069: do_dummy_2d_data_aug: True 
2025-10-09 10:39:31.408821: Using splits from existing split file: /home/rnga/tsdehaan/my-scratch/Data_nnUNet/nnUnet_preprocessed/Dataset001_AAA/splits_final.json 
2025-10-09 10:39:31.415822: The split file contains 5 splits. 
2025-10-09 10:39:31.415878: Desired fold for training: 3 
2025-10-09 10:39:31.415919: This split has 62 training and 15 validation cases. 
2025-10-09 10:39:49.435638: Using torch.compile... 

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 3, 'patch_size': [28, 192, 256], 'median_image_size_in_voxels': [27.0, 168.0, 256.0], 'spacing': [7.0, 1.7578125, 1.7578125], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [True], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[1, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [1, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': False} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset001_AAA', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [7.0, 1.7578125, 1.7578125], 'original_median_shape_after_transp': [27, 168, 256], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 2868.689453125, 'mean': 227.654052734375, 'median': 202.5998992919922, 'min': 16.920146942138672, 'percentile_00_5': 53.70329284667969, 'percentile_99_5': 852.1956787109375, 'std': 123.62262725830078}}} 
 
2025-10-09 10:39:52.545403: unpacking dataset... 
2025-10-09 10:40:04.390982: unpacking done... 
2025-10-09 10:40:04.430698: Unable to plot network architecture: nnUNet_compile is enabled! 
2025-10-09 10:40:04.522043:  
2025-10-09 10:40:04.522221: Epoch 0 
2025-10-09 10:40:04.522391: Current learning rate: 0.01 
2025-10-09 10:43:04.573865: train_loss -0.4906 
2025-10-09 10:43:04.574503: val_loss -0.6875 
2025-10-09 10:43:04.574617: Pseudo dice [0.7821] 
2025-10-09 10:43:04.574711: Epoch time: 180.05 s 
2025-10-09 10:43:04.574780: Yayy! New best EMA pseudo Dice: 0.7821 
2025-10-09 10:43:05.943076:  
2025-10-09 10:43:05.943277: Epoch 1 
2025-10-09 10:43:05.943387: Current learning rate: 0.00995 
2025-10-09 10:45:45.531925: train_loss -0.6869 
2025-10-09 10:45:45.532271: val_loss -0.7152 
2025-10-09 10:45:45.532396: Pseudo dice [0.7996] 
2025-10-09 10:45:45.532480: Epoch time: 159.59 s 
2025-10-09 10:45:45.532556: Yayy! New best EMA pseudo Dice: 0.7838 
2025-10-09 10:45:47.078080:  
2025-10-09 10:45:47.078250: Epoch 2 
2025-10-09 10:45:47.078387: Current learning rate: 0.00991 
2025-10-09 10:48:26.677087: train_loss -0.7105 
2025-10-09 10:48:26.677372: val_loss -0.7423 
2025-10-09 10:48:26.677444: Pseudo dice [0.8165] 
2025-10-09 10:48:26.677615: Epoch time: 159.6 s 
2025-10-09 10:48:26.677696: Yayy! New best EMA pseudo Dice: 0.7871 
2025-10-09 10:48:28.307553:  
2025-10-09 10:48:28.307767: Epoch 3 
2025-10-09 10:48:28.307875: Current learning rate: 0.00986 
2025-10-09 10:51:07.862161: train_loss -0.721 
2025-10-09 10:51:07.862448: val_loss -0.7497 
2025-10-09 10:51:07.862531: Pseudo dice [0.8218] 
2025-10-09 10:51:07.862615: Epoch time: 159.56 s 
2025-10-09 10:51:07.862688: Yayy! New best EMA pseudo Dice: 0.7906 
2025-10-09 10:51:09.548696:  
2025-10-09 10:51:09.548888: Epoch 4 
2025-10-09 10:51:09.549014: Current learning rate: 0.00982 
2025-10-09 10:53:49.271730: train_loss -0.7359 
2025-10-09 10:53:49.274310: val_loss -0.7556 
2025-10-09 10:53:49.274381: Pseudo dice [0.8219] 
2025-10-09 10:53:49.274485: Epoch time: 159.72 s 
2025-10-09 10:53:49.274562: Yayy! New best EMA pseudo Dice: 0.7937 
2025-10-09 10:53:50.865176:  
2025-10-09 10:53:50.865409: Epoch 5 
2025-10-09 10:53:50.865519: Current learning rate: 0.00977 
2025-10-09 10:56:30.846390: train_loss -0.7366 
2025-10-09 10:56:30.846907: val_loss -0.7611 
2025-10-09 10:56:30.846984: Pseudo dice [0.8278] 
2025-10-09 10:56:30.847062: Epoch time: 159.98 s 
2025-10-09 10:56:30.847121: Yayy! New best EMA pseudo Dice: 0.7971 
2025-10-09 10:56:32.527213:  
2025-10-09 10:56:32.527404: Epoch 6 
2025-10-09 10:56:32.527517: Current learning rate: 0.00973 
2025-10-09 10:59:12.322604: train_loss -0.7477 
2025-10-09 10:59:12.322916: val_loss -0.7655 
2025-10-09 10:59:12.322990: Pseudo dice [0.8295] 
2025-10-09 10:59:12.323070: Epoch time: 159.8 s 
2025-10-09 10:59:12.323128: Yayy! New best EMA pseudo Dice: 0.8004 
2025-10-09 10:59:14.012823:  
2025-10-09 10:59:14.013032: Epoch 7 
2025-10-09 10:59:14.013141: Current learning rate: 0.00968 
2025-10-09 11:01:53.495133: train_loss -0.754 
2025-10-09 11:01:53.495424: val_loss -0.7644 
2025-10-09 11:01:53.495497: Pseudo dice [0.8303] 
2025-10-09 11:01:53.495579: Epoch time: 159.48 s 
2025-10-09 11:01:53.495639: Yayy! New best EMA pseudo Dice: 0.8033 
2025-10-09 11:01:55.252825:  
2025-10-09 11:01:55.253031: Epoch 8 
2025-10-09 11:01:55.253155: Current learning rate: 0.00964 
2025-10-09 11:04:35.162109: train_loss -0.7622 
2025-10-09 11:04:35.162419: val_loss -0.7713 
2025-10-09 11:04:35.162494: Pseudo dice [0.8349] 
2025-10-09 11:04:35.162577: Epoch time: 159.91 s 
2025-10-09 11:04:35.162637: Yayy! New best EMA pseudo Dice: 0.8065 
2025-10-09 11:04:37.086811:  
2025-10-09 11:04:37.087013: Epoch 9 
2025-10-09 11:04:37.087123: Current learning rate: 0.00959 
2025-10-09 11:07:17.311976: train_loss -0.7639 
2025-10-09 11:07:17.312270: val_loss -0.771 
2025-10-09 11:07:17.312361: Pseudo dice [0.8317] 
2025-10-09 11:07:17.312443: Epoch time: 160.23 s 
2025-10-09 11:07:17.312505: Yayy! New best EMA pseudo Dice: 0.809 
2025-10-09 11:07:19.596614:  
2025-10-09 11:07:19.596836: Epoch 10 
2025-10-09 11:07:19.596959: Current learning rate: 0.00955 
2025-10-09 11:09:59.422914: train_loss -0.7596 
2025-10-09 11:09:59.423249: val_loss -0.7732 
2025-10-09 11:09:59.423348: Pseudo dice [0.8341] 
2025-10-09 11:09:59.423435: Epoch time: 159.83 s 
2025-10-09 11:09:59.423494: Yayy! New best EMA pseudo Dice: 0.8115 
2025-10-09 11:10:01.284532:  
2025-10-09 11:10:01.284735: Epoch 11 
2025-10-09 11:10:01.284842: Current learning rate: 0.0095 
2025-10-09 11:12:41.157699: train_loss -0.7698 
2025-10-09 11:12:41.158021: val_loss -0.7779 
2025-10-09 11:12:41.158096: Pseudo dice [0.8381] 
2025-10-09 11:12:41.158171: Epoch time: 159.87 s 
2025-10-09 11:12:41.158231: Yayy! New best EMA pseudo Dice: 0.8142 
2025-10-09 11:12:42.900573:  
2025-10-09 11:12:42.900752: Epoch 12 
2025-10-09 11:12:42.900856: Current learning rate: 0.00946 
2025-10-09 11:15:22.892642: train_loss -0.779 
2025-10-09 11:15:22.892963: val_loss -0.779 
2025-10-09 11:15:22.893036: Pseudo dice [0.8381] 
2025-10-09 11:15:22.893119: Epoch time: 159.99 s 
2025-10-09 11:15:22.893178: Yayy! New best EMA pseudo Dice: 0.8166 
2025-10-09 11:15:24.601621:  
2025-10-09 11:15:24.601812: Epoch 13 
2025-10-09 11:15:24.601918: Current learning rate: 0.00941 
2025-10-09 11:18:04.465942: train_loss -0.7731 
2025-10-09 11:18:04.466273: val_loss -0.7735 
2025-10-09 11:18:04.466366: Pseudo dice [0.8357] 
2025-10-09 11:18:04.466462: Epoch time: 159.87 s 
2025-10-09 11:18:04.466525: Yayy! New best EMA pseudo Dice: 0.8185 
2025-10-09 11:18:06.361954:  
2025-10-09 11:18:06.362190: Epoch 14 
2025-10-09 11:18:06.362323: Current learning rate: 0.00937 
2025-10-09 11:20:46.518794: train_loss -0.78 
2025-10-09 11:20:46.519121: val_loss -0.7787 
2025-10-09 11:20:46.519196: Pseudo dice [0.838] 
2025-10-09 11:20:46.519276: Epoch time: 160.16 s 
2025-10-09 11:20:46.519333: Yayy! New best EMA pseudo Dice: 0.8204 
2025-10-09 11:20:48.376071:  
2025-10-09 11:20:48.376245: Epoch 15 
2025-10-09 11:20:48.376354: Current learning rate: 0.00932 
2025-10-09 11:23:28.559049: train_loss -0.771 
2025-10-09 11:23:28.559384: val_loss -0.7715 
2025-10-09 11:23:28.559459: Pseudo dice [0.8299] 
2025-10-09 11:23:28.559538: Epoch time: 160.18 s 
2025-10-09 11:23:28.559599: Yayy! New best EMA pseudo Dice: 0.8214 
2025-10-09 11:23:30.312977:  
2025-10-09 11:23:30.313174: Epoch 16 
2025-10-09 11:23:30.313284: Current learning rate: 0.00928 
2025-10-09 11:26:10.777392: train_loss -0.7816 
2025-10-09 11:26:10.777718: val_loss -0.789 
2025-10-09 11:26:10.777840: Pseudo dice [0.8443] 
2025-10-09 11:26:10.777925: Epoch time: 160.47 s 
2025-10-09 11:26:10.778004: Yayy! New best EMA pseudo Dice: 0.8237 
2025-10-09 11:26:12.659250:  
2025-10-09 11:26:12.659462: Epoch 17 
2025-10-09 11:26:12.659571: Current learning rate: 0.00923 
2025-10-09 11:28:52.750988: train_loss -0.7867 
2025-10-09 11:28:52.751307: val_loss -0.7786 
2025-10-09 11:28:52.751386: Pseudo dice [0.8381] 
2025-10-09 11:28:52.751465: Epoch time: 160.09 s 
2025-10-09 11:28:52.751524: Yayy! New best EMA pseudo Dice: 0.8251 
2025-10-09 11:28:54.522778:  
2025-10-09 11:28:54.522989: Epoch 18 
2025-10-09 11:28:54.523099: Current learning rate: 0.00919 
2025-10-09 11:31:34.744585: train_loss -0.7897 
2025-10-09 11:31:34.744910: val_loss -0.7797 
2025-10-09 11:31:34.744986: Pseudo dice [0.8362] 
2025-10-09 11:31:34.745085: Epoch time: 160.22 s 
2025-10-09 11:31:34.745155: Yayy! New best EMA pseudo Dice: 0.8262 
2025-10-09 11:31:36.662390:  
2025-10-09 11:31:36.662589: Epoch 19 
2025-10-09 11:31:36.662709: Current learning rate: 0.00914 
2025-10-09 11:34:16.643163: train_loss -0.7931 
2025-10-09 11:34:16.643477: val_loss -0.7782 
2025-10-09 11:34:16.643548: Pseudo dice [0.8361] 
2025-10-09 11:34:16.643628: Epoch time: 159.98 s 
2025-10-09 11:34:16.643707: Yayy! New best EMA pseudo Dice: 0.8272 
2025-10-09 11:34:18.470247:  
2025-10-09 11:34:18.470460: Epoch 20 
2025-10-09 11:34:18.470578: Current learning rate: 0.0091 
2025-10-09 11:36:58.596208: train_loss -0.7895 
2025-10-09 11:36:58.596546: val_loss -0.7755 
2025-10-09 11:36:58.596626: Pseudo dice [0.832] 
2025-10-09 11:36:58.596729: Epoch time: 160.13 s 
2025-10-09 11:36:58.596792: Yayy! New best EMA pseudo Dice: 0.8277 
2025-10-09 11:37:01.060644:  
2025-10-09 11:37:01.060932: Epoch 21 
2025-10-09 11:37:01.061142: Current learning rate: 0.00905 
2025-10-09 11:39:41.129392: train_loss -0.7947 
2025-10-09 11:39:41.129726: val_loss -0.7841 
2025-10-09 11:39:41.129802: Pseudo dice [0.8402] 
2025-10-09 11:39:41.129884: Epoch time: 160.07 s 
2025-10-09 11:39:41.129943: Yayy! New best EMA pseudo Dice: 0.829 
2025-10-09 11:39:43.027800:  
2025-10-09 11:39:43.028029: Epoch 22 
2025-10-09 11:39:43.028146: Current learning rate: 0.009 
2025-10-09 11:42:23.026839: train_loss -0.7998 
2025-10-09 11:42:23.027168: val_loss -0.7817 
2025-10-09 11:42:23.027262: Pseudo dice [0.84] 
2025-10-09 11:42:23.027401: Epoch time: 160.0 s 
2025-10-09 11:42:23.027482: Yayy! New best EMA pseudo Dice: 0.8301 
2025-10-09 11:42:24.674798:  
2025-10-09 11:42:24.675006: Epoch 23 
2025-10-09 11:42:24.675117: Current learning rate: 0.00896 
2025-10-09 11:45:04.791627: train_loss -0.7981 
2025-10-09 11:45:04.791950: val_loss -0.782 
2025-10-09 11:45:04.792026: Pseudo dice [0.8386] 
2025-10-09 11:45:04.792109: Epoch time: 160.12 s 
2025-10-09 11:45:04.792168: Yayy! New best EMA pseudo Dice: 0.8309 
2025-10-09 11:45:06.609870:  
2025-10-09 11:45:06.610109: Epoch 24 
2025-10-09 11:45:06.610227: Current learning rate: 0.00891 
2025-10-09 11:47:47.040228: train_loss -0.8022 
2025-10-09 11:47:47.040534: val_loss -0.7703 
2025-10-09 11:47:47.040611: Pseudo dice [0.8321] 
2025-10-09 11:47:47.040707: Epoch time: 160.43 s 
2025-10-09 11:47:47.040771: Yayy! New best EMA pseudo Dice: 0.831 
2025-10-09 11:47:48.900371:  
2025-10-09 11:47:48.900571: Epoch 25 
2025-10-09 11:47:48.900686: Current learning rate: 0.00887 
2025-10-09 11:50:29.054946: train_loss -0.7978 
2025-10-09 11:50:29.055268: val_loss -0.7884 
2025-10-09 11:50:29.055343: Pseudo dice [0.8446] 
2025-10-09 11:50:29.055424: Epoch time: 160.16 s 
2025-10-09 11:50:29.055483: Yayy! New best EMA pseudo Dice: 0.8324 
2025-10-09 11:50:30.815247:  
2025-10-09 11:50:30.815485: Epoch 26 
2025-10-09 11:50:30.815604: Current learning rate: 0.00882 
2025-10-09 11:53:10.961449: train_loss -0.8072 
2025-10-09 11:53:10.961767: val_loss -0.7857 
2025-10-09 11:53:10.961881: Pseudo dice [0.8428] 
2025-10-09 11:53:10.961965: Epoch time: 160.15 s 
2025-10-09 11:53:10.962025: Yayy! New best EMA pseudo Dice: 0.8334 
2025-10-09 11:53:12.819193:  
2025-10-09 11:53:12.819498: Epoch 27 
2025-10-09 11:53:12.819644: Current learning rate: 0.00878 
2025-10-09 11:55:52.856826: train_loss -0.8039 
2025-10-09 11:55:52.857148: val_loss -0.7778 
2025-10-09 11:55:52.857225: Pseudo dice [0.8356] 
2025-10-09 11:55:52.857307: Epoch time: 160.04 s 
2025-10-09 11:55:52.857368: Yayy! New best EMA pseudo Dice: 0.8336 
2025-10-09 11:55:54.717977:  
2025-10-09 11:55:54.718168: Epoch 28 
2025-10-09 11:55:54.718281: Current learning rate: 0.00873 
2025-10-09 11:58:34.649378: train_loss -0.8033 
2025-10-09 11:58:34.649707: val_loss -0.7767 
2025-10-09 11:58:34.649796: Pseudo dice [0.8344] 
2025-10-09 11:58:34.649883: Epoch time: 159.93 s 
2025-10-09 11:58:34.649957: Yayy! New best EMA pseudo Dice: 0.8337 
2025-10-09 11:58:36.342699:  
2025-10-09 11:58:36.342897: Epoch 29 
2025-10-09 11:58:36.343006: Current learning rate: 0.00868 
2025-10-09 12:01:16.677189: train_loss -0.8138 
2025-10-09 12:01:16.677506: val_loss -0.7787 
2025-10-09 12:01:16.677591: Pseudo dice [0.8365] 
2025-10-09 12:01:16.677783: Epoch time: 160.34 s 
2025-10-09 12:01:16.677859: Yayy! New best EMA pseudo Dice: 0.834 
2025-10-09 12:01:18.685931:  
2025-10-09 12:01:18.686155: Epoch 30 
2025-10-09 12:01:18.686280: Current learning rate: 0.00864 
2025-10-09 12:03:58.858747: train_loss -0.8079 
2025-10-09 12:03:58.859081: val_loss -0.7826 
2025-10-09 12:03:58.859195: Pseudo dice [0.8388] 
2025-10-09 12:03:58.859285: Epoch time: 160.17 s 
2025-10-09 12:03:58.859419: Yayy! New best EMA pseudo Dice: 0.8345 
2025-10-09 12:04:00.701115:  
2025-10-09 12:04:00.701326: Epoch 31 
2025-10-09 12:04:00.701442: Current learning rate: 0.00859 
2025-10-09 12:06:40.800817: train_loss -0.816 
2025-10-09 12:06:40.801120: val_loss -0.7816 
2025-10-09 12:06:40.801210: Pseudo dice [0.8386] 
2025-10-09 12:06:40.801321: Epoch time: 160.1 s 
2025-10-09 12:06:40.801384: Yayy! New best EMA pseudo Dice: 0.8349 
2025-10-09 12:06:43.032137:  
2025-10-09 12:06:43.032351: Epoch 32 
2025-10-09 12:06:43.032458: Current learning rate: 0.00855 
2025-10-09 12:09:23.028292: train_loss -0.8095 
2025-10-09 12:09:23.028614: val_loss -0.7828 
2025-10-09 12:09:23.028699: Pseudo dice [0.841] 
2025-10-09 12:09:23.028780: Epoch time: 160.0 s 
2025-10-09 12:09:23.028841: Yayy! New best EMA pseudo Dice: 0.8355 
2025-10-09 12:09:24.939287:  
2025-10-09 12:09:24.939503: Epoch 33 
2025-10-09 12:09:24.939625: Current learning rate: 0.0085 
2025-10-09 12:12:04.955225: train_loss -0.8072 
2025-10-09 12:12:04.955522: val_loss -0.7719 
2025-10-09 12:12:04.955596: Pseudo dice [0.8341] 
2025-10-09 12:12:04.955688: Epoch time: 160.02 s 
2025-10-09 12:12:06.304998:  
2025-10-09 12:12:06.305225: Epoch 34 
2025-10-09 12:12:06.305375: Current learning rate: 0.00846 
2025-10-09 12:14:46.453369: train_loss -0.8112 
2025-10-09 12:14:46.453726: val_loss -0.7868 
2025-10-09 12:14:46.453804: Pseudo dice [0.842] 
2025-10-09 12:14:46.453884: Epoch time: 160.15 s 
2025-10-09 12:14:46.453945: Yayy! New best EMA pseudo Dice: 0.836 
2025-10-09 12:14:48.302963:  
2025-10-09 12:14:48.303165: Epoch 35 
2025-10-09 12:14:48.303275: Current learning rate: 0.00841 
2025-10-09 12:17:28.379212: train_loss -0.8171 
2025-10-09 12:17:28.379581: val_loss -0.779 
2025-10-09 12:17:28.379685: Pseudo dice [0.8378] 
2025-10-09 12:17:28.379777: Epoch time: 160.08 s 
2025-10-09 12:17:28.379881: Yayy! New best EMA pseudo Dice: 0.8362 
2025-10-09 12:17:30.181588:  
2025-10-09 12:17:30.181786: Epoch 36 
2025-10-09 12:17:30.181897: Current learning rate: 0.00836 
2025-10-09 12:20:10.697580: train_loss -0.8194 
2025-10-09 12:20:10.697919: val_loss -0.7852 
2025-10-09 12:20:10.697998: Pseudo dice [0.8422] 
2025-10-09 12:20:10.698079: Epoch time: 160.52 s 
2025-10-09 12:20:10.698141: Yayy! New best EMA pseudo Dice: 0.8368 
2025-10-09 12:20:12.505991:  
2025-10-09 12:20:12.506173: Epoch 37 
2025-10-09 12:20:12.506277: Current learning rate: 0.00832 
2025-10-09 12:22:52.615201: train_loss -0.8209 
2025-10-09 12:22:52.615502: val_loss -0.7728 
2025-10-09 12:22:52.615574: Pseudo dice [0.8332] 
2025-10-09 12:22:52.615659: Epoch time: 160.11 s 
2025-10-09 12:22:53.994756:  
2025-10-09 12:22:53.994944: Epoch 38 
2025-10-09 12:22:53.995050: Current learning rate: 0.00827 
2025-10-09 12:25:34.017179: train_loss -0.8167 
2025-10-09 12:25:34.017503: val_loss -0.7697 
2025-10-09 12:25:34.017589: Pseudo dice [0.8322] 
2025-10-09 12:25:34.017715: Epoch time: 160.02 s 
2025-10-09 12:25:35.425062:  
2025-10-09 12:25:35.425328: Epoch 39 
2025-10-09 12:25:35.425443: Current learning rate: 0.00823 
2025-10-09 12:28:15.516789: train_loss -0.817 
2025-10-09 12:28:15.517137: val_loss -0.7653 
2025-10-09 12:28:15.517214: Pseudo dice [0.8301] 
2025-10-09 12:28:15.517294: Epoch time: 160.09 s 
2025-10-09 12:28:16.868841:  
2025-10-09 12:28:16.869020: Epoch 40 
2025-10-09 12:28:16.869146: Current learning rate: 0.00818 
2025-10-09 12:30:56.991606: train_loss -0.8223 
2025-10-09 12:30:56.991931: val_loss -0.7742 
2025-10-09 12:30:56.992007: Pseudo dice [0.8366] 
2025-10-09 12:30:56.992092: Epoch time: 160.12 s 
2025-10-09 12:30:58.358866:  
2025-10-09 12:30:58.359116: Epoch 41 
2025-10-09 12:30:58.359304: Current learning rate: 0.00813 
2025-10-09 12:33:38.522295: train_loss -0.8275 
2025-10-09 12:33:38.522659: val_loss -0.7685 
2025-10-09 12:33:38.522743: Pseudo dice [0.8295] 
2025-10-09 12:33:38.522842: Epoch time: 160.16 s 
2025-10-09 12:33:39.759810:  
2025-10-09 12:33:39.760033: Epoch 42 
2025-10-09 12:33:39.760156: Current learning rate: 0.00809 
2025-10-09 12:36:19.814757: train_loss -0.8258 
2025-10-09 12:36:19.815107: val_loss -0.7774 
2025-10-09 12:36:19.815182: Pseudo dice [0.8362] 
2025-10-09 12:36:19.815264: Epoch time: 160.06 s 
2025-10-09 12:36:21.068243:  
2025-10-09 12:36:21.068468: Epoch 43 
2025-10-09 12:36:21.068635: Current learning rate: 0.00804 
2025-10-09 12:39:01.168008: train_loss -0.8279 
2025-10-09 12:39:01.168330: val_loss -0.7833 
2025-10-09 12:39:01.168434: Pseudo dice [0.84] 
2025-10-09 12:39:01.168523: Epoch time: 160.1 s 
2025-10-09 12:39:02.921448:  
2025-10-09 12:39:02.921946: Epoch 44 
2025-10-09 12:39:02.922062: Current learning rate: 0.008 
2025-10-09 12:41:43.004008: train_loss -0.8248 
2025-10-09 12:41:43.004330: val_loss -0.763 
2025-10-09 12:41:43.004453: Pseudo dice [0.8261] 
2025-10-09 12:41:43.004540: Epoch time: 160.08 s 
2025-10-09 12:41:44.309508:  
2025-10-09 12:41:44.309787: Epoch 45 
2025-10-09 12:41:44.309900: Current learning rate: 0.00795 
2025-10-09 12:44:24.357588: train_loss -0.8262 
2025-10-09 12:44:24.357920: val_loss -0.778 
2025-10-09 12:44:24.358020: Pseudo dice [0.8371] 
2025-10-09 12:44:24.358145: Epoch time: 160.05 s 
2025-10-09 12:44:25.619697:  
2025-10-09 12:44:25.619939: Epoch 46 
2025-10-09 12:44:25.620061: Current learning rate: 0.0079 
2025-10-09 12:47:05.665272: train_loss -0.8312 
2025-10-09 12:47:05.665595: val_loss -0.776 
2025-10-09 12:47:05.665708: Pseudo dice [0.835] 
2025-10-09 12:47:05.665818: Epoch time: 160.05 s 
2025-10-09 12:47:06.908873:  
2025-10-09 12:47:06.909120: Epoch 47 
2025-10-09 12:47:06.909287: Current learning rate: 0.00786 
2025-10-09 12:49:46.944916: train_loss -0.8317 
2025-10-09 12:49:46.945222: val_loss -0.7765 
2025-10-09 12:49:46.945299: Pseudo dice [0.8373] 
2025-10-09 12:49:46.945386: Epoch time: 160.04 s 
2025-10-09 12:49:48.241981:  
2025-10-09 12:49:48.242204: Epoch 48 
2025-10-09 12:49:48.242339: Current learning rate: 0.00781 
2025-10-09 12:52:28.571106: train_loss -0.8321 
2025-10-09 12:52:28.571436: val_loss -0.7743 
2025-10-09 12:52:28.571519: Pseudo dice [0.8343] 
2025-10-09 12:52:28.571622: Epoch time: 160.33 s 
2025-10-09 12:52:29.852983:  
2025-10-09 12:52:29.853182: Epoch 49 
2025-10-09 12:52:29.853292: Current learning rate: 0.00777 
2025-10-09 12:55:09.984552: train_loss -0.831 
2025-10-09 12:55:09.984900: val_loss -0.7782 
2025-10-09 12:55:09.984979: Pseudo dice [0.8364] 
2025-10-09 12:55:09.985062: Epoch time: 160.13 s 
2025-10-09 12:55:11.655864:  
2025-10-09 12:55:11.656117: Epoch 50 
2025-10-09 12:55:11.656239: Current learning rate: 0.00772 
2025-10-09 12:57:51.949131: train_loss -0.8386 
2025-10-09 12:57:51.949452: val_loss -0.7728 
2025-10-09 12:57:51.949524: Pseudo dice [0.8325] 
2025-10-09 12:57:51.949606: Epoch time: 160.29 s 
2025-10-09 12:57:53.242552:  
2025-10-09 12:57:53.242759: Epoch 51 
2025-10-09 12:57:53.242864: Current learning rate: 0.00767 
2025-10-09 13:00:33.312665: train_loss -0.8365 
2025-10-09 13:00:33.312999: val_loss -0.7763 
2025-10-09 13:00:33.313070: Pseudo dice [0.8369] 
2025-10-09 13:00:33.313150: Epoch time: 160.07 s 
2025-10-09 13:00:34.538801:  
2025-10-09 13:00:34.538985: Epoch 52 
2025-10-09 13:00:34.539086: Current learning rate: 0.00763 
2025-10-09 13:03:14.304947: train_loss -0.8397 
2025-10-09 13:03:14.305271: val_loss -0.7671 
2025-10-09 13:03:14.305356: Pseudo dice [0.8283] 
2025-10-09 13:03:14.305439: Epoch time: 159.77 s 
2025-10-09 13:03:15.592974:  
2025-10-09 13:03:15.593159: Epoch 53 
2025-10-09 13:03:15.593270: Current learning rate: 0.00758 
2025-10-09 13:05:55.568887: train_loss -0.8343 
2025-10-09 13:05:55.569309: val_loss -0.7632 
2025-10-09 13:05:55.569420: Pseudo dice [0.8288] 
2025-10-09 13:05:55.569519: Epoch time: 159.98 s 
2025-10-09 13:05:56.893845:  
2025-10-09 13:05:56.894045: Epoch 54 
2025-10-09 13:05:56.894156: Current learning rate: 0.00753 
2025-10-09 13:08:36.954371: train_loss -0.8389 
2025-10-09 13:08:36.954732: val_loss -0.7684 
2025-10-09 13:08:36.954809: Pseudo dice [0.8318] 
2025-10-09 13:08:36.954888: Epoch time: 160.06 s 
2025-10-09 13:08:38.714106:  
2025-10-09 13:08:38.714345: Epoch 55 
2025-10-09 13:08:38.714467: Current learning rate: 0.00749 
2025-10-09 13:11:18.652786: train_loss -0.8411 
2025-10-09 13:11:18.653152: val_loss -0.7718 
2025-10-09 13:11:18.653261: Pseudo dice [0.8334] 
2025-10-09 13:11:18.653356: Epoch time: 159.94 s 
2025-10-09 13:11:20.049330:  
2025-10-09 13:11:20.049539: Epoch 56 
2025-10-09 13:11:20.049720: Current learning rate: 0.00744 
2025-10-09 13:14:00.163920: train_loss -0.8413 
2025-10-09 13:14:00.164240: val_loss -0.7689 
2025-10-09 13:14:00.164317: Pseudo dice [0.8324] 
2025-10-09 13:14:00.164401: Epoch time: 160.12 s 
2025-10-09 13:14:01.521029:  
2025-10-09 13:14:01.521285: Epoch 57 
2025-10-09 13:14:01.521395: Current learning rate: 0.00739 
2025-10-09 13:16:41.603857: train_loss -0.8357 
2025-10-09 13:16:41.604197: val_loss -0.7713 
2025-10-09 13:16:41.604276: Pseudo dice [0.8315] 
2025-10-09 13:16:41.604400: Epoch time: 160.08 s 
2025-10-09 13:16:42.950470:  
2025-10-09 13:16:42.950711: Epoch 58 
2025-10-09 13:16:42.950830: Current learning rate: 0.00735 
2025-10-09 13:19:23.225223: train_loss -0.8341 
2025-10-09 13:19:23.225533: val_loss -0.7768 
2025-10-09 13:19:23.225610: Pseudo dice [0.8349] 
2025-10-09 13:19:23.225702: Epoch time: 160.28 s 
2025-10-09 13:19:24.620051:  
2025-10-09 13:19:24.620312: Epoch 59 
2025-10-09 13:19:24.620450: Current learning rate: 0.0073 
2025-10-09 13:22:04.649924: train_loss -0.838 
2025-10-09 13:22:04.650232: val_loss -0.7708 
2025-10-09 13:22:04.650305: Pseudo dice [0.8332] 
2025-10-09 13:22:04.650385: Epoch time: 160.03 s 
2025-10-09 13:22:06.013839:  
2025-10-09 13:22:06.014063: Epoch 60 
2025-10-09 13:22:06.014172: Current learning rate: 0.00725 
2025-10-09 13:24:46.134813: train_loss -0.8421 
2025-10-09 13:24:46.135123: val_loss -0.7771 
2025-10-09 13:24:46.135239: Pseudo dice [0.8335] 
2025-10-09 13:24:46.135335: Epoch time: 160.12 s 
2025-10-09 13:24:47.470871:  
2025-10-09 13:24:47.471102: Epoch 61 
2025-10-09 13:24:47.471231: Current learning rate: 0.00721 
2025-10-09 13:27:27.395465: train_loss -0.8497 
2025-10-09 13:27:27.395776: val_loss -0.7794 
2025-10-09 13:27:27.395849: Pseudo dice [0.8384] 
2025-10-09 13:27:27.395930: Epoch time: 159.93 s 
2025-10-09 13:27:28.702861:  
2025-10-09 13:27:28.703057: Epoch 62 
2025-10-09 13:27:28.703182: Current learning rate: 0.00716 
2025-10-09 13:30:08.563534: train_loss -0.8491 
2025-10-09 13:30:08.563896: val_loss -0.7687 
2025-10-09 13:30:08.563972: Pseudo dice [0.8322] 
2025-10-09 13:30:08.564085: Epoch time: 159.86 s 
2025-10-09 13:30:09.912909:  
2025-10-09 13:30:09.913099: Epoch 63 
2025-10-09 13:30:09.913210: Current learning rate: 0.00711 
2025-10-09 13:32:49.954804: train_loss -0.8489 
2025-10-09 13:32:49.955135: val_loss -0.7642 
2025-10-09 13:32:49.955211: Pseudo dice [0.8285] 
2025-10-09 13:32:49.955296: Epoch time: 160.04 s 
2025-10-09 13:32:51.322707:  
2025-10-09 13:32:51.322901: Epoch 64 
2025-10-09 13:32:51.323008: Current learning rate: 0.00707 
2025-10-09 13:35:31.271906: train_loss -0.847 
2025-10-09 13:35:31.272207: val_loss -0.7721 
2025-10-09 13:35:31.272280: Pseudo dice [0.8357] 
2025-10-09 13:35:31.272359: Epoch time: 159.95 s 
2025-10-09 13:35:32.615639:  
2025-10-09 13:35:32.615842: Epoch 65 
2025-10-09 13:35:32.615946: Current learning rate: 0.00702 
2025-10-09 13:38:12.714116: train_loss -0.8517 
2025-10-09 13:38:12.714442: val_loss -0.7766 
2025-10-09 13:38:12.714526: Pseudo dice [0.8351] 
2025-10-09 13:38:12.714636: Epoch time: 160.1 s 
2025-10-09 13:38:14.048342:  
2025-10-09 13:38:14.048532: Epoch 66 
2025-10-09 13:38:14.048645: Current learning rate: 0.00697 
2025-10-09 13:40:54.066224: train_loss -0.8519 
2025-10-09 13:40:54.066594: val_loss -0.7715 
2025-10-09 13:40:54.066686: Pseudo dice [0.8348] 
2025-10-09 13:40:54.066772: Epoch time: 160.02 s 
2025-10-09 13:40:55.875932:  
2025-10-09 13:40:55.876144: Epoch 67 
2025-10-09 13:40:55.876251: Current learning rate: 0.00693 
2025-10-09 13:43:35.998949: train_loss -0.8537 
2025-10-09 13:43:35.999268: val_loss -0.7718 
2025-10-09 13:43:35.999343: Pseudo dice [0.8343] 
2025-10-09 13:43:35.999424: Epoch time: 160.12 s 
2025-10-09 13:43:37.387858:  
2025-10-09 13:43:37.388131: Epoch 68 
2025-10-09 13:43:37.388263: Current learning rate: 0.00688 
2025-10-09 13:46:17.501434: train_loss -0.8523 
2025-10-09 13:46:17.501782: val_loss -0.7597 
2025-10-09 13:46:17.501857: Pseudo dice [0.8263] 
2025-10-09 13:46:17.501942: Epoch time: 160.11 s 
2025-10-09 13:46:18.867587:  
2025-10-09 13:46:18.867883: Epoch 69 
2025-10-09 13:46:18.868004: Current learning rate: 0.00683 
2025-10-09 13:48:58.857626: train_loss -0.8418 
2025-10-09 13:48:58.857940: val_loss -0.7694 
2025-10-09 13:48:58.858018: Pseudo dice [0.832] 
2025-10-09 13:48:58.858222: Epoch time: 159.99 s 
2025-10-09 13:49:00.258826:  
2025-10-09 13:49:00.259390: Epoch 70 
2025-10-09 13:49:00.259550: Current learning rate: 0.00679 
2025-10-09 13:51:40.393984: train_loss -0.846 
2025-10-09 13:51:40.394339: val_loss -0.7742 
2025-10-09 13:51:40.394420: Pseudo dice [0.8366] 
2025-10-09 13:51:40.394505: Epoch time: 160.14 s 
2025-10-09 13:51:41.793057:  
2025-10-09 13:51:41.793289: Epoch 71 
2025-10-09 13:51:41.793400: Current learning rate: 0.00674 
2025-10-09 13:54:21.588059: train_loss -0.8541 
2025-10-09 13:54:21.588382: val_loss -0.7736 
2025-10-09 13:54:21.588463: Pseudo dice [0.8358] 
2025-10-09 13:54:21.588547: Epoch time: 159.8 s 
2025-10-09 13:54:22.968618:  
2025-10-09 13:54:22.968857: Epoch 72 
2025-10-09 13:54:22.968980: Current learning rate: 0.00669 
2025-10-09 13:57:02.971350: train_loss -0.8582 
2025-10-09 13:57:02.971704: val_loss -0.7753 
2025-10-09 13:57:02.971786: Pseudo dice [0.8363] 
2025-10-09 13:57:02.971875: Epoch time: 160.0 s 
2025-10-09 13:57:04.384538:  
2025-10-09 13:57:04.384783: Epoch 73 
2025-10-09 13:57:04.384945: Current learning rate: 0.00665 
2025-10-09 13:59:44.302155: train_loss -0.8598 
2025-10-09 13:59:44.302470: val_loss -0.7669 
2025-10-09 13:59:44.302542: Pseudo dice [0.8297] 
2025-10-09 13:59:44.302627: Epoch time: 159.92 s 
2025-10-09 13:59:45.683147:  
2025-10-09 13:59:45.683360: Epoch 74 
2025-10-09 13:59:45.683475: Current learning rate: 0.0066 
2025-10-09 14:02:25.726276: train_loss -0.8629 
2025-10-09 14:02:25.726628: val_loss -0.774 
2025-10-09 14:02:25.726737: Pseudo dice [0.834] 
2025-10-09 14:02:25.726828: Epoch time: 160.04 s 
2025-10-09 14:02:27.099432:  
2025-10-09 14:02:27.099614: Epoch 75 
2025-10-09 14:02:27.099841: Current learning rate: 0.00655 
2025-10-09 14:05:07.191528: train_loss -0.8575 
2025-10-09 14:05:07.191850: val_loss -0.7745 
2025-10-09 14:05:07.191926: Pseudo dice [0.8355] 
2025-10-09 14:05:07.192007: Epoch time: 160.09 s 
2025-10-09 14:05:08.560721:  
2025-10-09 14:05:08.560969: Epoch 76 
2025-10-09 14:05:08.561117: Current learning rate: 0.0065 
2025-10-09 14:07:48.741953: train_loss -0.8604 
2025-10-09 14:07:48.742261: val_loss -0.7644 
2025-10-09 14:07:48.742336: Pseudo dice [0.831] 
2025-10-09 14:07:48.742418: Epoch time: 160.18 s 
2025-10-09 14:07:50.103504:  
2025-10-09 14:07:50.103698: Epoch 77 
2025-10-09 14:07:50.103851: Current learning rate: 0.00646 
2025-10-09 14:10:29.967868: train_loss -0.8588 
2025-10-09 14:10:29.968181: val_loss -0.7664 
2025-10-09 14:10:29.968267: Pseudo dice [0.8304] 
2025-10-09 14:10:29.968352: Epoch time: 159.87 s 
2025-10-09 14:10:31.775450:  
2025-10-09 14:10:31.775693: Epoch 78 
2025-10-09 14:10:31.775803: Current learning rate: 0.00641 
2025-10-09 14:13:11.872471: train_loss -0.8582 
2025-10-09 14:13:11.872804: val_loss -0.7699 
2025-10-09 14:13:11.872877: Pseudo dice [0.8324] 
2025-10-09 14:13:11.872959: Epoch time: 160.1 s 
2025-10-09 14:13:13.267195:  
2025-10-09 14:13:13.267438: Epoch 79 
2025-10-09 14:13:13.267557: Current learning rate: 0.00636 
2025-10-09 14:15:53.731965: train_loss -0.8582 
2025-10-09 14:15:53.732325: val_loss -0.768 
2025-10-09 14:15:53.732412: Pseudo dice [0.8304] 
2025-10-09 14:15:53.732495: Epoch time: 160.47 s 
2025-10-09 14:15:55.168912:  
2025-10-09 14:15:55.169152: Epoch 80 
2025-10-09 14:15:55.169269: Current learning rate: 0.00631 
2025-10-09 14:18:35.318511: train_loss -0.8647 
2025-10-09 14:18:35.318878: val_loss -0.7597 
2025-10-09 14:18:35.318957: Pseudo dice [0.8254] 
2025-10-09 14:18:35.319058: Epoch time: 160.15 s 
2025-10-09 14:18:36.698953:  
2025-10-09 14:18:36.699181: Epoch 81 
2025-10-09 14:18:36.699288: Current learning rate: 0.00627 
2025-10-09 14:21:16.797863: train_loss -0.8613 
2025-10-09 14:21:16.798177: val_loss -0.7683 
2025-10-09 14:21:16.798247: Pseudo dice [0.8331] 
2025-10-09 14:21:16.798348: Epoch time: 160.1 s 
2025-10-09 14:21:18.175944:  
2025-10-09 14:21:18.176178: Epoch 82 
2025-10-09 14:21:18.176284: Current learning rate: 0.00622 
2025-10-09 14:23:58.231012: train_loss -0.8654 
2025-10-09 14:23:58.231436: val_loss -0.7669 
2025-10-09 14:23:58.231521: Pseudo dice [0.8327] 
2025-10-09 14:23:58.231605: Epoch time: 160.06 s 
2025-10-09 14:23:59.574601:  
2025-10-09 14:23:59.574852: Epoch 83 
2025-10-09 14:23:59.574969: Current learning rate: 0.00617 
2025-10-09 14:26:39.502605: train_loss -0.8625 
2025-10-09 14:26:39.502950: val_loss -0.7763 
2025-10-09 14:26:39.503024: Pseudo dice [0.838] 
2025-10-09 14:26:39.503123: Epoch time: 159.93 s 
2025-10-09 14:26:40.813426:  
2025-10-09 14:26:40.813668: Epoch 84 
2025-10-09 14:26:40.813781: Current learning rate: 0.00612 
2025-10-09 14:29:20.770925: train_loss -0.8628 
2025-10-09 14:29:20.771236: val_loss -0.772 
2025-10-09 14:29:20.771319: Pseudo dice [0.8362] 
2025-10-09 14:29:20.771403: Epoch time: 159.96 s 
2025-10-09 14:29:22.086567:  
2025-10-09 14:29:22.086807: Epoch 85 
2025-10-09 14:29:22.086920: Current learning rate: 0.00608 
2025-10-09 14:32:01.971853: train_loss -0.8633 
2025-10-09 14:32:01.972356: val_loss -0.7625 
2025-10-09 14:32:01.972442: Pseudo dice [0.83] 
2025-10-09 14:32:01.972524: Epoch time: 159.89 s 
2025-10-09 14:32:03.275148:  
2025-10-09 14:32:03.275376: Epoch 86 
2025-10-09 14:32:03.275501: Current learning rate: 0.00603 
2025-10-09 14:34:43.185166: train_loss -0.8644 
2025-10-09 14:34:43.185472: val_loss -0.7595 
2025-10-09 14:34:43.185550: Pseudo dice [0.8263] 
2025-10-09 14:34:43.185638: Epoch time: 159.91 s 
2025-10-09 14:34:44.516428:  
2025-10-09 14:34:44.516641: Epoch 87 
2025-10-09 14:34:44.516762: Current learning rate: 0.00598 
2025-10-09 14:37:24.451204: train_loss -0.8699 
2025-10-09 14:37:24.451519: val_loss -0.7668 
2025-10-09 14:37:24.451591: Pseudo dice [0.832] 
2025-10-09 14:37:24.451715: Epoch time: 159.94 s 
2025-10-09 14:37:25.780843:  
2025-10-09 14:37:25.781069: Epoch 88 
2025-10-09 14:37:25.781177: Current learning rate: 0.00593 
2025-10-09 14:40:05.746134: train_loss -0.8638 
2025-10-09 14:40:05.746493: val_loss -0.7623 
2025-10-09 14:40:05.746576: Pseudo dice [0.8294] 
2025-10-09 14:40:05.746672: Epoch time: 159.97 s 
2025-10-09 14:40:07.514615:  
2025-10-09 14:40:07.514840: Epoch 89 
2025-10-09 14:40:07.514956: Current learning rate: 0.00589 
2025-10-09 14:42:47.557687: train_loss -0.8702 
2025-10-09 14:42:47.558016: val_loss -0.7581 
2025-10-09 14:42:47.558088: Pseudo dice [0.8251] 
2025-10-09 14:42:47.558172: Epoch time: 160.04 s 
2025-10-09 14:42:48.868223:  
2025-10-09 14:42:48.868453: Epoch 90 
2025-10-09 14:42:48.868564: Current learning rate: 0.00584 
2025-10-09 14:45:28.830983: train_loss -0.8671 
2025-10-09 14:45:28.831299: val_loss -0.7587 
2025-10-09 14:45:28.831376: Pseudo dice [0.8268] 
2025-10-09 14:45:28.831457: Epoch time: 159.96 s 
2025-10-09 14:45:30.108841:  
2025-10-09 14:45:30.109113: Epoch 91 
2025-10-09 14:45:30.109231: Current learning rate: 0.00579 
2025-10-09 14:48:09.967135: train_loss -0.8726 
2025-10-09 14:48:09.967443: val_loss -0.7625 
2025-10-09 14:48:09.967513: Pseudo dice [0.8298] 
2025-10-09 14:48:09.967596: Epoch time: 159.86 s 
2025-10-09 14:48:11.257205:  
2025-10-09 14:48:11.257476: Epoch 92 
2025-10-09 14:48:11.257591: Current learning rate: 0.00574 
2025-10-09 14:50:51.338801: train_loss -0.8711 
2025-10-09 14:50:51.339133: val_loss -0.7663 
2025-10-09 14:50:51.339268: Pseudo dice [0.8336] 
2025-10-09 14:50:51.339354: Epoch time: 160.08 s 
2025-10-09 14:50:52.618489:  
2025-10-09 14:50:52.618750: Epoch 93 
2025-10-09 14:50:52.618860: Current learning rate: 0.0057 
2025-10-09 14:53:32.764513: train_loss -0.8672 
2025-10-09 14:53:32.764859: val_loss -0.7711 
2025-10-09 14:53:32.764933: Pseudo dice [0.8362] 
2025-10-09 14:53:32.765011: Epoch time: 160.15 s 
2025-10-09 14:53:34.033177:  
2025-10-09 14:53:34.033392: Epoch 94 
2025-10-09 14:53:34.033499: Current learning rate: 0.00565 
2025-10-09 14:56:14.145496: train_loss -0.8672 
2025-10-09 14:56:14.145848: val_loss -0.7716 
2025-10-09 14:56:14.145923: Pseudo dice [0.8344] 
2025-10-09 14:56:14.146003: Epoch time: 160.11 s 
2025-10-09 14:56:15.489018:  
2025-10-09 14:56:15.489278: Epoch 95 
2025-10-09 14:56:15.489405: Current learning rate: 0.0056 
2025-10-09 14:58:55.347972: train_loss -0.8715 
2025-10-09 14:58:55.348291: val_loss -0.7652 
2025-10-09 14:58:55.348364: Pseudo dice [0.8304] 
2025-10-09 14:58:55.348444: Epoch time: 159.86 s 
2025-10-09 14:58:56.579784:  
2025-10-09 14:58:56.580234: Epoch 96 
2025-10-09 14:58:56.580349: Current learning rate: 0.00555 
2025-10-09 15:01:36.603332: train_loss -0.8728 
2025-10-09 15:01:36.603669: val_loss -0.7611 
2025-10-09 15:01:36.603752: Pseudo dice [0.8281] 
2025-10-09 15:01:36.603829: Epoch time: 160.02 s 
2025-10-09 15:01:37.900897:  
2025-10-09 15:01:37.901084: Epoch 97 
2025-10-09 15:01:37.901189: Current learning rate: 0.0055 
2025-10-09 15:04:17.721375: train_loss -0.8715 
2025-10-09 15:04:17.721694: val_loss -0.7515 
2025-10-09 15:04:17.721771: Pseudo dice [0.8248] 
2025-10-09 15:04:17.721846: Epoch time: 159.82 s 
2025-10-09 15:04:18.971497:  
2025-10-09 15:04:18.971684: Epoch 98 
2025-10-09 15:04:18.971785: Current learning rate: 0.00546 
2025-10-09 15:06:58.582234: train_loss -0.8746 
2025-10-09 15:06:58.582505: val_loss -0.7596 
2025-10-09 15:06:58.582570: Pseudo dice [0.8262] 
2025-10-09 15:06:58.582655: Epoch time: 159.61 s 
2025-10-09 15:06:59.775748:  
2025-10-09 15:06:59.775923: Epoch 99 
2025-10-09 15:06:59.776032: Current learning rate: 0.00541 
2025-10-09 15:09:39.589025: train_loss -0.8733 
2025-10-09 15:09:39.589353: val_loss -0.7683 
2025-10-09 15:09:39.589442: Pseudo dice [0.8336] 
2025-10-09 15:09:39.589528: Epoch time: 159.81 s 
2025-10-09 15:09:41.890068:  
2025-10-09 15:09:41.890329: Epoch 100 
2025-10-09 15:09:41.890440: Current learning rate: 0.00536 
2025-10-09 15:12:21.896786: train_loss -0.8695 
2025-10-09 15:12:21.897099: val_loss -0.7653 
2025-10-09 15:12:21.897169: Pseudo dice [0.8321] 
2025-10-09 15:12:21.897247: Epoch time: 160.01 s 
2025-10-09 15:12:23.182621:  
2025-10-09 15:12:23.182820: Epoch 101 
2025-10-09 15:12:23.182929: Current learning rate: 0.00531 
2025-10-09 15:15:03.262285: train_loss -0.8713 
2025-10-09 15:15:03.262641: val_loss -0.7703 
2025-10-09 15:15:03.262732: Pseudo dice [0.8338] 
2025-10-09 15:15:03.262817: Epoch time: 160.08 s 
2025-10-09 15:15:04.555670:  
2025-10-09 15:15:04.555874: Epoch 102 
2025-10-09 15:15:04.555999: Current learning rate: 0.00526 
2025-10-09 15:17:44.532141: train_loss -0.8782 
2025-10-09 15:17:44.532460: val_loss -0.7637 
2025-10-09 15:17:44.532575: Pseudo dice [0.8344] 
2025-10-09 15:17:44.532745: Epoch time: 159.98 s 
2025-10-09 15:17:45.821018:  
2025-10-09 15:17:45.821273: Epoch 103 
2025-10-09 15:17:45.821385: Current learning rate: 0.00521 
2025-10-09 15:20:25.783969: train_loss -0.8732 
2025-10-09 15:20:25.784293: val_loss -0.7637 
2025-10-09 15:20:25.784411: Pseudo dice [0.833] 
2025-10-09 15:20:25.784498: Epoch time: 159.96 s 
2025-10-09 15:20:27.122416:  
2025-10-09 15:20:27.122633: Epoch 104 
2025-10-09 15:20:27.122775: Current learning rate: 0.00517 
2025-10-09 15:23:07.189796: train_loss -0.8766 
2025-10-09 15:23:07.190133: val_loss -0.7647 
2025-10-09 15:23:07.190210: Pseudo dice [0.8307] 
2025-10-09 15:23:07.190292: Epoch time: 160.07 s 
2025-10-09 15:23:08.492314:  
2025-10-09 15:23:08.492541: Epoch 105 
2025-10-09 15:23:08.492656: Current learning rate: 0.00512 
2025-10-09 15:25:48.864476: train_loss -0.8747 
2025-10-09 15:25:48.864834: val_loss -0.7575 
2025-10-09 15:25:48.864909: Pseudo dice [0.8265] 
2025-10-09 15:25:48.864990: Epoch time: 160.37 s 
2025-10-09 15:25:50.199493:  
2025-10-09 15:25:50.199731: Epoch 106 
2025-10-09 15:25:50.199847: Current learning rate: 0.00507 
2025-10-09 15:28:30.432347: train_loss -0.8761 
2025-10-09 15:28:30.432706: val_loss -0.7688 
2025-10-09 15:28:30.432784: Pseudo dice [0.8327] 
2025-10-09 15:28:30.432868: Epoch time: 160.23 s 
2025-10-09 15:28:31.774296:  
2025-10-09 15:28:31.774504: Epoch 107 
2025-10-09 15:28:31.774615: Current learning rate: 0.00502 
2025-10-09 15:31:12.071013: train_loss -0.8781 
2025-10-09 15:31:12.071332: val_loss -0.766 
2025-10-09 15:31:12.071424: Pseudo dice [0.8322] 
2025-10-09 15:31:12.071509: Epoch time: 160.3 s 
2025-10-09 15:31:13.388632:  
2025-10-09 15:31:13.388823: Epoch 108 
2025-10-09 15:31:13.388978: Current learning rate: 0.00497 
2025-10-09 15:33:53.630961: train_loss -0.8775 
2025-10-09 15:33:53.631267: val_loss -0.7682 
2025-10-09 15:33:53.631351: Pseudo dice [0.8344] 
2025-10-09 15:33:53.631496: Epoch time: 160.24 s 
2025-10-09 15:33:54.929322:  
2025-10-09 15:33:54.929487: Epoch 109 
2025-10-09 15:33:54.929629: Current learning rate: 0.00492 
2025-10-09 15:36:34.983311: train_loss -0.8789 
2025-10-09 15:36:34.983631: val_loss -0.7664 
2025-10-09 15:36:34.983725: Pseudo dice [0.8314] 
2025-10-09 15:36:34.983832: Epoch time: 160.06 s 
2025-10-09 15:36:36.309545:  
2025-10-09 15:36:36.309790: Epoch 110 
2025-10-09 15:36:36.309907: Current learning rate: 0.00487 
2025-10-09 15:39:16.647405: train_loss -0.8812 
2025-10-09 15:39:16.647772: val_loss -0.7584 
2025-10-09 15:39:16.647846: Pseudo dice [0.8277] 
2025-10-09 15:39:16.647926: Epoch time: 160.34 s 
2025-10-09 15:39:17.953126:  
2025-10-09 15:39:17.953329: Epoch 111 
2025-10-09 15:39:17.953443: Current learning rate: 0.00483 
2025-10-09 15:41:58.349927: train_loss -0.8793 
2025-10-09 15:41:58.350266: val_loss -0.7597 
2025-10-09 15:41:58.350389: Pseudo dice [0.8306] 
2025-10-09 15:41:58.350479: Epoch time: 160.4 s 
2025-10-09 15:42:00.119946:  
2025-10-09 15:42:00.120189: Epoch 112 
2025-10-09 15:42:00.120355: Current learning rate: 0.00478 
2025-10-09 15:44:40.451316: train_loss -0.8787 
2025-10-09 15:44:40.451680: val_loss -0.7734 
2025-10-09 15:44:40.451761: Pseudo dice [0.8362] 
2025-10-09 15:44:40.451842: Epoch time: 160.33 s 
2025-10-09 15:44:41.791341:  
2025-10-09 15:44:41.791537: Epoch 113 
2025-10-09 15:44:41.791644: Current learning rate: 0.00473 
2025-10-09 15:47:22.082383: train_loss -0.8782 
2025-10-09 15:47:22.082735: val_loss -0.7566 
2025-10-09 15:47:22.082844: Pseudo dice [0.8276] 
2025-10-09 15:47:22.082930: Epoch time: 160.29 s 
2025-10-09 15:47:23.390185:  
2025-10-09 15:47:23.390395: Epoch 114 
2025-10-09 15:47:23.390507: Current learning rate: 0.00468 
2025-10-09 15:50:03.773152: train_loss -0.8853 
2025-10-09 15:50:03.773507: val_loss -0.7667 
2025-10-09 15:50:03.773582: Pseudo dice [0.8327] 
2025-10-09 15:50:03.773677: Epoch time: 160.38 s 
2025-10-09 15:50:05.064374:  
2025-10-09 15:50:05.064642: Epoch 115 
2025-10-09 15:50:05.064763: Current learning rate: 0.00463 
2025-10-09 15:52:45.220555: train_loss -0.8773 
2025-10-09 15:52:45.220873: val_loss -0.7629 
2025-10-09 15:52:45.220946: Pseudo dice [0.8314] 
2025-10-09 15:52:45.221026: Epoch time: 160.16 s 
2025-10-09 15:52:46.525519:  
2025-10-09 15:52:46.525722: Epoch 116 
2025-10-09 15:52:46.525878: Current learning rate: 0.00458 
2025-10-09 15:55:26.827756: train_loss -0.8773 
2025-10-09 15:55:26.828074: val_loss -0.7538 
2025-10-09 15:55:26.828212: Pseudo dice [0.8249] 
2025-10-09 15:55:26.828334: Epoch time: 160.3 s 
2025-10-09 15:55:28.148522:  
2025-10-09 15:55:28.148736: Epoch 117 
2025-10-09 15:55:28.148840: Current learning rate: 0.00453 
2025-10-09 15:58:08.245275: train_loss -0.879 
2025-10-09 15:58:08.245616: val_loss -0.7672 
2025-10-09 15:58:08.245699: Pseudo dice [0.8351] 
2025-10-09 15:58:08.245777: Epoch time: 160.1 s 
2025-10-09 15:58:09.583441:  
2025-10-09 15:58:09.583636: Epoch 118 
2025-10-09 15:58:09.583754: Current learning rate: 0.00448 
2025-10-09 16:00:49.817538: train_loss -0.8811 
2025-10-09 16:00:49.817878: val_loss -0.7679 
2025-10-09 16:00:49.817967: Pseudo dice [0.8359] 
2025-10-09 16:00:49.818065: Epoch time: 160.24 s 
2025-10-09 16:00:51.129877:  
2025-10-09 16:00:51.130083: Epoch 119 
2025-10-09 16:00:51.130193: Current learning rate: 0.00443 
2025-10-09 16:03:31.352552: train_loss -0.8795 
2025-10-09 16:03:31.352868: val_loss -0.7606 
2025-10-09 16:03:31.352968: Pseudo dice [0.8309] 
2025-10-09 16:03:31.353109: Epoch time: 160.22 s 
2025-10-09 16:03:32.676488:  
2025-10-09 16:03:32.676697: Epoch 120 
2025-10-09 16:03:32.676854: Current learning rate: 0.00438 
2025-10-09 16:06:12.927118: train_loss -0.8789 
2025-10-09 16:06:12.927460: val_loss -0.7575 
2025-10-09 16:06:12.927536: Pseudo dice [0.827] 
2025-10-09 16:06:12.927616: Epoch time: 160.25 s 
2025-10-09 16:06:14.232313:  
2025-10-09 16:06:14.232514: Epoch 121 
2025-10-09 16:06:14.232624: Current learning rate: 0.00433 
2025-10-09 16:08:54.520679: train_loss -0.8789 
2025-10-09 16:08:54.520986: val_loss -0.7656 
2025-10-09 16:08:54.521057: Pseudo dice [0.834] 
2025-10-09 16:08:54.521136: Epoch time: 160.29 s 
2025-10-09 16:08:55.889144:  
2025-10-09 16:08:55.889421: Epoch 122 
2025-10-09 16:08:55.889538: Current learning rate: 0.00429 
2025-10-09 16:11:36.233299: train_loss -0.8842 
2025-10-09 16:11:36.233617: val_loss -0.7648 
2025-10-09 16:11:36.233711: Pseudo dice [0.8323] 
2025-10-09 16:11:36.233840: Epoch time: 160.35 s 
2025-10-09 16:11:37.978550:  
2025-10-09 16:11:37.978844: Epoch 123 
2025-10-09 16:11:37.978989: Current learning rate: 0.00424 
2025-10-09 16:14:18.264229: train_loss -0.8826 
2025-10-09 16:14:18.264542: val_loss -0.7545 
2025-10-09 16:14:18.264621: Pseudo dice [0.826] 
2025-10-09 16:14:18.264735: Epoch time: 160.29 s 
2025-10-09 16:14:19.571304:  
2025-10-09 16:14:19.571512: Epoch 124 
2025-10-09 16:14:19.571618: Current learning rate: 0.00419 
2025-10-09 16:16:59.693502: train_loss -0.8826 
2025-10-09 16:16:59.693875: val_loss -0.7683 
2025-10-09 16:16:59.693981: Pseudo dice [0.8346] 
2025-10-09 16:16:59.694068: Epoch time: 160.12 s 
2025-10-09 16:17:00.998199:  
2025-10-09 16:17:00.998432: Epoch 125 
2025-10-09 16:17:00.998543: Current learning rate: 0.00414 
2025-10-09 16:19:41.073256: train_loss -0.8866 
2025-10-09 16:19:41.073574: val_loss -0.7576 
2025-10-09 16:19:41.073642: Pseudo dice [0.8301] 
2025-10-09 16:19:41.073739: Epoch time: 160.08 s 
2025-10-09 16:19:42.380566:  
2025-10-09 16:19:42.380809: Epoch 126 
2025-10-09 16:19:42.380914: Current learning rate: 0.00409 
2025-10-09 16:22:22.583310: train_loss -0.8841 
2025-10-09 16:22:22.583625: val_loss -0.7594 
2025-10-09 16:22:22.583713: Pseudo dice [0.8312] 
2025-10-09 16:22:22.583795: Epoch time: 160.2 s 
2025-10-09 16:22:23.890586:  
2025-10-09 16:22:23.890823: Epoch 127 
2025-10-09 16:22:23.891019: Current learning rate: 0.00404 
2025-10-09 16:25:04.161608: train_loss -0.8853 
2025-10-09 16:25:04.161929: val_loss -0.764 
2025-10-09 16:25:04.162002: Pseudo dice [0.8329] 
2025-10-09 16:25:04.162084: Epoch time: 160.27 s 
2025-10-09 16:25:05.474664:  
2025-10-09 16:25:05.474925: Epoch 128 
2025-10-09 16:25:05.475044: Current learning rate: 0.00399 
2025-10-09 16:27:45.740875: train_loss -0.8845 
2025-10-09 16:27:45.741210: val_loss -0.7556 
2025-10-09 16:27:45.741282: Pseudo dice [0.8287] 
2025-10-09 16:27:45.741468: Epoch time: 160.27 s 
2025-10-09 16:27:47.048551:  
2025-10-09 16:27:47.048784: Epoch 129 
2025-10-09 16:27:47.048893: Current learning rate: 0.00394 
2025-10-09 16:30:27.344244: train_loss -0.8853 
2025-10-09 16:30:27.344588: val_loss -0.7629 
2025-10-09 16:30:27.344669: Pseudo dice [0.8342] 
2025-10-09 16:30:27.344759: Epoch time: 160.3 s 
2025-10-09 16:30:28.675712:  
2025-10-09 16:30:28.675957: Epoch 130 
2025-10-09 16:30:28.676061: Current learning rate: 0.00389 
2025-10-09 16:33:08.807691: train_loss -0.8865 
2025-10-09 16:33:08.807994: val_loss -0.7499 
2025-10-09 16:33:08.808065: Pseudo dice [0.8247] 
2025-10-09 16:33:08.808145: Epoch time: 160.13 s 
2025-10-09 16:33:10.094785:  
2025-10-09 16:33:10.094990: Epoch 131 
2025-10-09 16:33:10.095094: Current learning rate: 0.00384 
2025-10-09 16:35:50.191738: train_loss -0.887 
2025-10-09 16:35:50.192047: val_loss -0.7534 
2025-10-09 16:35:50.192125: Pseudo dice [0.8257] 
2025-10-09 16:35:50.192247: Epoch time: 160.1 s 
2025-10-09 16:35:51.484721:  
2025-10-09 16:35:51.484908: Epoch 132 
2025-10-09 16:35:51.485018: Current learning rate: 0.00379 
2025-10-09 16:38:31.456187: train_loss -0.8884 
2025-10-09 16:38:31.456550: val_loss -0.7664 
2025-10-09 16:38:31.456697: Pseudo dice [0.8332] 
2025-10-09 16:38:31.456820: Epoch time: 159.97 s 
2025-10-09 16:38:32.729465:  
2025-10-09 16:38:32.729677: Epoch 133 
2025-10-09 16:38:32.729794: Current learning rate: 0.00374 
2025-10-09 16:41:12.703911: train_loss -0.887 
2025-10-09 16:41:12.704260: val_loss -0.7616 
2025-10-09 16:41:12.704333: Pseudo dice [0.8293] 
2025-10-09 16:41:12.704417: Epoch time: 159.98 s 
2025-10-09 16:41:13.983687:  
2025-10-09 16:41:13.983884: Epoch 134 
2025-10-09 16:41:13.983995: Current learning rate: 0.00369 
2025-10-09 16:43:53.824731: train_loss -0.891 
2025-10-09 16:43:53.825034: val_loss -0.7562 
2025-10-09 16:43:53.825115: Pseudo dice [0.8267] 
2025-10-09 16:43:53.825193: Epoch time: 159.84 s 
2025-10-09 16:43:55.591499:  
2025-10-09 16:43:55.591709: Epoch 135 
2025-10-09 16:43:55.591813: Current learning rate: 0.00364 
2025-10-09 16:46:35.424956: train_loss -0.8872 
2025-10-09 16:46:35.425304: val_loss -0.7718 
2025-10-09 16:46:35.425376: Pseudo dice [0.8372] 
2025-10-09 16:46:35.425455: Epoch time: 159.83 s 
2025-10-09 16:46:36.727358:  
2025-10-09 16:46:36.727616: Epoch 136 
2025-10-09 16:46:36.727776: Current learning rate: 0.00359 
2025-10-09 16:49:16.986495: train_loss -0.8902 
2025-10-09 16:49:16.986882: val_loss -0.7539 
2025-10-09 16:49:16.986967: Pseudo dice [0.8248] 
2025-10-09 16:49:16.987049: Epoch time: 160.26 s 
2025-10-09 16:49:18.302461:  
2025-10-09 16:49:18.302664: Epoch 137 
2025-10-09 16:49:18.302771: Current learning rate: 0.00354 
2025-10-09 16:51:58.630481: train_loss -0.8891 
2025-10-09 16:51:58.630816: val_loss -0.7583 
2025-10-09 16:51:58.630893: Pseudo dice [0.8318] 
2025-10-09 16:51:58.630972: Epoch time: 160.33 s 
2025-10-09 16:51:59.933601:  
2025-10-09 16:51:59.933853: Epoch 138 
2025-10-09 16:51:59.933959: Current learning rate: 0.00349 
2025-10-09 16:54:40.457507: train_loss -0.8891 
2025-10-09 16:54:40.457891: val_loss -0.7554 
2025-10-09 16:54:40.457964: Pseudo dice [0.8278] 
2025-10-09 16:54:40.458041: Epoch time: 160.53 s 
2025-10-09 16:54:41.830559:  
2025-10-09 16:54:41.830812: Epoch 139 
2025-10-09 16:54:41.830916: Current learning rate: 0.00343 
2025-10-09 16:57:22.468913: train_loss -0.8891 
2025-10-09 16:57:22.469278: val_loss -0.7663 
2025-10-09 16:57:22.469352: Pseudo dice [0.8343] 
2025-10-09 16:57:22.469435: Epoch time: 160.64 s 
2025-10-09 16:57:23.791772:  
2025-10-09 16:57:23.792042: Epoch 140 
2025-10-09 16:57:23.792157: Current learning rate: 0.00338 
2025-10-09 17:00:04.332379: train_loss -0.8882 
2025-10-09 17:00:04.332805: val_loss -0.7643 
2025-10-09 17:00:04.332875: Pseudo dice [0.8329] 
2025-10-09 17:00:04.332954: Epoch time: 160.54 s 
2025-10-09 17:00:05.635035:  
2025-10-09 17:00:05.635274: Epoch 141 
2025-10-09 17:00:05.635384: Current learning rate: 0.00333 
2025-10-09 17:02:46.172139: train_loss -0.8944 
2025-10-09 17:02:46.172452: val_loss -0.7525 
2025-10-09 17:02:46.172520: Pseudo dice [0.8272] 
2025-10-09 17:02:46.172598: Epoch time: 160.54 s 
2025-10-09 17:02:47.477761:  
2025-10-09 17:02:47.477983: Epoch 142 
2025-10-09 17:02:47.478086: Current learning rate: 0.00328 
2025-10-09 17:05:27.863481: train_loss -0.8942 
2025-10-09 17:05:27.863837: val_loss -0.7537 
2025-10-09 17:05:27.863945: Pseudo dice [0.8281] 
2025-10-09 17:05:27.864028: Epoch time: 160.39 s 
2025-10-09 17:05:29.175308:  
2025-10-09 17:05:29.175502: Epoch 143 
2025-10-09 17:05:29.175603: Current learning rate: 0.00323 
2025-10-09 17:08:09.130309: train_loss -0.8942 
2025-10-09 17:08:09.130612: val_loss -0.7638 
2025-10-09 17:08:09.130702: Pseudo dice [0.8322] 
2025-10-09 17:08:09.130783: Epoch time: 159.96 s 
2025-10-09 17:08:10.414941:  
2025-10-09 17:08:10.415121: Epoch 144 
2025-10-09 17:08:10.415232: Current learning rate: 0.00318 
2025-10-09 17:10:49.980192: train_loss -0.889 
2025-10-09 17:10:49.980478: val_loss -0.7664 
2025-10-09 17:10:49.980545: Pseudo dice [0.8341] 
2025-10-09 17:10:49.980622: Epoch time: 159.57 s 
2025-10-09 17:10:51.635347:  
2025-10-09 17:10:51.635494: Epoch 145 
2025-10-09 17:10:51.635606: Current learning rate: 0.00313 
2025-10-09 17:13:31.295594: train_loss -0.8927 
2025-10-09 17:13:31.295902: val_loss -0.7657 
2025-10-09 17:13:31.295971: Pseudo dice [0.835] 
2025-10-09 17:13:31.296047: Epoch time: 159.66 s 
2025-10-09 17:13:32.553661:  
2025-10-09 17:13:32.553834: Epoch 146 
2025-10-09 17:13:32.553936: Current learning rate: 0.00308 
2025-10-09 17:16:12.120723: train_loss -0.8907 
2025-10-09 17:16:12.121049: val_loss -0.7506 
2025-10-09 17:16:12.121134: Pseudo dice [0.8247] 
2025-10-09 17:16:12.121260: Epoch time: 159.57 s 
2025-10-09 17:16:13.365288:  
2025-10-09 17:16:13.365474: Epoch 147 
2025-10-09 17:16:13.365574: Current learning rate: 0.00303 
2025-10-09 17:18:53.041642: train_loss -0.8909 
2025-10-09 17:18:53.041977: val_loss -0.7516 
2025-10-09 17:18:53.042048: Pseudo dice [0.8263] 
2025-10-09 17:18:53.042127: Epoch time: 159.68 s 
2025-10-09 17:18:54.292108:  
2025-10-09 17:18:54.292310: Epoch 148 
2025-10-09 17:18:54.292412: Current learning rate: 0.00297 
2025-10-09 17:21:33.861834: train_loss -0.8938 
2025-10-09 17:21:33.862094: val_loss -0.7532 
2025-10-09 17:21:33.862164: Pseudo dice [0.8258] 
2025-10-09 17:21:33.862245: Epoch time: 159.57 s 
2025-10-09 17:21:35.103040:  
2025-10-09 17:21:35.103230: Epoch 149 
2025-10-09 17:21:35.103334: Current learning rate: 0.00292 
2025-10-09 17:24:14.731665: train_loss -0.8954 
2025-10-09 17:24:14.731937: val_loss -0.7451 
2025-10-09 17:24:14.732004: Pseudo dice [0.826] 
2025-10-09 17:24:14.732078: Epoch time: 159.63 s 
2025-10-09 17:24:16.470555:  
2025-10-09 17:24:16.470711: Epoch 150 
2025-10-09 17:24:16.470810: Current learning rate: 0.00287 
2025-10-09 17:26:56.195849: train_loss -0.8923 
2025-10-09 17:26:56.196157: val_loss -0.7519 
2025-10-09 17:26:56.196234: Pseudo dice [0.8264] 
2025-10-09 17:26:56.196310: Epoch time: 159.73 s 
2025-10-09 17:26:57.437251:  
2025-10-09 17:26:57.437478: Epoch 151 
2025-10-09 17:26:57.437627: Current learning rate: 0.00282 
2025-10-09 17:29:37.379982: train_loss -0.8956 
2025-10-09 17:29:37.380289: val_loss -0.7543 
2025-10-09 17:29:37.380361: Pseudo dice [0.8299] 
2025-10-09 17:29:37.380437: Epoch time: 159.94 s 
2025-10-09 17:29:38.621722:  
2025-10-09 17:29:38.621918: Epoch 152 
2025-10-09 17:29:38.622022: Current learning rate: 0.00277 
2025-10-09 17:32:18.406539: train_loss -0.8943 
2025-10-09 17:32:18.406860: val_loss -0.7554 
2025-10-09 17:32:18.406937: Pseudo dice [0.8285] 
2025-10-09 17:32:18.407031: Epoch time: 159.79 s 
2025-10-09 17:32:19.663584:  
2025-10-09 17:32:19.663785: Epoch 153 
2025-10-09 17:32:19.663902: Current learning rate: 0.00272 
2025-10-09 17:34:59.570249: train_loss -0.8938 
2025-10-09 17:34:59.570549: val_loss -0.7552 
2025-10-09 17:34:59.570619: Pseudo dice [0.8264] 
2025-10-09 17:34:59.570712: Epoch time: 159.91 s 
2025-10-09 17:35:00.829462:  
2025-10-09 17:35:00.829628: Epoch 154 
2025-10-09 17:35:00.829742: Current learning rate: 0.00266 
2025-10-09 17:37:41.178127: train_loss -0.897 
2025-10-09 17:37:41.178436: val_loss -0.7602 
2025-10-09 17:37:41.178504: Pseudo dice [0.8316] 
2025-10-09 17:37:41.178582: Epoch time: 160.35 s 
2025-10-09 17:37:42.490901:  
2025-10-09 17:37:42.491080: Epoch 155 
2025-10-09 17:37:42.491185: Current learning rate: 0.00261 
2025-10-09 17:40:23.332777: train_loss -0.8976 
2025-10-09 17:40:23.333081: val_loss -0.7642 
2025-10-09 17:40:23.333154: Pseudo dice [0.8325] 
2025-10-09 17:40:23.333233: Epoch time: 160.84 s 
2025-10-09 17:40:25.050014:  
2025-10-09 17:40:25.050225: Epoch 156 
2025-10-09 17:40:25.050330: Current learning rate: 0.00256 
2025-10-09 17:43:05.813568: train_loss -0.9007 
2025-10-09 17:43:05.813893: val_loss -0.7631 
2025-10-09 17:43:05.813978: Pseudo dice [0.8324] 
2025-10-09 17:43:05.814073: Epoch time: 160.76 s 
2025-10-09 17:43:07.135531:  
2025-10-09 17:43:07.135750: Epoch 157 
2025-10-09 17:43:07.135860: Current learning rate: 0.00251 
2025-10-09 17:45:47.334340: train_loss -0.8991 
2025-10-09 17:45:47.334716: val_loss -0.7528 
2025-10-09 17:45:47.334808: Pseudo dice [0.8278] 
2025-10-09 17:45:47.335001: Epoch time: 160.2 s 
2025-10-09 17:45:48.643896:  
2025-10-09 17:45:48.644098: Epoch 158 
2025-10-09 17:45:48.644198: Current learning rate: 0.00245 
2025-10-09 17:48:29.227864: train_loss -0.8967 
2025-10-09 17:48:29.228198: val_loss -0.7575 
2025-10-09 17:48:29.228266: Pseudo dice [0.8287] 
2025-10-09 17:48:29.228345: Epoch time: 160.59 s 
2025-10-09 17:48:30.528157:  
2025-10-09 17:48:30.528365: Epoch 159 
2025-10-09 17:48:30.528475: Current learning rate: 0.0024 
2025-10-09 17:51:10.735447: train_loss -0.8974 
2025-10-09 17:51:10.735751: val_loss -0.753 
2025-10-09 17:51:10.735825: Pseudo dice [0.8283] 
2025-10-09 17:51:10.735907: Epoch time: 160.21 s 
2025-10-09 17:51:12.006613:  
2025-10-09 17:51:12.006968: Epoch 160 
2025-10-09 17:51:12.007096: Current learning rate: 0.00235 
2025-10-09 17:53:52.915533: train_loss -0.8941 
2025-10-09 17:53:52.915906: val_loss -0.7428 
2025-10-09 17:53:52.915976: Pseudo dice [0.822] 
2025-10-09 17:53:52.916055: Epoch time: 160.91 s 
2025-10-09 17:53:54.252328:  
2025-10-09 17:53:54.252540: Epoch 161 
2025-10-09 17:53:54.252682: Current learning rate: 0.0023 
2025-10-09 17:56:34.816224: train_loss -0.897 
2025-10-09 17:56:34.816535: val_loss -0.7524 
2025-10-09 17:56:34.816609: Pseudo dice [0.8274] 
2025-10-09 17:56:34.816701: Epoch time: 160.57 s 
2025-10-09 17:56:36.123852:  
2025-10-09 17:56:36.124059: Epoch 162 
2025-10-09 17:56:36.124166: Current learning rate: 0.00224 
2025-10-09 17:59:16.537376: train_loss -0.8988 
2025-10-09 17:59:16.537667: val_loss -0.7587 
2025-10-09 17:59:16.537739: Pseudo dice [0.8316] 
2025-10-09 17:59:16.537826: Epoch time: 160.41 s 
2025-10-09 17:59:17.830354:  
2025-10-09 17:59:17.830582: Epoch 163 
2025-10-09 17:59:17.830746: Current learning rate: 0.00219 
2025-10-09 18:01:58.149152: train_loss -0.8975 
2025-10-09 18:01:58.149440: val_loss -0.7655 
2025-10-09 18:01:58.149507: Pseudo dice [0.8337] 
2025-10-09 18:01:58.149608: Epoch time: 160.32 s 
2025-10-09 18:01:59.468094:  
2025-10-09 18:01:59.468292: Epoch 164 
2025-10-09 18:01:59.468400: Current learning rate: 0.00214 
2025-10-09 18:04:39.875579: train_loss -0.8993 
2025-10-09 18:04:39.875923: val_loss -0.7562 
2025-10-09 18:04:39.875996: Pseudo dice [0.8282] 
2025-10-09 18:04:39.876075: Epoch time: 160.41 s 
2025-10-09 18:04:41.108845:  
2025-10-09 18:04:41.109053: Epoch 165 
2025-10-09 18:04:41.109158: Current learning rate: 0.00208 
2025-10-09 18:07:21.991442: train_loss -0.897 
2025-10-09 18:07:21.991884: val_loss -0.7495 
2025-10-09 18:07:21.991961: Pseudo dice [0.8239] 
2025-10-09 18:07:21.992040: Epoch time: 160.88 s 
2025-10-09 18:07:23.246444:  
2025-10-09 18:07:23.246674: Epoch 166 
2025-10-09 18:07:23.246808: Current learning rate: 0.00203 
2025-10-09 18:10:04.049414: train_loss -0.8956 
2025-10-09 18:10:04.049722: val_loss -0.754 
2025-10-09 18:10:04.049827: Pseudo dice [0.8284] 
2025-10-09 18:10:04.049928: Epoch time: 160.8 s 
2025-10-09 18:10:05.698379:  
2025-10-09 18:10:05.698636: Epoch 167 
2025-10-09 18:10:05.698770: Current learning rate: 0.00198 
2025-10-09 18:12:46.104375: train_loss -0.8995 
2025-10-09 18:12:46.104702: val_loss -0.7568 
2025-10-09 18:12:46.104784: Pseudo dice [0.8306] 
2025-10-09 18:12:46.104901: Epoch time: 160.41 s 
2025-10-09 18:12:47.383818:  
2025-10-09 18:12:47.384044: Epoch 168 
2025-10-09 18:12:47.384152: Current learning rate: 0.00192 
2025-10-09 18:15:27.926084: train_loss -0.9031 
2025-10-09 18:15:27.926404: val_loss -0.7439 
2025-10-09 18:15:27.926476: Pseudo dice [0.821] 
2025-10-09 18:15:27.926556: Epoch time: 160.54 s 
2025-10-09 18:15:29.166445:  
2025-10-09 18:15:29.166695: Epoch 169 
2025-10-09 18:15:29.166799: Current learning rate: 0.00187 
2025-10-09 18:18:09.748967: train_loss -0.9032 
2025-10-09 18:18:09.749439: val_loss -0.7526 
2025-10-09 18:18:09.749518: Pseudo dice [0.828] 
2025-10-09 18:18:09.749597: Epoch time: 160.58 s 
2025-10-09 18:18:11.126064:  
2025-10-09 18:18:11.126416: Epoch 170 
2025-10-09 18:18:11.126562: Current learning rate: 0.00181 
2025-10-09 18:20:51.622100: train_loss -0.9014 
2025-10-09 18:20:51.622384: val_loss -0.7499 
2025-10-09 18:20:51.622453: Pseudo dice [0.8271] 
2025-10-09 18:20:51.622531: Epoch time: 160.5 s 
2025-10-09 18:20:52.854994:  
2025-10-09 18:20:52.855223: Epoch 171 
2025-10-09 18:20:52.855323: Current learning rate: 0.00176 
2025-10-09 18:23:33.378639: train_loss -0.9042 
2025-10-09 18:23:33.379016: val_loss -0.7512 
2025-10-09 18:23:33.379116: Pseudo dice [0.8263] 
2025-10-09 18:23:33.379256: Epoch time: 160.53 s 
2025-10-09 18:23:34.640012:  
2025-10-09 18:23:34.640192: Epoch 172 
2025-10-09 18:23:34.640289: Current learning rate: 0.0017 
2025-10-09 18:26:14.769912: train_loss -0.9031 
2025-10-09 18:26:14.770245: val_loss -0.7502 
2025-10-09 18:26:14.770328: Pseudo dice [0.8264] 
2025-10-09 18:26:14.770406: Epoch time: 160.13 s 
2025-10-09 18:26:15.999412:  
2025-10-09 18:26:15.999618: Epoch 173 
2025-10-09 18:26:15.999735: Current learning rate: 0.00165 
2025-10-09 18:28:56.386676: train_loss -0.9024 
2025-10-09 18:28:56.386985: val_loss -0.7574 
2025-10-09 18:28:56.387056: Pseudo dice [0.8313] 
2025-10-09 18:28:56.421888: Epoch time: 160.39 s 
2025-10-09 18:28:57.648903:  
2025-10-09 18:28:57.649094: Epoch 174 
2025-10-09 18:28:57.649194: Current learning rate: 0.00159 
2025-10-09 18:31:38.191987: train_loss -0.9019 
2025-10-09 18:31:38.192381: val_loss -0.7609 
2025-10-09 18:31:38.192455: Pseudo dice [0.8315] 
2025-10-09 18:31:38.192538: Epoch time: 160.54 s 
2025-10-09 18:31:39.599328:  
2025-10-09 18:31:39.599740: Epoch 175 
2025-10-09 18:31:39.599881: Current learning rate: 0.00154 
2025-10-09 18:34:20.028697: train_loss -0.8992 
2025-10-09 18:34:20.029030: val_loss -0.7622 
2025-10-09 18:34:20.029104: Pseudo dice [0.8331] 
2025-10-09 18:34:20.029238: Epoch time: 160.43 s 
2025-10-09 18:34:21.263899:  
2025-10-09 18:34:21.264095: Epoch 176 
2025-10-09 18:34:21.264209: Current learning rate: 0.00148 
2025-10-09 18:37:01.944787: train_loss -0.9036 
2025-10-09 18:37:01.945098: val_loss -0.747 
2025-10-09 18:37:01.945170: Pseudo dice [0.8262] 
2025-10-09 18:37:01.945245: Epoch time: 160.68 s 
2025-10-09 18:37:03.178384:  
2025-10-09 18:37:03.178632: Epoch 177 
2025-10-09 18:37:03.178782: Current learning rate: 0.00143 
2025-10-09 18:39:43.460449: train_loss -0.9046 
2025-10-09 18:39:43.460821: val_loss -0.746 
2025-10-09 18:39:43.460892: Pseudo dice [0.8235] 
2025-10-09 18:39:43.460989: Epoch time: 160.28 s 
2025-10-09 18:39:44.732002:  
2025-10-09 18:39:44.732273: Epoch 178 
2025-10-09 18:39:44.732462: Current learning rate: 0.00137 
2025-10-09 18:42:25.234745: train_loss -0.9002 
2025-10-09 18:42:25.235060: val_loss -0.7549 
2025-10-09 18:42:25.235143: Pseudo dice [0.83] 
2025-10-09 18:42:25.235221: Epoch time: 160.5 s 
2025-10-09 18:42:26.823608:  
2025-10-09 18:42:26.823872: Epoch 179 
2025-10-09 18:42:26.824029: Current learning rate: 0.00132 
2025-10-09 18:45:07.234994: train_loss -0.9033 
2025-10-09 18:45:07.235297: val_loss -0.7539 
2025-10-09 18:45:07.235367: Pseudo dice [0.8283] 
2025-10-09 18:45:07.235452: Epoch time: 160.41 s 
2025-10-09 18:45:08.550771:  
2025-10-09 18:45:08.551110: Epoch 180 
2025-10-09 18:45:08.551237: Current learning rate: 0.00126 
2025-10-09 18:47:49.280036: train_loss -0.9064 
2025-10-09 18:47:49.280356: val_loss -0.7481 
2025-10-09 18:47:49.280440: Pseudo dice [0.8258] 
2025-10-09 18:47:49.280520: Epoch time: 160.73 s 
2025-10-09 18:47:50.505572:  
2025-10-09 18:47:50.505821: Epoch 181 
2025-10-09 18:47:50.505957: Current learning rate: 0.0012 
2025-10-09 18:50:30.922462: train_loss -0.9032 
2025-10-09 18:50:30.922758: val_loss -0.7489 
2025-10-09 18:50:30.922825: Pseudo dice [0.8264] 
2025-10-09 18:50:30.922943: Epoch time: 160.42 s 
2025-10-09 18:50:32.145037:  
2025-10-09 18:50:32.145247: Epoch 182 
2025-10-09 18:50:32.145350: Current learning rate: 0.00115 
2025-10-09 18:53:12.381692: train_loss -0.9028 
2025-10-09 18:53:12.382038: val_loss -0.7443 
2025-10-09 18:53:12.382138: Pseudo dice [0.823] 
2025-10-09 18:53:12.382230: Epoch time: 160.24 s 
2025-10-09 18:53:13.634934:  
2025-10-09 18:53:13.635144: Epoch 183 
2025-10-09 18:53:13.635245: Current learning rate: 0.00109 
2025-10-09 18:55:54.079166: train_loss -0.9056 
2025-10-09 18:55:54.079438: val_loss -0.7497 
2025-10-09 18:55:54.079510: Pseudo dice [0.8267] 
2025-10-09 18:55:54.079595: Epoch time: 160.45 s 
2025-10-09 18:55:55.327153:  
2025-10-09 18:55:55.327396: Epoch 184 
2025-10-09 18:55:55.327516: Current learning rate: 0.00103 
2025-10-09 18:58:35.936691: train_loss -0.9048 
2025-10-09 18:58:35.937076: val_loss -0.7566 
2025-10-09 18:58:35.937151: Pseudo dice [0.8309] 
2025-10-09 18:58:35.937233: Epoch time: 160.61 s 
2025-10-09 18:58:37.297019:  
2025-10-09 18:58:37.297256: Epoch 185 
2025-10-09 18:58:37.297364: Current learning rate: 0.00097 
2025-10-09 19:01:17.735225: train_loss -0.9015 
2025-10-09 19:01:17.735573: val_loss -0.7535 
2025-10-09 19:01:17.735719: Pseudo dice [0.8279] 
2025-10-09 19:01:17.735806: Epoch time: 160.44 s 
2025-10-09 19:01:18.995496:  
2025-10-09 19:01:18.995774: Epoch 186 
2025-10-09 19:01:18.995893: Current learning rate: 0.00091 
2025-10-09 19:03:59.479049: train_loss -0.9037 
2025-10-09 19:03:59.479503: val_loss -0.7442 
2025-10-09 19:03:59.479572: Pseudo dice [0.8249] 
2025-10-09 19:03:59.479661: Epoch time: 160.48 s 
2025-10-09 19:04:00.773622:  
2025-10-09 19:04:00.773840: Epoch 187 
2025-10-09 19:04:00.773946: Current learning rate: 0.00085 
2025-10-09 19:06:40.981766: train_loss -0.9039 
2025-10-09 19:06:40.982054: val_loss -0.7471 
2025-10-09 19:06:40.982124: Pseudo dice [0.8259] 
2025-10-09 19:06:40.982198: Epoch time: 160.21 s 
2025-10-09 19:06:42.229472:  
2025-10-09 19:06:42.229724: Epoch 188 
2025-10-09 19:06:42.229839: Current learning rate: 0.00079 
2025-10-09 19:09:22.583730: train_loss -0.9053 
2025-10-09 19:09:22.584040: val_loss -0.75 
2025-10-09 19:09:22.584129: Pseudo dice [0.826] 
2025-10-09 19:09:22.584208: Epoch time: 160.36 s 
2025-10-09 19:09:23.864669:  
2025-10-09 19:09:23.864851: Epoch 189 
2025-10-09 19:09:23.864987: Current learning rate: 0.00074 
2025-10-09 19:12:04.351525: train_loss -0.9054 
2025-10-09 19:12:04.351825: val_loss -0.7569 
2025-10-09 19:12:04.351896: Pseudo dice [0.8304] 
2025-10-09 19:12:04.351975: Epoch time: 160.49 s 
2025-10-09 19:12:05.600300:  
2025-10-09 19:12:05.600493: Epoch 190 
2025-10-09 19:12:05.600667: Current learning rate: 0.00067 
2025-10-09 19:14:46.138378: train_loss -0.9062 
2025-10-09 19:14:46.138749: val_loss -0.7507 
2025-10-09 19:14:46.138818: Pseudo dice [0.8276] 
2025-10-09 19:14:46.138894: Epoch time: 160.54 s 
2025-10-09 19:14:47.943349:  
2025-10-09 19:14:47.943591: Epoch 191 
2025-10-09 19:14:47.943709: Current learning rate: 0.00061 
2025-10-09 19:17:28.216172: train_loss -0.9064 
2025-10-09 19:17:28.216543: val_loss -0.7546 
2025-10-09 19:17:28.216618: Pseudo dice [0.8277] 
2025-10-09 19:17:28.216708: Epoch time: 160.27 s 
2025-10-09 19:17:29.499605:  
2025-10-09 19:17:29.499840: Epoch 192 
2025-10-09 19:17:29.499947: Current learning rate: 0.00055 
2025-10-09 19:20:09.733407: train_loss -0.9064 
2025-10-09 19:20:09.733758: val_loss -0.7585 
2025-10-09 19:20:09.733828: Pseudo dice [0.8316] 
2025-10-09 19:20:09.733907: Epoch time: 160.23 s 
2025-10-09 19:20:10.993557:  
2025-10-09 19:20:10.993799: Epoch 193 
2025-10-09 19:20:10.993907: Current learning rate: 0.00049 
2025-10-09 19:22:51.201590: train_loss -0.9061 
2025-10-09 19:22:51.201928: val_loss -0.7511 
2025-10-09 19:22:51.201999: Pseudo dice [0.8265] 
2025-10-09 19:22:51.202088: Epoch time: 160.21 s 
2025-10-09 19:22:52.458796:  
2025-10-09 19:22:52.459037: Epoch 194 
2025-10-09 19:22:52.459160: Current learning rate: 0.00043 
2025-10-09 19:25:32.482468: train_loss -0.9078 
2025-10-09 19:25:32.482775: val_loss -0.7471 
2025-10-09 19:25:32.482843: Pseudo dice [0.8258] 
2025-10-09 19:25:32.482919: Epoch time: 160.03 s 
2025-10-09 19:25:33.724489:  
2025-10-09 19:25:33.724711: Epoch 195 
2025-10-09 19:25:33.724811: Current learning rate: 0.00036 
2025-10-09 19:28:13.836045: train_loss -0.9047 
2025-10-09 19:28:13.836346: val_loss -0.7419 
2025-10-09 19:28:13.836416: Pseudo dice [0.8217] 
2025-10-09 19:28:13.836493: Epoch time: 160.11 s 
2025-10-09 19:28:15.088983:  
2025-10-09 19:28:15.089214: Epoch 196 
2025-10-09 19:28:15.089325: Current learning rate: 0.0003 
2025-10-09 19:30:55.123452: train_loss -0.9059 
2025-10-09 19:30:55.123765: val_loss -0.7486 
2025-10-09 19:30:55.123834: Pseudo dice [0.8282] 
2025-10-09 19:30:55.123910: Epoch time: 160.04 s 
2025-10-09 19:30:56.368003:  
2025-10-09 19:30:56.368219: Epoch 197 
2025-10-09 19:30:56.368326: Current learning rate: 0.00023 
2025-10-09 19:33:36.420962: train_loss -0.908 
2025-10-09 19:33:36.421251: val_loss -0.7521 
2025-10-09 19:33:36.421324: Pseudo dice [0.8275] 
2025-10-09 19:33:36.421413: Epoch time: 160.05 s 
2025-10-09 19:33:37.660512:  
2025-10-09 19:33:37.660724: Epoch 198 
2025-10-09 19:33:37.660829: Current learning rate: 0.00016 
2025-10-09 19:36:17.623558: train_loss -0.9054 
2025-10-09 19:36:17.623875: val_loss -0.7479 
2025-10-09 19:36:17.623943: Pseudo dice [0.8252] 
2025-10-09 19:36:17.624019: Epoch time: 159.96 s 
2025-10-09 19:36:18.865469:  
2025-10-09 19:36:18.865658: Epoch 199 
2025-10-09 19:36:18.865769: Current learning rate: 8e-05 
2025-10-09 19:38:58.832006: train_loss -0.9069 
2025-10-09 19:38:58.832267: val_loss -0.7582 
2025-10-09 19:38:58.832334: Pseudo dice [0.8323] 
2025-10-09 19:38:58.832683: Epoch time: 159.97 s 
2025-10-09 19:39:00.463236: Training done. 
2025-10-09 19:39:00.971992: Using splits from existing split file: /home/rnga/tsdehaan/my-scratch/Data_nnUNet/nnUnet_preprocessed/Dataset001_AAA/splits_final.json 
2025-10-09 19:39:00.984012: The split file contains 5 splits. 
2025-10-09 19:39:00.984161: Desired fold for training: 3 
2025-10-09 19:39:00.984206: This split has 62 training and 15 validation cases. 
2025-10-09 19:39:00.984465: predicting IVIM_001 
2025-10-09 19:39:00.999828: IVIM_001, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-09 19:39:09.433326: predicting IVIM_010 
2025-10-09 19:39:09.434676: IVIM_010, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-09 19:39:09.989007: predicting IVIM_015 
2025-10-09 19:39:09.990353: IVIM_015, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-09 19:39:10.501747: predicting IVIM_040 
2025-10-09 19:39:10.506055: IVIM_040, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-09 19:39:11.004524: predicting IVIM_054 
2025-10-09 19:39:11.005727: IVIM_054, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-09 19:39:11.514813: predicting IVIM_064 
2025-10-09 19:39:11.516199: IVIM_064, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-09 19:39:15.258697: predicting IVIM_074 
2025-10-09 19:39:15.260017: IVIM_074, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-09 19:39:15.758738: predicting IVIM_077 
2025-10-09 19:39:15.760079: IVIM_077, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-09 19:39:16.260532: predicting IVIM_119 
2025-10-09 19:39:16.262051: IVIM_119, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-09 19:39:16.762568: predicting IVIM_134 
2025-10-09 19:39:16.764067: IVIM_134, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-09 19:39:17.261385: predicting IVIM_135 
2025-10-09 19:39:17.262864: IVIM_135, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-09 19:39:17.762500: predicting IVIM_143 
2025-10-09 19:39:17.763910: IVIM_143, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-09 19:39:18.264089: predicting IVIM_150 
2025-10-09 19:39:18.265379: IVIM_150, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-09 19:39:18.765340: predicting IVIM_156 
2025-10-09 19:39:18.766751: IVIM_156, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-09 19:39:19.267196: predicting IVIM_159 
2025-10-09 19:39:19.268581: IVIM_159, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-09 19:39:30.921779: Validation complete 
2025-10-09 19:39:30.921897: Mean Validation Dice:  0.8339248009567041 
