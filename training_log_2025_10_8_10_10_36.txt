
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 
2025-10-08 10:10:36.820372: do_dummy_2d_data_aug: True 
2025-10-08 10:10:36.821299: Using splits from existing split file: /home/rnga/tsdehaan/my-scratch/Data_nnUNet/nnUnet_preprocessed/Dataset001_AAA/splits_final.json 
2025-10-08 10:10:36.825390: The split file contains 5 splits. 
2025-10-08 10:10:36.825448: Desired fold for training: 1 
2025-10-08 10:10:36.825490: This split has 61 training and 16 validation cases. 
2025-10-08 10:10:44.498268: Using torch.compile... 

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 3, 'patch_size': [28, 192, 256], 'median_image_size_in_voxels': [27.0, 168.0, 256.0], 'spacing': [7.0, 1.7578125, 1.7578125], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [True], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[1, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [1, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': False} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset001_AAA', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [7.0, 1.7578125, 1.7578125], 'original_median_shape_after_transp': [27, 168, 256], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 2868.689453125, 'mean': 227.654052734375, 'median': 202.5998992919922, 'min': 16.920146942138672, 'percentile_00_5': 53.70329284667969, 'percentile_99_5': 852.1956787109375, 'std': 123.62262725830078}}} 
 
2025-10-08 10:10:52.490895: unpacking dataset... 
2025-10-08 10:11:03.609445: unpacking done... 
2025-10-08 10:11:03.611857: Unable to plot network architecture: nnUNet_compile is enabled! 
2025-10-08 10:11:03.676634:  
2025-10-08 10:11:03.676794: Epoch 0 
2025-10-08 10:11:03.676997: Current learning rate: 0.01 
2025-10-08 10:14:06.601450: train_loss -0.52 
2025-10-08 10:14:06.601696: val_loss -0.6502 
2025-10-08 10:14:06.601776: Pseudo dice [0.7714] 
2025-10-08 10:14:06.601904: Epoch time: 182.93 s 
2025-10-08 10:14:06.601985: Yayy! New best EMA pseudo Dice: 0.7714 
2025-10-08 10:14:08.009558:  
2025-10-08 10:14:08.009737: Epoch 1 
2025-10-08 10:14:08.009822: Current learning rate: 0.00995 
2025-10-08 10:16:48.312738: train_loss -0.6857 
2025-10-08 10:16:48.313050: val_loss -0.6903 
2025-10-08 10:16:48.313124: Pseudo dice [0.796] 
2025-10-08 10:16:48.313205: Epoch time: 160.3 s 
2025-10-08 10:16:48.313282: Yayy! New best EMA pseudo Dice: 0.7739 
2025-10-08 10:16:50.028238:  
2025-10-08 10:16:50.028400: Epoch 2 
2025-10-08 10:16:50.028512: Current learning rate: 0.00991 
2025-10-08 10:19:30.138296: train_loss -0.7199 
2025-10-08 10:19:30.138586: val_loss -0.7119 
2025-10-08 10:19:30.138668: Pseudo dice [0.8082] 
2025-10-08 10:19:30.138762: Epoch time: 160.11 s 
2025-10-08 10:19:30.138838: Yayy! New best EMA pseudo Dice: 0.7773 
2025-10-08 10:19:31.810408:  
2025-10-08 10:19:31.810761: Epoch 3 
2025-10-08 10:19:31.810899: Current learning rate: 0.00986 
2025-10-08 10:22:12.075058: train_loss -0.7413 
2025-10-08 10:22:12.075370: val_loss -0.7141 
2025-10-08 10:22:12.075440: Pseudo dice [0.8098] 
2025-10-08 10:22:12.075516: Epoch time: 160.27 s 
2025-10-08 10:22:12.075573: Yayy! New best EMA pseudo Dice: 0.7806 
2025-10-08 10:22:13.691975:  
2025-10-08 10:22:13.692154: Epoch 4 
2025-10-08 10:22:13.692263: Current learning rate: 0.00982 
2025-10-08 10:24:53.744113: train_loss -0.7475 
2025-10-08 10:24:53.744388: val_loss -0.7075 
2025-10-08 10:24:53.744462: Pseudo dice [0.8025] 
2025-10-08 10:24:53.744544: Epoch time: 160.05 s 
2025-10-08 10:24:53.744604: Yayy! New best EMA pseudo Dice: 0.7828 
2025-10-08 10:24:55.455065:  
2025-10-08 10:24:55.455251: Epoch 5 
2025-10-08 10:24:55.455381: Current learning rate: 0.00977 
2025-10-08 10:27:35.788134: train_loss -0.7531 
2025-10-08 10:27:35.788427: val_loss -0.7032 
2025-10-08 10:27:35.788494: Pseudo dice [0.7982] 
2025-10-08 10:27:35.788572: Epoch time: 160.33 s 
2025-10-08 10:27:35.788632: Yayy! New best EMA pseudo Dice: 0.7843 
2025-10-08 10:27:37.361380:  
2025-10-08 10:27:37.361572: Epoch 6 
2025-10-08 10:27:37.361702: Current learning rate: 0.00973 
2025-10-08 10:30:17.565824: train_loss -0.7578 
2025-10-08 10:30:17.566095: val_loss -0.7157 
2025-10-08 10:30:17.566166: Pseudo dice [0.807] 
2025-10-08 10:30:17.566243: Epoch time: 160.21 s 
2025-10-08 10:30:17.566304: Yayy! New best EMA pseudo Dice: 0.7866 
2025-10-08 10:30:19.161282:  
2025-10-08 10:30:19.161462: Epoch 7 
2025-10-08 10:30:19.161577: Current learning rate: 0.00968 
2025-10-08 10:32:59.411104: train_loss -0.7607 
2025-10-08 10:32:59.411480: val_loss -0.7139 
2025-10-08 10:32:59.411553: Pseudo dice [0.8084] 
2025-10-08 10:32:59.411642: Epoch time: 160.25 s 
2025-10-08 10:32:59.411721: Yayy! New best EMA pseudo Dice: 0.7888 
2025-10-08 10:33:01.049561:  
2025-10-08 10:33:01.049760: Epoch 8 
2025-10-08 10:33:01.049873: Current learning rate: 0.00964 
2025-10-08 10:35:41.254729: train_loss -0.7729 
2025-10-08 10:35:41.255073: val_loss -0.7098 
2025-10-08 10:35:41.255175: Pseudo dice [0.8063] 
2025-10-08 10:35:41.255276: Epoch time: 160.21 s 
2025-10-08 10:35:41.255341: Yayy! New best EMA pseudo Dice: 0.7905 
2025-10-08 10:35:42.956898:  
2025-10-08 10:35:42.957082: Epoch 9 
2025-10-08 10:35:42.957195: Current learning rate: 0.00959 
2025-10-08 10:38:23.397514: train_loss -0.7783 
2025-10-08 10:38:23.397812: val_loss -0.7244 
2025-10-08 10:38:23.397882: Pseudo dice [0.8164] 
2025-10-08 10:38:23.397964: Epoch time: 160.44 s 
2025-10-08 10:38:23.398088: Yayy! New best EMA pseudo Dice: 0.7931 
2025-10-08 10:38:25.262228:  
2025-10-08 10:38:25.262409: Epoch 10 
2025-10-08 10:38:25.262523: Current learning rate: 0.00955 
2025-10-08 10:41:05.583709: train_loss -0.778 
2025-10-08 10:41:05.584028: val_loss -0.7455 
2025-10-08 10:41:05.584098: Pseudo dice [0.8291] 
2025-10-08 10:41:05.584226: Epoch time: 160.32 s 
2025-10-08 10:41:05.584371: Yayy! New best EMA pseudo Dice: 0.7967 
2025-10-08 10:41:07.204636:  
2025-10-08 10:41:07.204824: Epoch 11 
2025-10-08 10:41:07.204989: Current learning rate: 0.0095 
2025-10-08 10:43:47.286770: train_loss -0.7841 
2025-10-08 10:43:47.287063: val_loss -0.74 
2025-10-08 10:43:47.287131: Pseudo dice [0.8247] 
2025-10-08 10:43:47.287206: Epoch time: 160.08 s 
2025-10-08 10:43:47.287281: Yayy! New best EMA pseudo Dice: 0.7995 
2025-10-08 10:43:48.896286:  
2025-10-08 10:43:48.896469: Epoch 12 
2025-10-08 10:43:48.896588: Current learning rate: 0.00946 
2025-10-08 10:46:29.064643: train_loss -0.7883 
2025-10-08 10:46:29.065037: val_loss -0.7181 
2025-10-08 10:46:29.065112: Pseudo dice [0.8186] 
2025-10-08 10:46:29.065193: Epoch time: 160.17 s 
2025-10-08 10:46:29.065249: Yayy! New best EMA pseudo Dice: 0.8014 
2025-10-08 10:46:30.680201:  
2025-10-08 10:46:30.680399: Epoch 13 
2025-10-08 10:46:30.680519: Current learning rate: 0.00941 
2025-10-08 10:49:11.047545: train_loss -0.7911 
2025-10-08 10:49:11.047893: val_loss -0.7433 
2025-10-08 10:49:11.048052: Pseudo dice [0.833] 
2025-10-08 10:49:11.048164: Epoch time: 160.37 s 
2025-10-08 10:49:11.048231: Yayy! New best EMA pseudo Dice: 0.8046 
2025-10-08 10:49:12.673613:  
2025-10-08 10:49:12.673809: Epoch 14 
2025-10-08 10:49:12.673924: Current learning rate: 0.00937 
2025-10-08 10:51:52.811455: train_loss -0.7932 
2025-10-08 10:51:52.811802: val_loss -0.737 
2025-10-08 10:51:52.811878: Pseudo dice [0.8275] 
2025-10-08 10:51:52.811958: Epoch time: 160.14 s 
2025-10-08 10:51:52.812016: Yayy! New best EMA pseudo Dice: 0.8069 
2025-10-08 10:51:54.432792:  
2025-10-08 10:51:54.433095: Epoch 15 
2025-10-08 10:51:54.433235: Current learning rate: 0.00932 
2025-10-08 10:54:34.546109: train_loss -0.7955 
2025-10-08 10:54:34.546440: val_loss -0.7169 
2025-10-08 10:54:34.546508: Pseudo dice [0.8143] 
2025-10-08 10:54:34.546584: Epoch time: 160.11 s 
2025-10-08 10:54:34.546640: Yayy! New best EMA pseudo Dice: 0.8076 
2025-10-08 10:54:36.246626:  
2025-10-08 10:54:36.246798: Epoch 16 
2025-10-08 10:54:36.246909: Current learning rate: 0.00928 
2025-10-08 10:57:16.594639: train_loss -0.7964 
2025-10-08 10:57:16.594965: val_loss -0.7038 
2025-10-08 10:57:16.595041: Pseudo dice [0.8111] 
2025-10-08 10:57:16.595193: Epoch time: 160.35 s 
2025-10-08 10:57:16.595266: Yayy! New best EMA pseudo Dice: 0.808 
2025-10-08 10:57:18.246583:  
2025-10-08 10:57:18.246773: Epoch 17 
2025-10-08 10:57:18.246890: Current learning rate: 0.00923 
2025-10-08 10:59:58.239046: train_loss -0.7877 
2025-10-08 10:59:58.239365: val_loss -0.7129 
2025-10-08 10:59:58.239437: Pseudo dice [0.8105] 
2025-10-08 10:59:58.239513: Epoch time: 159.99 s 
2025-10-08 10:59:58.239576: Yayy! New best EMA pseudo Dice: 0.8082 
2025-10-08 10:59:59.925153:  
2025-10-08 10:59:59.925325: Epoch 18 
2025-10-08 10:59:59.925441: Current learning rate: 0.00919 
2025-10-08 11:02:40.064777: train_loss -0.7863 
2025-10-08 11:02:40.065073: val_loss -0.7267 
2025-10-08 11:02:40.065180: Pseudo dice [0.8187] 
2025-10-08 11:02:40.065330: Epoch time: 160.14 s 
2025-10-08 11:02:40.065454: Yayy! New best EMA pseudo Dice: 0.8093 
2025-10-08 11:02:41.677328:  
2025-10-08 11:02:41.677502: Epoch 19 
2025-10-08 11:02:41.677619: Current learning rate: 0.00914 
2025-10-08 11:05:21.750808: train_loss -0.7985 
2025-10-08 11:05:21.751165: val_loss -0.72 
2025-10-08 11:05:21.751238: Pseudo dice [0.815] 
2025-10-08 11:05:21.751321: Epoch time: 160.07 s 
2025-10-08 11:05:21.751386: Yayy! New best EMA pseudo Dice: 0.8098 
2025-10-08 11:05:23.354670:  
2025-10-08 11:05:23.354851: Epoch 20 
2025-10-08 11:05:23.354961: Current learning rate: 0.0091 
2025-10-08 11:08:03.439256: train_loss -0.8019 
2025-10-08 11:08:03.439568: val_loss -0.7255 
2025-10-08 11:08:03.439645: Pseudo dice [0.8206] 
2025-10-08 11:08:03.439777: Epoch time: 160.09 s 
2025-10-08 11:08:03.439848: Yayy! New best EMA pseudo Dice: 0.8109 
2025-10-08 11:08:05.463258:  
2025-10-08 11:08:05.463466: Epoch 21 
2025-10-08 11:08:05.463674: Current learning rate: 0.00905 
2025-10-08 11:10:45.676532: train_loss -0.8024 
2025-10-08 11:10:45.676854: val_loss -0.7627 
2025-10-08 11:10:45.676926: Pseudo dice [0.8414] 
2025-10-08 11:10:45.677006: Epoch time: 160.21 s 
2025-10-08 11:10:45.677067: Yayy! New best EMA pseudo Dice: 0.814 
2025-10-08 11:10:47.275097:  
2025-10-08 11:10:47.275330: Epoch 22 
2025-10-08 11:10:47.275447: Current learning rate: 0.009 
2025-10-08 11:13:27.411748: train_loss -0.8077 
2025-10-08 11:13:27.412060: val_loss -0.7401 
2025-10-08 11:13:27.412134: Pseudo dice [0.8305] 
2025-10-08 11:13:27.412224: Epoch time: 160.14 s 
2025-10-08 11:13:27.412325: Yayy! New best EMA pseudo Dice: 0.8156 
2025-10-08 11:13:28.987523:  
2025-10-08 11:13:28.987742: Epoch 23 
2025-10-08 11:13:28.987856: Current learning rate: 0.00896 
2025-10-08 11:16:08.966829: train_loss -0.8079 
2025-10-08 11:16:08.967152: val_loss -0.7015 
2025-10-08 11:16:08.967229: Pseudo dice [0.8102] 
2025-10-08 11:16:08.967315: Epoch time: 159.98 s 
2025-10-08 11:16:10.346143:  
2025-10-08 11:16:10.346351: Epoch 24 
2025-10-08 11:16:10.346466: Current learning rate: 0.00891 
2025-10-08 11:18:50.450503: train_loss -0.8106 
2025-10-08 11:18:50.450832: val_loss -0.7501 
2025-10-08 11:18:50.450963: Pseudo dice [0.8387] 
2025-10-08 11:18:50.451091: Epoch time: 160.11 s 
2025-10-08 11:18:50.451157: Yayy! New best EMA pseudo Dice: 0.8174 
2025-10-08 11:18:52.042198:  
2025-10-08 11:18:52.042410: Epoch 25 
2025-10-08 11:18:52.042521: Current learning rate: 0.00887 
2025-10-08 11:21:32.093963: train_loss -0.8119 
2025-10-08 11:21:32.094285: val_loss -0.7223 
2025-10-08 11:21:32.094374: Pseudo dice [0.8235] 
2025-10-08 11:21:32.094455: Epoch time: 160.05 s 
2025-10-08 11:21:32.094514: Yayy! New best EMA pseudo Dice: 0.818 
2025-10-08 11:21:33.653563:  
2025-10-08 11:21:33.653752: Epoch 26 
2025-10-08 11:21:33.653867: Current learning rate: 0.00882 
2025-10-08 11:24:13.587390: train_loss -0.8166 
2025-10-08 11:24:13.587717: val_loss -0.7453 
2025-10-08 11:24:13.587795: Pseudo dice [0.8336] 
2025-10-08 11:24:13.587875: Epoch time: 159.93 s 
2025-10-08 11:24:13.587940: Yayy! New best EMA pseudo Dice: 0.8196 
2025-10-08 11:24:15.172134:  
2025-10-08 11:24:15.172308: Epoch 27 
2025-10-08 11:24:15.172416: Current learning rate: 0.00878 
2025-10-08 11:26:55.341613: train_loss -0.8184 
2025-10-08 11:26:55.341906: val_loss -0.7432 
2025-10-08 11:26:55.341977: Pseudo dice [0.8325] 
2025-10-08 11:26:55.342057: Epoch time: 160.17 s 
2025-10-08 11:26:55.342118: Yayy! New best EMA pseudo Dice: 0.8209 
2025-10-08 11:26:56.912849:  
2025-10-08 11:26:56.913036: Epoch 28 
2025-10-08 11:26:56.913155: Current learning rate: 0.00873 
2025-10-08 11:29:36.926618: train_loss -0.8161 
2025-10-08 11:29:36.926957: val_loss -0.6986 
2025-10-08 11:29:36.927029: Pseudo dice [0.805] 
2025-10-08 11:29:36.927109: Epoch time: 160.01 s 
2025-10-08 11:29:38.134449:  
2025-10-08 11:29:38.134628: Epoch 29 
2025-10-08 11:29:38.134746: Current learning rate: 0.00868 
2025-10-08 11:32:18.183761: train_loss -0.8189 
2025-10-08 11:32:18.184077: val_loss -0.7334 
2025-10-08 11:32:18.184161: Pseudo dice [0.826] 
2025-10-08 11:32:18.184309: Epoch time: 160.05 s 
2025-10-08 11:32:19.409634:  
2025-10-08 11:32:19.409868: Epoch 30 
2025-10-08 11:32:19.409982: Current learning rate: 0.00864 
2025-10-08 11:34:59.463012: train_loss -0.8229 
2025-10-08 11:34:59.463300: val_loss -0.7218 
2025-10-08 11:34:59.463369: Pseudo dice [0.8202] 
2025-10-08 11:34:59.463447: Epoch time: 160.05 s 
2025-10-08 11:35:00.703838:  
2025-10-08 11:35:00.704034: Epoch 31 
2025-10-08 11:35:00.704155: Current learning rate: 0.00859 
2025-10-08 11:37:40.821132: train_loss -0.8174 
2025-10-08 11:37:40.821453: val_loss -0.7452 
2025-10-08 11:37:40.821523: Pseudo dice [0.8357] 
2025-10-08 11:37:40.821601: Epoch time: 160.12 s 
2025-10-08 11:37:40.821668: Yayy! New best EMA pseudo Dice: 0.8215 
2025-10-08 11:37:42.881306:  
2025-10-08 11:37:42.881684: Epoch 32 
2025-10-08 11:37:42.881824: Current learning rate: 0.00855 
2025-10-08 11:40:22.988043: train_loss -0.8216 
2025-10-08 11:40:22.988344: val_loss -0.6898 
2025-10-08 11:40:22.988412: Pseudo dice [0.7996] 
2025-10-08 11:40:22.988490: Epoch time: 160.11 s 
2025-10-08 11:40:24.300281:  
2025-10-08 11:40:24.300468: Epoch 33 
2025-10-08 11:40:24.300580: Current learning rate: 0.0085 
2025-10-08 11:43:04.338003: train_loss -0.8245 
2025-10-08 11:43:04.338356: val_loss -0.7366 
2025-10-08 11:43:04.338450: Pseudo dice [0.8298] 
2025-10-08 11:43:04.338537: Epoch time: 160.04 s 
2025-10-08 11:43:05.622601:  
2025-10-08 11:43:05.622823: Epoch 34 
2025-10-08 11:43:05.622956: Current learning rate: 0.00846 
2025-10-08 11:45:45.791375: train_loss -0.8225 
2025-10-08 11:45:45.791751: val_loss -0.7253 
2025-10-08 11:45:45.791830: Pseudo dice [0.8238] 
2025-10-08 11:45:45.791911: Epoch time: 160.17 s 
2025-10-08 11:45:47.062953:  
2025-10-08 11:45:47.063145: Epoch 35 
2025-10-08 11:45:47.063262: Current learning rate: 0.00841 
2025-10-08 11:48:27.212214: train_loss -0.8271 
2025-10-08 11:48:27.212533: val_loss -0.7311 
2025-10-08 11:48:27.212606: Pseudo dice [0.8261] 
2025-10-08 11:48:27.212762: Epoch time: 160.15 s 
2025-10-08 11:48:28.575708:  
2025-10-08 11:48:28.575924: Epoch 36 
2025-10-08 11:48:28.576056: Current learning rate: 0.00836 
2025-10-08 11:51:08.901113: train_loss -0.8296 
2025-10-08 11:51:08.901388: val_loss -0.7403 
2025-10-08 11:51:08.901461: Pseudo dice [0.8344] 
2025-10-08 11:51:08.901536: Epoch time: 160.33 s 
2025-10-08 11:51:08.901595: Yayy! New best EMA pseudo Dice: 0.8226 
2025-10-08 11:51:10.619071:  
2025-10-08 11:51:10.619255: Epoch 37 
2025-10-08 11:51:10.619388: Current learning rate: 0.00832 
2025-10-08 11:53:50.740643: train_loss -0.833 
2025-10-08 11:53:50.740948: val_loss -0.7222 
2025-10-08 11:53:50.741019: Pseudo dice [0.8204] 
2025-10-08 11:53:50.741096: Epoch time: 160.12 s 
2025-10-08 11:53:52.026930:  
2025-10-08 11:53:52.027183: Epoch 38 
2025-10-08 11:53:52.027315: Current learning rate: 0.00827 
2025-10-08 11:56:32.247899: train_loss -0.828 
2025-10-08 11:56:32.248214: val_loss -0.7344 
2025-10-08 11:56:32.248290: Pseudo dice [0.8259] 
2025-10-08 11:56:32.248373: Epoch time: 160.22 s 
2025-10-08 11:56:32.248432: Yayy! New best EMA pseudo Dice: 0.8227 
2025-10-08 11:56:33.944077:  
2025-10-08 11:56:33.944277: Epoch 39 
2025-10-08 11:56:33.944380: Current learning rate: 0.00823 
2025-10-08 11:59:14.044880: train_loss -0.8293 
2025-10-08 11:59:14.045192: val_loss -0.7541 
2025-10-08 11:59:14.045268: Pseudo dice [0.8381] 
2025-10-08 11:59:14.045369: Epoch time: 160.1 s 
2025-10-08 11:59:14.045452: Yayy! New best EMA pseudo Dice: 0.8242 
2025-10-08 11:59:15.678908:  
2025-10-08 11:59:15.679084: Epoch 40 
2025-10-08 11:59:15.679215: Current learning rate: 0.00818 
2025-10-08 12:01:56.028526: train_loss -0.8294 
2025-10-08 12:01:56.029003: val_loss -0.7391 
2025-10-08 12:01:56.029099: Pseudo dice [0.8264] 
2025-10-08 12:01:56.029191: Epoch time: 160.35 s 
2025-10-08 12:01:56.029253: Yayy! New best EMA pseudo Dice: 0.8245 
2025-10-08 12:01:57.733998:  
2025-10-08 12:01:57.734194: Epoch 41 
2025-10-08 12:01:57.734336: Current learning rate: 0.00813 
2025-10-08 12:04:38.029103: train_loss -0.8342 
2025-10-08 12:04:38.029405: val_loss -0.7104 
2025-10-08 12:04:38.029494: Pseudo dice [0.8144] 
2025-10-08 12:04:38.029575: Epoch time: 160.3 s 
2025-10-08 12:04:39.230248:  
2025-10-08 12:04:39.230489: Epoch 42 
2025-10-08 12:04:39.230605: Current learning rate: 0.00809 
2025-10-08 12:07:19.422262: train_loss -0.835 
2025-10-08 12:07:19.422603: val_loss -0.7461 
2025-10-08 12:07:19.422693: Pseudo dice [0.8302] 
2025-10-08 12:07:19.422771: Epoch time: 160.19 s 
2025-10-08 12:07:20.987431:  
2025-10-08 12:07:20.987659: Epoch 43 
2025-10-08 12:07:20.987771: Current learning rate: 0.00804 
2025-10-08 12:10:01.034555: train_loss -0.837 
2025-10-08 12:10:01.034871: val_loss -0.7173 
2025-10-08 12:10:01.034944: Pseudo dice [0.8157] 
2025-10-08 12:10:01.035022: Epoch time: 160.05 s 
2025-10-08 12:10:02.250914:  
2025-10-08 12:10:02.251247: Epoch 44 
2025-10-08 12:10:02.251419: Current learning rate: 0.008 
2025-10-08 12:12:42.370844: train_loss -0.8386 
2025-10-08 12:12:42.371164: val_loss -0.7395 
2025-10-08 12:12:42.371245: Pseudo dice [0.8304] 
2025-10-08 12:12:42.371375: Epoch time: 160.12 s 
2025-10-08 12:12:43.636196:  
2025-10-08 12:12:43.636435: Epoch 45 
2025-10-08 12:12:43.636559: Current learning rate: 0.00795 
2025-10-08 12:15:23.684619: train_loss -0.8408 
2025-10-08 12:15:23.684923: val_loss -0.7072 
2025-10-08 12:15:23.685019: Pseudo dice [0.8113] 
2025-10-08 12:15:23.685143: Epoch time: 160.05 s 
2025-10-08 12:15:24.869491:  
2025-10-08 12:15:24.869699: Epoch 46 
2025-10-08 12:15:24.869816: Current learning rate: 0.0079 
2025-10-08 12:18:04.917135: train_loss -0.8416 
2025-10-08 12:18:04.917500: val_loss -0.7452 
2025-10-08 12:18:04.917769: Pseudo dice [0.8356] 
2025-10-08 12:18:04.917899: Epoch time: 160.05 s 
2025-10-08 12:18:06.150705:  
2025-10-08 12:18:06.150934: Epoch 47 
2025-10-08 12:18:06.151054: Current learning rate: 0.00786 
2025-10-08 12:20:46.239220: train_loss -0.8407 
2025-10-08 12:20:46.239694: val_loss -0.7425 
2025-10-08 12:20:46.239770: Pseudo dice [0.8337] 
2025-10-08 12:20:46.239850: Epoch time: 160.09 s 
2025-10-08 12:20:46.239909: Yayy! New best EMA pseudo Dice: 0.825 
2025-10-08 12:20:47.819418:  
2025-10-08 12:20:47.819679: Epoch 48 
2025-10-08 12:20:47.819796: Current learning rate: 0.00781 
2025-10-08 12:23:27.660351: train_loss -0.8421 
2025-10-08 12:23:27.660760: val_loss -0.7183 
2025-10-08 12:23:27.660839: Pseudo dice [0.8156] 
2025-10-08 12:23:27.660930: Epoch time: 159.84 s 
2025-10-08 12:23:28.881582:  
2025-10-08 12:23:28.881891: Epoch 49 
2025-10-08 12:23:28.882001: Current learning rate: 0.00777 
2025-10-08 12:26:08.990379: train_loss -0.8404 
2025-10-08 12:26:08.990684: val_loss -0.731 
2025-10-08 12:26:08.990758: Pseudo dice [0.8266] 
2025-10-08 12:26:08.990836: Epoch time: 160.11 s 
2025-10-08 12:26:10.486860:  
2025-10-08 12:26:10.487062: Epoch 50 
2025-10-08 12:26:10.487175: Current learning rate: 0.00772 
2025-10-08 12:28:50.505682: train_loss -0.8434 
2025-10-08 12:28:50.506006: val_loss -0.7377 
2025-10-08 12:28:50.506086: Pseudo dice [0.8282] 
2025-10-08 12:28:50.506189: Epoch time: 160.02 s 
2025-10-08 12:28:51.753608:  
2025-10-08 12:28:51.753805: Epoch 51 
2025-10-08 12:28:51.753918: Current learning rate: 0.00767 
2025-10-08 12:31:31.715757: train_loss -0.8419 
2025-10-08 12:31:31.716067: val_loss -0.7384 
2025-10-08 12:31:31.716148: Pseudo dice [0.8315] 
2025-10-08 12:31:31.716229: Epoch time: 159.96 s 
2025-10-08 12:31:31.716313: Yayy! New best EMA pseudo Dice: 0.8254 
2025-10-08 12:31:33.297301:  
2025-10-08 12:31:33.297485: Epoch 52 
2025-10-08 12:31:33.297597: Current learning rate: 0.00763 
2025-10-08 12:34:13.332550: train_loss -0.8434 
2025-10-08 12:34:13.342841: val_loss -0.7387 
2025-10-08 12:34:13.343024: Pseudo dice [0.8263] 
2025-10-08 12:34:13.343119: Epoch time: 160.04 s 
2025-10-08 12:34:13.343192: Yayy! New best EMA pseudo Dice: 0.8255 
2025-10-08 12:34:14.944385:  
2025-10-08 12:34:14.944570: Epoch 53 
2025-10-08 12:34:14.944687: Current learning rate: 0.00758 
2025-10-08 12:36:55.105097: train_loss -0.8435 
2025-10-08 12:36:55.105385: val_loss -0.7259 
2025-10-08 12:36:55.105457: Pseudo dice [0.822] 
2025-10-08 12:36:55.105535: Epoch time: 160.16 s 
2025-10-08 12:36:56.368306:  
2025-10-08 12:36:56.368501: Epoch 54 
2025-10-08 12:36:56.368621: Current learning rate: 0.00753 
2025-10-08 12:39:36.652308: train_loss -0.8431 
2025-10-08 12:39:36.652638: val_loss -0.7354 
2025-10-08 12:39:36.652722: Pseudo dice [0.8279] 
2025-10-08 12:39:36.652800: Epoch time: 160.29 s 
2025-10-08 12:39:37.889162:  
2025-10-08 12:39:37.889396: Epoch 55 
2025-10-08 12:39:37.889511: Current learning rate: 0.00749 
2025-10-08 12:42:18.307287: train_loss -0.8474 
2025-10-08 12:42:18.307643: val_loss -0.7504 
2025-10-08 12:42:18.307826: Pseudo dice [0.8362] 
2025-10-08 12:42:18.307916: Epoch time: 160.42 s 
2025-10-08 12:42:18.307979: Yayy! New best EMA pseudo Dice: 0.8265 
2025-10-08 12:42:19.940583:  
2025-10-08 12:42:19.940899: Epoch 56 
2025-10-08 12:42:19.941024: Current learning rate: 0.00744 
2025-10-08 12:45:00.475795: train_loss -0.8474 
2025-10-08 12:45:00.476248: val_loss -0.7258 
2025-10-08 12:45:00.476323: Pseudo dice [0.818] 
2025-10-08 12:45:00.476403: Epoch time: 160.54 s 
2025-10-08 12:45:01.703544:  
2025-10-08 12:45:01.703795: Epoch 57 
2025-10-08 12:45:01.703927: Current learning rate: 0.00739 
2025-10-08 12:47:42.118232: train_loss -0.8483 
2025-10-08 12:47:42.118525: val_loss -0.7444 
2025-10-08 12:47:42.118644: Pseudo dice [0.8314] 
2025-10-08 12:47:42.118740: Epoch time: 160.42 s 
2025-10-08 12:47:43.362956:  
2025-10-08 12:47:43.363197: Epoch 58 
2025-10-08 12:47:43.363335: Current learning rate: 0.00735 
2025-10-08 12:50:23.971548: train_loss -0.8496 
2025-10-08 12:50:23.972064: val_loss -0.7502 
2025-10-08 12:50:23.972186: Pseudo dice [0.8353] 
2025-10-08 12:50:23.972270: Epoch time: 160.61 s 
2025-10-08 12:50:23.972328: Yayy! New best EMA pseudo Dice: 0.8271 
2025-10-08 12:50:25.689118:  
2025-10-08 12:50:25.689363: Epoch 59 
2025-10-08 12:50:25.689482: Current learning rate: 0.0073 
2025-10-08 12:53:06.253621: train_loss -0.8515 
2025-10-08 12:53:06.254063: val_loss -0.7195 
2025-10-08 12:53:06.254142: Pseudo dice [0.8191] 
2025-10-08 12:53:06.254239: Epoch time: 160.57 s 
2025-10-08 12:53:07.548918:  
2025-10-08 12:53:07.549069: Epoch 60 
2025-10-08 12:53:07.549175: Current learning rate: 0.00725 
2025-10-08 12:55:47.827357: train_loss -0.8502 
2025-10-08 12:55:47.827632: val_loss -0.7217 
2025-10-08 12:55:47.827717: Pseudo dice [0.8212] 
2025-10-08 12:55:47.827799: Epoch time: 160.28 s 
2025-10-08 12:55:49.088315:  
2025-10-08 12:55:49.088528: Epoch 61 
2025-10-08 12:55:49.088641: Current learning rate: 0.00721 
2025-10-08 12:58:29.370416: train_loss -0.8519 
2025-10-08 12:58:29.370713: val_loss -0.7245 
2025-10-08 12:58:29.370795: Pseudo dice [0.8245] 
2025-10-08 12:58:29.370876: Epoch time: 160.28 s 
2025-10-08 12:58:30.626148:  
2025-10-08 12:58:30.626347: Epoch 62 
2025-10-08 12:58:30.626464: Current learning rate: 0.00716 
2025-10-08 13:01:10.967777: train_loss -0.8524 
2025-10-08 13:01:10.968166: val_loss -0.705 
2025-10-08 13:01:10.968240: Pseudo dice [0.8066] 
2025-10-08 13:01:10.968320: Epoch time: 160.34 s 
2025-10-08 13:01:12.224726:  
2025-10-08 13:01:12.224942: Epoch 63 
2025-10-08 13:01:12.225032: Current learning rate: 0.00711 
2025-10-08 13:03:52.810581: train_loss -0.8585 
2025-10-08 13:03:52.811136: val_loss -0.7396 
2025-10-08 13:03:52.811255: Pseudo dice [0.8329] 
2025-10-08 13:03:52.811347: Epoch time: 160.59 s 
2025-10-08 13:03:54.137606:  
2025-10-08 13:03:54.137831: Epoch 64 
2025-10-08 13:03:54.137960: Current learning rate: 0.00707 
2025-10-08 13:06:34.610187: train_loss -0.8501 
2025-10-08 13:06:34.610510: val_loss -0.7318 
2025-10-08 13:06:34.610594: Pseudo dice [0.8288] 
2025-10-08 13:06:34.610699: Epoch time: 160.47 s 
2025-10-08 13:06:35.884074:  
2025-10-08 13:06:35.884320: Epoch 65 
2025-10-08 13:06:35.884436: Current learning rate: 0.00702 
2025-10-08 13:09:16.672543: train_loss -0.8523 
2025-10-08 13:09:16.672880: val_loss -0.7362 
2025-10-08 13:09:16.672970: Pseudo dice [0.8279] 
2025-10-08 13:09:16.673050: Epoch time: 160.79 s 
2025-10-08 13:09:17.936828:  
2025-10-08 13:09:17.937047: Epoch 66 
2025-10-08 13:09:17.937151: Current learning rate: 0.00697 
2025-10-08 13:11:58.407716: train_loss -0.8575 
2025-10-08 13:11:58.408044: val_loss -0.7528 
2025-10-08 13:11:58.408115: Pseudo dice [0.8444] 
2025-10-08 13:11:58.408216: Epoch time: 160.47 s 
2025-10-08 13:11:58.408294: Yayy! New best EMA pseudo Dice: 0.8273 
2025-10-08 13:12:00.100374:  
2025-10-08 13:12:00.100582: Epoch 67 
2025-10-08 13:12:00.100712: Current learning rate: 0.00693 
2025-10-08 13:14:40.556697: train_loss -0.8574 
2025-10-08 13:14:40.556989: val_loss -0.734 
2025-10-08 13:14:40.557058: Pseudo dice [0.8259] 
2025-10-08 13:14:40.557133: Epoch time: 160.46 s 
2025-10-08 13:14:41.829418:  
2025-10-08 13:14:41.829623: Epoch 68 
2025-10-08 13:14:41.829775: Current learning rate: 0.00688 
2025-10-08 13:17:22.172953: train_loss -0.8572 
2025-10-08 13:17:22.173273: val_loss -0.7649 
2025-10-08 13:17:22.173342: Pseudo dice [0.846] 
2025-10-08 13:17:22.173422: Epoch time: 160.34 s 
2025-10-08 13:17:22.173481: Yayy! New best EMA pseudo Dice: 0.829 
2025-10-08 13:17:23.832653:  
2025-10-08 13:17:23.832879: Epoch 69 
2025-10-08 13:17:23.832990: Current learning rate: 0.00683 
2025-10-08 13:20:04.435768: train_loss -0.8614 
2025-10-08 13:20:04.436063: val_loss -0.7444 
2025-10-08 13:20:04.436133: Pseudo dice [0.8353] 
2025-10-08 13:20:04.436213: Epoch time: 160.6 s 
2025-10-08 13:20:04.436274: Yayy! New best EMA pseudo Dice: 0.8296 
2025-10-08 13:20:06.198726:  
2025-10-08 13:20:06.198924: Epoch 70 
2025-10-08 13:20:06.199041: Current learning rate: 0.00679 
2025-10-08 13:22:46.436770: train_loss -0.8596 
2025-10-08 13:22:46.437109: val_loss -0.7252 
2025-10-08 13:22:46.437192: Pseudo dice [0.8188] 
2025-10-08 13:22:46.437272: Epoch time: 160.24 s 
2025-10-08 13:22:47.714547:  
2025-10-08 13:22:47.714742: Epoch 71 
2025-10-08 13:22:47.714860: Current learning rate: 0.00674 
2025-10-08 13:25:28.044869: train_loss -0.8623 
2025-10-08 13:25:28.045190: val_loss -0.7371 
2025-10-08 13:25:28.045274: Pseudo dice [0.8307] 
2025-10-08 13:25:28.045385: Epoch time: 160.33 s 
2025-10-08 13:25:29.347320:  
2025-10-08 13:25:29.347515: Epoch 72 
2025-10-08 13:25:29.347658: Current learning rate: 0.00669 
2025-10-08 13:28:09.673873: train_loss -0.8581 
2025-10-08 13:28:09.674188: val_loss -0.7315 
2025-10-08 13:28:09.674269: Pseudo dice [0.8258] 
2025-10-08 13:28:09.674349: Epoch time: 160.33 s 
2025-10-08 13:28:10.966173:  
2025-10-08 13:28:10.966464: Epoch 73 
2025-10-08 13:28:10.966601: Current learning rate: 0.00665 
2025-10-08 13:30:51.359353: train_loss -0.8589 
2025-10-08 13:30:51.359664: val_loss -0.7346 
2025-10-08 13:30:51.359737: Pseudo dice [0.8314] 
2025-10-08 13:30:51.359816: Epoch time: 160.39 s 
2025-10-08 13:30:52.641545:  
2025-10-08 13:30:52.641739: Epoch 74 
2025-10-08 13:30:52.641870: Current learning rate: 0.0066 
2025-10-08 13:33:32.959488: train_loss -0.8621 
2025-10-08 13:33:32.959817: val_loss -0.7458 
2025-10-08 13:33:32.959963: Pseudo dice [0.8391] 
2025-10-08 13:33:32.960051: Epoch time: 160.32 s 
2025-10-08 13:33:32.960114: Yayy! New best EMA pseudo Dice: 0.8298 
2025-10-08 13:33:34.758411:  
2025-10-08 13:33:34.758839: Epoch 75 
2025-10-08 13:33:34.759017: Current learning rate: 0.00655 
2025-10-08 13:36:15.064943: train_loss -0.8595 
2025-10-08 13:36:15.065351: val_loss -0.7244 
2025-10-08 13:36:15.065430: Pseudo dice [0.8205] 
2025-10-08 13:36:15.065533: Epoch time: 160.31 s 
2025-10-08 13:36:16.718916:  
2025-10-08 13:36:16.719115: Epoch 76 
2025-10-08 13:36:16.719225: Current learning rate: 0.0065 
2025-10-08 13:38:56.889930: train_loss -0.8626 
2025-10-08 13:38:56.890210: val_loss -0.7399 
2025-10-08 13:38:56.890276: Pseudo dice [0.8282] 
2025-10-08 13:38:56.890355: Epoch time: 160.17 s 
2025-10-08 13:38:58.183393:  
2025-10-08 13:38:58.183598: Epoch 77 
2025-10-08 13:38:58.183729: Current learning rate: 0.00646 
2025-10-08 13:41:38.100587: train_loss -0.8624 
2025-10-08 13:41:38.100934: val_loss -0.7179 
2025-10-08 13:41:38.101006: Pseudo dice [0.8174] 
2025-10-08 13:41:38.101109: Epoch time: 159.92 s 
2025-10-08 13:41:39.413568:  
2025-10-08 13:41:39.413794: Epoch 78 
2025-10-08 13:41:39.413908: Current learning rate: 0.00641 
2025-10-08 13:44:19.513183: train_loss -0.8643 
2025-10-08 13:44:19.513522: val_loss -0.7255 
2025-10-08 13:44:19.513596: Pseudo dice [0.8213] 
2025-10-08 13:44:19.513708: Epoch time: 160.1 s 
2025-10-08 13:44:20.878985:  
2025-10-08 13:44:20.879219: Epoch 79 
2025-10-08 13:44:20.879355: Current learning rate: 0.00636 
2025-10-08 13:47:00.968860: train_loss -0.8628 
2025-10-08 13:47:00.969180: val_loss -0.7456 
2025-10-08 13:47:00.969249: Pseudo dice [0.8386] 
2025-10-08 13:47:00.969331: Epoch time: 160.09 s 
2025-10-08 13:47:02.285964:  
2025-10-08 13:47:02.286161: Epoch 80 
2025-10-08 13:47:02.286272: Current learning rate: 0.00631 
2025-10-08 13:49:42.328347: train_loss -0.8663 
2025-10-08 13:49:42.328717: val_loss -0.7441 
2025-10-08 13:49:42.328817: Pseudo dice [0.8319] 
2025-10-08 13:49:42.328932: Epoch time: 160.04 s 
2025-10-08 13:49:43.667883:  
2025-10-08 13:49:43.668082: Epoch 81 
2025-10-08 13:49:43.668199: Current learning rate: 0.00627 
2025-10-08 13:52:23.731987: train_loss -0.8677 
2025-10-08 13:52:23.732322: val_loss -0.7502 
2025-10-08 13:52:23.732404: Pseudo dice [0.8392] 
2025-10-08 13:52:23.732489: Epoch time: 160.07 s 
2025-10-08 13:52:25.025478:  
2025-10-08 13:52:25.025704: Epoch 82 
2025-10-08 13:52:25.025828: Current learning rate: 0.00622 
2025-10-08 13:55:05.227589: train_loss -0.8662 
2025-10-08 13:55:05.227908: val_loss -0.7458 
2025-10-08 13:55:05.227993: Pseudo dice [0.8368] 
2025-10-08 13:55:05.228078: Epoch time: 160.2 s 
2025-10-08 13:55:05.228145: Yayy! New best EMA pseudo Dice: 0.8303 
2025-10-08 13:55:06.875153:  
2025-10-08 13:55:06.875352: Epoch 83 
2025-10-08 13:55:06.875467: Current learning rate: 0.00617 
2025-10-08 13:57:46.971316: train_loss -0.8703 
2025-10-08 13:57:46.971702: val_loss -0.7406 
2025-10-08 13:57:46.971791: Pseudo dice [0.8306] 
2025-10-08 13:57:46.971890: Epoch time: 160.1 s 
2025-10-08 13:57:46.971947: Yayy! New best EMA pseudo Dice: 0.8304 
2025-10-08 13:57:48.618366:  
2025-10-08 13:57:48.618722: Epoch 84 
2025-10-08 13:57:48.618895: Current learning rate: 0.00612 
2025-10-08 14:00:28.786433: train_loss -0.866 
2025-10-08 14:00:28.786757: val_loss -0.7122 
2025-10-08 14:00:28.786828: Pseudo dice [0.8154] 
2025-10-08 14:00:28.786910: Epoch time: 160.17 s 
2025-10-08 14:00:30.062658:  
2025-10-08 14:00:30.062858: Epoch 85 
2025-10-08 14:00:30.062982: Current learning rate: 0.00608 
2025-10-08 14:03:10.393869: train_loss -0.8723 
2025-10-08 14:03:10.394165: val_loss -0.7315 
2025-10-08 14:03:10.394231: Pseudo dice [0.8289] 
2025-10-08 14:03:10.394309: Epoch time: 160.33 s 
2025-10-08 14:03:11.640207:  
2025-10-08 14:03:11.640450: Epoch 86 
2025-10-08 14:03:11.640582: Current learning rate: 0.00603 
2025-10-08 14:05:51.795919: train_loss -0.8711 
2025-10-08 14:05:51.796189: val_loss -0.7283 
2025-10-08 14:05:51.796258: Pseudo dice [0.8265] 
2025-10-08 14:05:51.796337: Epoch time: 160.16 s 
2025-10-08 14:05:53.032278:  
2025-10-08 14:05:53.032457: Epoch 87 
2025-10-08 14:05:53.032570: Current learning rate: 0.00598 
2025-10-08 14:08:33.362007: train_loss -0.8684 
2025-10-08 14:08:33.362368: val_loss -0.7345 
2025-10-08 14:08:33.362459: Pseudo dice [0.8304] 
2025-10-08 14:08:33.362539: Epoch time: 160.33 s 
2025-10-08 14:08:34.601393:  
2025-10-08 14:08:34.601600: Epoch 88 
2025-10-08 14:08:34.601722: Current learning rate: 0.00593 
2025-10-08 14:11:14.854163: train_loss -0.8693 
2025-10-08 14:11:14.854516: val_loss -0.7408 
2025-10-08 14:11:14.854611: Pseudo dice [0.8298] 
2025-10-08 14:11:14.854730: Epoch time: 160.25 s 
2025-10-08 14:11:16.082273:  
2025-10-08 14:11:16.082495: Epoch 89 
2025-10-08 14:11:16.082607: Current learning rate: 0.00589 
2025-10-08 14:13:56.244944: train_loss -0.87 
2025-10-08 14:13:56.245245: val_loss -0.723 
2025-10-08 14:13:56.245317: Pseudo dice [0.8219] 
2025-10-08 14:13:56.245399: Epoch time: 160.16 s 
2025-10-08 14:13:57.472982:  
2025-10-08 14:13:57.473162: Epoch 90 
2025-10-08 14:13:57.473305: Current learning rate: 0.00584 
2025-10-08 14:16:37.468987: train_loss -0.8728 
2025-10-08 14:16:37.469378: val_loss -0.7035 
2025-10-08 14:16:37.469450: Pseudo dice [0.8122] 
2025-10-08 14:16:37.469536: Epoch time: 160.0 s 
2025-10-08 14:16:38.681953:  
2025-10-08 14:16:38.682252: Epoch 91 
2025-10-08 14:16:38.682406: Current learning rate: 0.00579 
2025-10-08 14:19:18.697117: train_loss -0.8728 
2025-10-08 14:19:18.697418: val_loss -0.7297 
2025-10-08 14:19:18.697484: Pseudo dice [0.8252] 
2025-10-08 14:19:18.697564: Epoch time: 160.02 s 
2025-10-08 14:19:19.942540:  
2025-10-08 14:19:19.942747: Epoch 92 
2025-10-08 14:19:19.942866: Current learning rate: 0.00574 
2025-10-08 14:22:00.078922: train_loss -0.8716 
2025-10-08 14:22:00.079207: val_loss -0.7214 
2025-10-08 14:22:00.079273: Pseudo dice [0.8198] 
2025-10-08 14:22:00.079350: Epoch time: 160.14 s 
2025-10-08 14:22:01.312378:  
2025-10-08 14:22:01.312587: Epoch 93 
2025-10-08 14:22:01.312720: Current learning rate: 0.0057 
2025-10-08 14:24:41.328231: train_loss -0.8706 
2025-10-08 14:24:41.328574: val_loss -0.7184 
2025-10-08 14:24:41.328656: Pseudo dice [0.8173] 
2025-10-08 14:24:41.328763: Epoch time: 160.02 s 
2025-10-08 14:24:42.580554:  
2025-10-08 14:24:42.580774: Epoch 94 
2025-10-08 14:24:42.580891: Current learning rate: 0.00565 
2025-10-08 14:27:22.482526: train_loss -0.8732 
2025-10-08 14:27:22.482835: val_loss -0.7294 
2025-10-08 14:27:22.483160: Pseudo dice [0.8262] 
2025-10-08 14:27:22.483244: Epoch time: 159.9 s 
2025-10-08 14:27:23.734790:  
2025-10-08 14:27:23.734967: Epoch 95 
2025-10-08 14:27:23.735094: Current learning rate: 0.0056 
2025-10-08 14:30:03.961438: train_loss -0.8691 
2025-10-08 14:30:03.961789: val_loss -0.7316 
2025-10-08 14:30:03.961860: Pseudo dice [0.8295] 
2025-10-08 14:30:03.961941: Epoch time: 160.23 s 
2025-10-08 14:30:05.317031:  
2025-10-08 14:30:05.317223: Epoch 96 
2025-10-08 14:30:05.317379: Current learning rate: 0.00555 
2025-10-08 14:32:45.275030: train_loss -0.8772 
2025-10-08 14:32:45.275344: val_loss -0.7394 
2025-10-08 14:32:45.275414: Pseudo dice [0.8339] 
2025-10-08 14:32:45.275595: Epoch time: 159.96 s 
2025-10-08 14:32:46.532745:  
2025-10-08 14:32:46.532926: Epoch 97 
2025-10-08 14:32:46.533038: Current learning rate: 0.0055 
2025-10-08 14:35:26.406898: train_loss -0.8777 
2025-10-08 14:35:26.407204: val_loss -0.7164 
2025-10-08 14:35:26.407275: Pseudo dice [0.8197] 
2025-10-08 14:35:26.407356: Epoch time: 159.88 s 
2025-10-08 14:35:27.668501:  
2025-10-08 14:35:27.668724: Epoch 98 
2025-10-08 14:35:27.668826: Current learning rate: 0.00546 
2025-10-08 14:38:07.806442: train_loss -0.8762 
2025-10-08 14:38:07.806745: val_loss -0.7601 
2025-10-08 14:38:07.806814: Pseudo dice [0.8462] 
2025-10-08 14:38:07.806893: Epoch time: 160.14 s 
2025-10-08 14:38:09.458526:  
2025-10-08 14:38:09.458781: Epoch 99 
2025-10-08 14:38:09.458925: Current learning rate: 0.00541 
2025-10-08 14:40:49.477623: train_loss -0.878 
2025-10-08 14:40:49.477959: val_loss -0.7158 
2025-10-08 14:40:49.478033: Pseudo dice [0.817] 
2025-10-08 14:40:49.478111: Epoch time: 160.02 s 
2025-10-08 14:40:51.121564:  
2025-10-08 14:40:51.121789: Epoch 100 
2025-10-08 14:40:51.121911: Current learning rate: 0.00536 
2025-10-08 14:43:31.150844: train_loss -0.879 
2025-10-08 14:43:31.151452: val_loss -0.7454 
2025-10-08 14:43:31.151526: Pseudo dice [0.8381] 
2025-10-08 14:43:31.151608: Epoch time: 160.03 s 
2025-10-08 14:43:32.400078:  
2025-10-08 14:43:32.400401: Epoch 101 
2025-10-08 14:43:32.400521: Current learning rate: 0.00531 
2025-10-08 14:46:12.610420: train_loss -0.8773 
2025-10-08 14:46:12.610793: val_loss -0.716 
2025-10-08 14:46:12.610871: Pseudo dice [0.8197] 
2025-10-08 14:46:12.610954: Epoch time: 160.21 s 
2025-10-08 14:46:13.872806:  
2025-10-08 14:46:13.873022: Epoch 102 
2025-10-08 14:46:13.873144: Current learning rate: 0.00526 
2025-10-08 14:48:53.916931: train_loss -0.8766 
2025-10-08 14:48:53.917320: val_loss -0.755 
2025-10-08 14:48:53.917403: Pseudo dice [0.8432] 
2025-10-08 14:48:53.917501: Epoch time: 160.05 s 
2025-10-08 14:48:55.182110:  
2025-10-08 14:48:55.182334: Epoch 103 
2025-10-08 14:48:55.182448: Current learning rate: 0.00521 
2025-10-08 14:51:35.493122: train_loss -0.8788 
2025-10-08 14:51:35.493412: val_loss -0.7221 
2025-10-08 14:51:35.493481: Pseudo dice [0.8248] 
2025-10-08 14:51:35.493564: Epoch time: 160.31 s 
2025-10-08 14:51:36.787668:  
2025-10-08 14:51:36.788046: Epoch 104 
2025-10-08 14:51:36.788192: Current learning rate: 0.00517 
2025-10-08 14:54:17.008460: train_loss -0.877 
2025-10-08 14:54:17.008807: val_loss -0.7228 
2025-10-08 14:54:17.008883: Pseudo dice [0.8291] 
2025-10-08 14:54:17.008968: Epoch time: 160.22 s 
2025-10-08 14:54:18.281198:  
2025-10-08 14:54:18.281414: Epoch 105 
2025-10-08 14:54:18.281526: Current learning rate: 0.00512 
2025-10-08 14:56:58.327214: train_loss -0.8771 
2025-10-08 14:56:58.327538: val_loss -0.7222 
2025-10-08 14:56:58.327639: Pseudo dice [0.8266] 
2025-10-08 14:56:58.327838: Epoch time: 160.05 s 
2025-10-08 14:56:59.606237:  
2025-10-08 14:56:59.606554: Epoch 106 
2025-10-08 14:56:59.606699: Current learning rate: 0.00507 
2025-10-08 14:59:39.901935: train_loss -0.8788 
2025-10-08 14:59:39.902524: val_loss -0.734 
2025-10-08 14:59:39.902606: Pseudo dice [0.8264] 
2025-10-08 14:59:39.902718: Epoch time: 160.3 s 
2025-10-08 14:59:41.161749:  
2025-10-08 14:59:41.161964: Epoch 107 
2025-10-08 14:59:41.162092: Current learning rate: 0.00502 
2025-10-08 15:02:21.516905: train_loss -0.8782 
2025-10-08 15:02:21.517264: val_loss -0.7129 
2025-10-08 15:02:21.517364: Pseudo dice [0.8211] 
2025-10-08 15:02:21.517451: Epoch time: 160.36 s 
2025-10-08 15:02:22.774729:  
2025-10-08 15:02:22.774959: Epoch 108 
2025-10-08 15:02:22.775097: Current learning rate: 0.00497 
2025-10-08 15:05:03.119493: train_loss -0.8797 
2025-10-08 15:05:03.119845: val_loss -0.7442 
2025-10-08 15:05:03.119919: Pseudo dice [0.8379] 
2025-10-08 15:05:03.119998: Epoch time: 160.35 s 
2025-10-08 15:05:04.480847:  
2025-10-08 15:05:04.481039: Epoch 109 
2025-10-08 15:05:04.481150: Current learning rate: 0.00492 
2025-10-08 15:07:44.560178: train_loss -0.8812 
2025-10-08 15:07:44.560527: val_loss -0.7479 
2025-10-08 15:07:44.560600: Pseudo dice [0.8374] 
2025-10-08 15:07:44.560697: Epoch time: 160.08 s 
2025-10-08 15:07:45.815897:  
2025-10-08 15:07:45.816069: Epoch 110 
2025-10-08 15:07:45.816185: Current learning rate: 0.00487 
2025-10-08 15:10:25.923894: train_loss -0.8814 
2025-10-08 15:10:25.924207: val_loss -0.6835 
2025-10-08 15:10:25.924285: Pseudo dice [0.8012] 
2025-10-08 15:10:25.924366: Epoch time: 160.11 s 
2025-10-08 15:10:27.633642:  
2025-10-08 15:10:27.633859: Epoch 111 
2025-10-08 15:10:27.633981: Current learning rate: 0.00483 
2025-10-08 15:13:07.647165: train_loss -0.883 
2025-10-08 15:13:07.647527: val_loss -0.7387 
2025-10-08 15:13:07.647621: Pseudo dice [0.8317] 
2025-10-08 15:13:07.647805: Epoch time: 160.02 s 
2025-10-08 15:13:08.916555:  
2025-10-08 15:13:08.916729: Epoch 112 
2025-10-08 15:13:08.916841: Current learning rate: 0.00478 
2025-10-08 15:15:49.054184: train_loss -0.8793 
2025-10-08 15:15:49.054483: val_loss -0.7181 
2025-10-08 15:15:49.054554: Pseudo dice [0.8256] 
2025-10-08 15:15:49.054637: Epoch time: 160.14 s 
2025-10-08 15:15:50.340155:  
2025-10-08 15:15:50.340399: Epoch 113 
2025-10-08 15:15:50.340539: Current learning rate: 0.00473 
2025-10-08 15:18:30.396307: train_loss -0.879 
2025-10-08 15:18:30.396621: val_loss -0.7528 
2025-10-08 15:18:30.396710: Pseudo dice [0.8418] 
2025-10-08 15:18:30.396795: Epoch time: 160.06 s 
2025-10-08 15:18:31.684376:  
2025-10-08 15:18:31.684707: Epoch 114 
2025-10-08 15:18:31.684911: Current learning rate: 0.00468 
2025-10-08 15:21:11.827576: train_loss -0.8845 
2025-10-08 15:21:11.828067: val_loss -0.7056 
2025-10-08 15:21:11.828166: Pseudo dice [0.8136] 
2025-10-08 15:21:11.828282: Epoch time: 160.14 s 
2025-10-08 15:21:13.076402:  
2025-10-08 15:21:13.076630: Epoch 115 
2025-10-08 15:21:13.076755: Current learning rate: 0.00463 
2025-10-08 15:23:53.200393: train_loss -0.881 
2025-10-08 15:23:53.200711: val_loss -0.7173 
2025-10-08 15:23:53.200786: Pseudo dice [0.8174] 
2025-10-08 15:23:53.200868: Epoch time: 160.13 s 
2025-10-08 15:23:54.500266:  
2025-10-08 15:23:54.500475: Epoch 116 
2025-10-08 15:23:54.500589: Current learning rate: 0.00458 
2025-10-08 15:26:34.614892: train_loss -0.8846 
2025-10-08 15:26:34.615545: val_loss -0.7151 
2025-10-08 15:26:34.615624: Pseudo dice [0.8211] 
2025-10-08 15:26:34.615738: Epoch time: 160.12 s 
2025-10-08 15:26:35.939188:  
2025-10-08 15:26:35.939488: Epoch 117 
2025-10-08 15:26:35.939658: Current learning rate: 0.00453 
2025-10-08 15:29:16.145256: train_loss -0.8873 
2025-10-08 15:29:16.145563: val_loss -0.762 
2025-10-08 15:29:16.145640: Pseudo dice [0.8464] 
2025-10-08 15:29:16.145750: Epoch time: 160.21 s 
2025-10-08 15:29:17.463087:  
2025-10-08 15:29:17.463459: Epoch 118 
2025-10-08 15:29:17.463660: Current learning rate: 0.00448 
2025-10-08 15:31:57.891989: train_loss -0.8842 
2025-10-08 15:31:57.892502: val_loss -0.7195 
2025-10-08 15:31:57.892611: Pseudo dice [0.8249] 
2025-10-08 15:31:57.892766: Epoch time: 160.43 s 
2025-10-08 15:31:59.248686:  
2025-10-08 15:31:59.248893: Epoch 119 
2025-10-08 15:31:59.249017: Current learning rate: 0.00443 
2025-10-08 15:34:39.537615: train_loss -0.8861 
2025-10-08 15:34:39.537963: val_loss -0.7283 
2025-10-08 15:34:39.538043: Pseudo dice [0.8242] 
2025-10-08 15:34:39.538139: Epoch time: 160.29 s 
2025-10-08 15:34:40.894727:  
2025-10-08 15:34:40.894943: Epoch 120 
2025-10-08 15:34:40.895077: Current learning rate: 0.00438 
2025-10-08 15:37:21.187784: train_loss -0.8855 
2025-10-08 15:37:21.198765: val_loss -0.7323 
2025-10-08 15:37:21.198887: Pseudo dice [0.8289] 
2025-10-08 15:37:21.199008: Epoch time: 160.29 s 
2025-10-08 15:37:22.588928:  
2025-10-08 15:37:22.589113: Epoch 121 
2025-10-08 15:37:22.589235: Current learning rate: 0.00433 
2025-10-08 15:40:02.871157: train_loss -0.8886 
2025-10-08 15:40:02.871476: val_loss -0.7077 
2025-10-08 15:40:02.871659: Pseudo dice [0.8132] 
2025-10-08 15:40:02.871753: Epoch time: 160.28 s 
2025-10-08 15:40:04.572469:  
2025-10-08 15:40:04.572761: Epoch 122 
2025-10-08 15:40:04.572887: Current learning rate: 0.00429 
2025-10-08 15:42:44.987777: train_loss -0.8874 
2025-10-08 15:42:44.988168: val_loss -0.7266 
2025-10-08 15:42:44.988260: Pseudo dice [0.8272] 
2025-10-08 15:42:44.988365: Epoch time: 160.42 s 
2025-10-08 15:42:46.320910:  
2025-10-08 15:42:46.321142: Epoch 123 
2025-10-08 15:42:46.321266: Current learning rate: 0.00424 
2025-10-08 15:45:26.571625: train_loss -0.8851 
2025-10-08 15:45:26.572176: val_loss -0.7049 
2025-10-08 15:45:26.572258: Pseudo dice [0.816] 
2025-10-08 15:45:26.572559: Epoch time: 160.25 s 
2025-10-08 15:45:27.929519:  
2025-10-08 15:45:27.929756: Epoch 124 
2025-10-08 15:45:27.929875: Current learning rate: 0.00419 
2025-10-08 15:48:08.160473: train_loss -0.8901 
2025-10-08 15:48:08.160764: val_loss -0.7319 
2025-10-08 15:48:08.160868: Pseudo dice [0.8366] 
2025-10-08 15:48:08.161047: Epoch time: 160.23 s 
2025-10-08 15:48:09.708980:  
2025-10-08 15:48:09.709208: Epoch 125 
2025-10-08 15:48:09.709325: Current learning rate: 0.00414 
2025-10-08 15:50:49.960407: train_loss -0.8874 
2025-10-08 15:50:49.960792: val_loss -0.7087 
2025-10-08 15:50:49.960866: Pseudo dice [0.8195] 
2025-10-08 15:50:49.960952: Epoch time: 160.25 s 
2025-10-08 15:50:51.303221:  
2025-10-08 15:50:51.303426: Epoch 126 
2025-10-08 15:50:51.303567: Current learning rate: 0.00409 
2025-10-08 15:53:31.448689: train_loss -0.8871 
2025-10-08 15:53:31.449058: val_loss -0.7098 
2025-10-08 15:53:31.449188: Pseudo dice [0.8184] 
2025-10-08 15:53:31.449279: Epoch time: 160.15 s 
2025-10-08 15:53:32.771863:  
2025-10-08 15:53:32.772141: Epoch 127 
2025-10-08 15:53:32.772281: Current learning rate: 0.00404 
2025-10-08 15:56:13.006770: train_loss -0.8862 
2025-10-08 15:56:13.007074: val_loss -0.7139 
2025-10-08 15:56:13.007144: Pseudo dice [0.8238] 
2025-10-08 15:56:13.007224: Epoch time: 160.24 s 
2025-10-08 15:56:14.319948:  
2025-10-08 15:56:14.320170: Epoch 128 
2025-10-08 15:56:14.320288: Current learning rate: 0.00399 
2025-10-08 15:58:54.591347: train_loss -0.8876 
2025-10-08 15:58:54.591686: val_loss -0.7345 
2025-10-08 15:58:54.591760: Pseudo dice [0.8288] 
2025-10-08 15:58:54.591844: Epoch time: 160.27 s 
2025-10-08 15:58:55.899472:  
2025-10-08 15:58:55.899697: Epoch 129 
2025-10-08 15:58:55.899808: Current learning rate: 0.00394 
2025-10-08 16:01:35.970415: train_loss -0.8885 
2025-10-08 16:01:35.970735: val_loss -0.7281 
2025-10-08 16:01:35.970808: Pseudo dice [0.8259] 
2025-10-08 16:01:35.970888: Epoch time: 160.07 s 
2025-10-08 16:01:37.310380:  
2025-10-08 16:01:37.310578: Epoch 130 
2025-10-08 16:01:37.310706: Current learning rate: 0.00389 
2025-10-08 16:04:17.512747: train_loss -0.8887 
2025-10-08 16:04:17.513115: val_loss -0.7373 
2025-10-08 16:04:17.513189: Pseudo dice [0.8319] 
2025-10-08 16:04:17.513276: Epoch time: 160.2 s 
2025-10-08 16:04:18.885164:  
2025-10-08 16:04:18.885353: Epoch 131 
2025-10-08 16:04:18.885462: Current learning rate: 0.00384 
2025-10-08 16:06:59.098307: train_loss -0.8902 
2025-10-08 16:06:59.098616: val_loss -0.7264 
2025-10-08 16:06:59.098705: Pseudo dice [0.8269] 
2025-10-08 16:06:59.098785: Epoch time: 160.21 s 
2025-10-08 16:07:00.430556:  
2025-10-08 16:07:00.430729: Epoch 132 
2025-10-08 16:07:00.430842: Current learning rate: 0.00379 
2025-10-08 16:09:40.764998: train_loss -0.888 
2025-10-08 16:09:40.765303: val_loss -0.7172 
2025-10-08 16:09:40.765373: Pseudo dice [0.821] 
2025-10-08 16:09:40.765455: Epoch time: 160.34 s 
2025-10-08 16:09:42.434839:  
2025-10-08 16:09:42.435157: Epoch 133 
2025-10-08 16:09:42.435312: Current learning rate: 0.00374 
2025-10-08 16:12:22.482605: train_loss -0.8917 
2025-10-08 16:12:22.482942: val_loss -0.7242 
2025-10-08 16:12:22.483025: Pseudo dice [0.8272] 
2025-10-08 16:12:22.483118: Epoch time: 160.05 s 
2025-10-08 16:12:23.764704:  
2025-10-08 16:12:23.764912: Epoch 134 
2025-10-08 16:12:23.765028: Current learning rate: 0.00369 
2025-10-08 16:15:03.785508: train_loss -0.8913 
2025-10-08 16:15:03.785830: val_loss -0.7126 
2025-10-08 16:15:03.785903: Pseudo dice [0.8235] 
2025-10-08 16:15:03.785981: Epoch time: 160.02 s 
2025-10-08 16:15:05.139229:  
2025-10-08 16:15:05.139456: Epoch 135 
2025-10-08 16:15:05.139578: Current learning rate: 0.00364 
2025-10-08 16:17:45.408579: train_loss -0.891 
2025-10-08 16:17:45.410378: val_loss -0.7327 
2025-10-08 16:17:45.410458: Pseudo dice [0.8297] 
2025-10-08 16:17:45.410544: Epoch time: 160.27 s 
2025-10-08 16:17:46.746635:  
2025-10-08 16:17:46.746881: Epoch 136 
2025-10-08 16:17:46.747095: Current learning rate: 0.00359 
2025-10-08 16:20:27.153888: train_loss -0.8904 
2025-10-08 16:20:27.154221: val_loss -0.7014 
2025-10-08 16:20:27.154386: Pseudo dice [0.8145] 
2025-10-08 16:20:27.154475: Epoch time: 160.41 s 
2025-10-08 16:20:28.516661:  
2025-10-08 16:20:28.516891: Epoch 137 
2025-10-08 16:20:28.517009: Current learning rate: 0.00354 
2025-10-08 16:23:08.822979: train_loss -0.8928 
2025-10-08 16:23:08.823280: val_loss -0.7144 
2025-10-08 16:23:08.823350: Pseudo dice [0.8195] 
2025-10-08 16:23:08.823429: Epoch time: 160.31 s 
2025-10-08 16:23:10.178951:  
2025-10-08 16:23:10.179155: Epoch 138 
2025-10-08 16:23:10.179264: Current learning rate: 0.00349 
2025-10-08 16:25:50.386963: train_loss -0.8936 
2025-10-08 16:25:50.387268: val_loss -0.7256 
2025-10-08 16:25:50.387337: Pseudo dice [0.8272] 
2025-10-08 16:25:50.387416: Epoch time: 160.21 s 
2025-10-08 16:25:51.720132:  
2025-10-08 16:25:51.720377: Epoch 139 
2025-10-08 16:25:51.720497: Current learning rate: 0.00343 
2025-10-08 16:28:31.817756: train_loss -0.8949 
2025-10-08 16:28:31.818182: val_loss -0.7389 
2025-10-08 16:28:31.818258: Pseudo dice [0.8342] 
2025-10-08 16:28:31.818347: Epoch time: 160.1 s 
2025-10-08 16:28:33.115387:  
2025-10-08 16:28:33.115625: Epoch 140 
2025-10-08 16:28:33.115757: Current learning rate: 0.00338 
2025-10-08 16:31:13.166434: train_loss -0.8925 
2025-10-08 16:31:13.166748: val_loss -0.7194 
2025-10-08 16:31:13.166820: Pseudo dice [0.8226] 
2025-10-08 16:31:13.166899: Epoch time: 160.05 s 
2025-10-08 16:31:14.479269:  
2025-10-08 16:31:14.479489: Epoch 141 
2025-10-08 16:31:14.479601: Current learning rate: 0.00333 
2025-10-08 16:33:54.492046: train_loss -0.8952 
2025-10-08 16:33:54.492355: val_loss -0.6976 
2025-10-08 16:33:54.492442: Pseudo dice [0.8133] 
2025-10-08 16:33:54.492547: Epoch time: 160.01 s 
2025-10-08 16:33:55.798872:  
2025-10-08 16:33:55.799158: Epoch 142 
2025-10-08 16:33:55.799380: Current learning rate: 0.00328 
2025-10-08 16:36:35.879900: train_loss -0.8956 
2025-10-08 16:36:35.880194: val_loss -0.7195 
2025-10-08 16:36:35.880287: Pseudo dice [0.8225] 
2025-10-08 16:36:35.880397: Epoch time: 160.08 s 
2025-10-08 16:36:37.183991:  
2025-10-08 16:36:37.184184: Epoch 143 
2025-10-08 16:36:37.184283: Current learning rate: 0.00323 
2025-10-08 16:39:17.263011: train_loss -0.8951 
2025-10-08 16:39:17.263306: val_loss -0.7181 
2025-10-08 16:39:17.263374: Pseudo dice [0.8271] 
2025-10-08 16:39:17.263454: Epoch time: 160.08 s 
2025-10-08 16:39:18.918833:  
2025-10-08 16:39:18.919009: Epoch 144 
2025-10-08 16:39:18.919121: Current learning rate: 0.00318 
2025-10-08 16:41:58.997769: train_loss -0.8953 
2025-10-08 16:41:58.998082: val_loss -0.6919 
2025-10-08 16:41:58.998161: Pseudo dice [0.805] 
2025-10-08 16:41:58.998296: Epoch time: 160.08 s 
2025-10-08 16:42:00.325165:  
2025-10-08 16:42:00.325352: Epoch 145 
2025-10-08 16:42:00.325466: Current learning rate: 0.00313 
2025-10-08 16:44:40.343534: train_loss -0.8967 
2025-10-08 16:44:40.343988: val_loss -0.7256 
2025-10-08 16:44:40.344072: Pseudo dice [0.8256] 
2025-10-08 16:44:40.344305: Epoch time: 160.02 s 
2025-10-08 16:44:41.681877:  
2025-10-08 16:44:41.682121: Epoch 146 
2025-10-08 16:44:41.682237: Current learning rate: 0.00308 
2025-10-08 16:47:21.691901: train_loss -0.8956 
2025-10-08 16:47:21.692467: val_loss -0.7055 
2025-10-08 16:47:21.692537: Pseudo dice [0.8178] 
2025-10-08 16:47:21.692612: Epoch time: 160.01 s 
2025-10-08 16:47:22.997821:  
2025-10-08 16:47:22.998006: Epoch 147 
2025-10-08 16:47:22.998113: Current learning rate: 0.00303 
2025-10-08 16:50:03.087264: train_loss -0.8948 
2025-10-08 16:50:03.087707: val_loss -0.7304 
2025-10-08 16:50:03.087786: Pseudo dice [0.8325] 
2025-10-08 16:50:03.087876: Epoch time: 160.09 s 
2025-10-08 16:50:04.399311:  
2025-10-08 16:50:04.399540: Epoch 148 
2025-10-08 16:50:04.399662: Current learning rate: 0.00297 
2025-10-08 16:52:44.580208: train_loss -0.8966 
2025-10-08 16:52:44.580512: val_loss -0.753 
2025-10-08 16:52:44.580579: Pseudo dice [0.8463] 
2025-10-08 16:52:44.580679: Epoch time: 160.18 s 
2025-10-08 16:52:45.877585:  
2025-10-08 16:52:45.877838: Epoch 149 
2025-10-08 16:52:45.877959: Current learning rate: 0.00292 
2025-10-08 16:55:26.106204: train_loss -0.8996 
2025-10-08 16:55:26.106585: val_loss -0.7156 
2025-10-08 16:55:26.106704: Pseudo dice [0.8241] 
2025-10-08 16:55:26.106827: Epoch time: 160.23 s 
2025-10-08 16:55:27.846788:  
2025-10-08 16:55:27.846948: Epoch 150 
2025-10-08 16:55:27.847087: Current learning rate: 0.00287 
2025-10-08 16:58:07.959388: train_loss -0.8987 
2025-10-08 16:58:07.959679: val_loss -0.6893 
2025-10-08 16:58:07.959754: Pseudo dice [0.8063] 
2025-10-08 16:58:07.959832: Epoch time: 160.11 s 
2025-10-08 16:58:09.261068:  
2025-10-08 16:58:09.261252: Epoch 151 
2025-10-08 16:58:09.261364: Current learning rate: 0.00282 
2025-10-08 17:00:49.377635: train_loss -0.8962 
2025-10-08 17:00:49.378008: val_loss -0.7308 
2025-10-08 17:00:49.378082: Pseudo dice [0.8336] 
2025-10-08 17:00:49.378185: Epoch time: 160.12 s 
2025-10-08 17:00:50.791481:  
2025-10-08 17:00:50.791688: Epoch 152 
2025-10-08 17:00:50.791824: Current learning rate: 0.00277 
2025-10-08 17:03:30.741997: train_loss -0.8964 
2025-10-08 17:03:30.742292: val_loss -0.7173 
2025-10-08 17:03:30.742359: Pseudo dice [0.8228] 
2025-10-08 17:03:30.742437: Epoch time: 159.95 s 
2025-10-08 17:03:32.044930:  
2025-10-08 17:03:32.045131: Epoch 153 
2025-10-08 17:03:32.045243: Current learning rate: 0.00272 
2025-10-08 17:06:12.024300: train_loss -0.9002 
2025-10-08 17:06:12.024644: val_loss -0.7291 
2025-10-08 17:06:12.024737: Pseudo dice [0.8307] 
2025-10-08 17:06:12.024829: Epoch time: 159.98 s 
2025-10-08 17:06:13.769294:  
2025-10-08 17:06:13.769557: Epoch 154 
2025-10-08 17:06:13.769694: Current learning rate: 0.00266 
2025-10-08 17:08:53.784159: train_loss -0.896 
2025-10-08 17:08:53.784478: val_loss -0.7401 
2025-10-08 17:08:53.784548: Pseudo dice [0.8383] 
2025-10-08 17:08:53.784623: Epoch time: 160.02 s 
2025-10-08 17:08:55.117256:  
2025-10-08 17:08:55.117565: Epoch 155 
2025-10-08 17:08:55.117696: Current learning rate: 0.00261 
2025-10-08 17:11:35.026126: train_loss -0.8974 
2025-10-08 17:11:35.026469: val_loss -0.7294 
2025-10-08 17:11:35.026544: Pseudo dice [0.8301] 
2025-10-08 17:11:35.026629: Epoch time: 159.91 s 
2025-10-08 17:11:36.357862:  
2025-10-08 17:11:36.358058: Epoch 156 
2025-10-08 17:11:36.358176: Current learning rate: 0.00256 
2025-10-08 17:14:16.457620: train_loss -0.9025 
2025-10-08 17:14:16.457967: val_loss -0.7233 
2025-10-08 17:14:16.458089: Pseudo dice [0.8231] 
2025-10-08 17:14:16.458179: Epoch time: 160.1 s 
2025-10-08 17:14:17.829295:  
2025-10-08 17:14:17.829644: Epoch 157 
2025-10-08 17:14:17.829874: Current learning rate: 0.00251 
2025-10-08 17:16:57.814838: train_loss -0.8974 
2025-10-08 17:16:57.815178: val_loss -0.7122 
2025-10-08 17:16:57.815244: Pseudo dice [0.8239] 
2025-10-08 17:16:57.815329: Epoch time: 159.99 s 
2025-10-08 17:16:59.193406:  
2025-10-08 17:16:59.193606: Epoch 158 
2025-10-08 17:16:59.193726: Current learning rate: 0.00245 
2025-10-08 17:19:39.235462: train_loss -0.8979 
2025-10-08 17:19:39.235790: val_loss -0.709 
2025-10-08 17:19:39.235895: Pseudo dice [0.8185] 
2025-10-08 17:19:39.236016: Epoch time: 160.04 s 
2025-10-08 17:19:40.595750:  
2025-10-08 17:19:40.595967: Epoch 159 
2025-10-08 17:19:40.596078: Current learning rate: 0.0024 
2025-10-08 17:22:20.539440: train_loss -0.8998 
2025-10-08 17:22:20.539761: val_loss -0.7068 
2025-10-08 17:22:20.539879: Pseudo dice [0.82] 
2025-10-08 17:22:20.540001: Epoch time: 159.94 s 
2025-10-08 17:22:21.859042:  
2025-10-08 17:22:21.859284: Epoch 160 
2025-10-08 17:22:21.859410: Current learning rate: 0.00235 
2025-10-08 17:25:01.960370: train_loss -0.9012 
2025-10-08 17:25:01.960688: val_loss -0.7078 
2025-10-08 17:25:01.960758: Pseudo dice [0.8171] 
2025-10-08 17:25:01.960836: Epoch time: 160.1 s 
2025-10-08 17:25:03.283190:  
2025-10-08 17:25:03.283368: Epoch 161 
2025-10-08 17:25:03.283478: Current learning rate: 0.0023 
2025-10-08 17:27:43.304411: train_loss -0.9016 
2025-10-08 17:27:43.304762: val_loss -0.7401 
2025-10-08 17:27:43.304840: Pseudo dice [0.8396] 
2025-10-08 17:27:43.304926: Epoch time: 160.02 s 
2025-10-08 17:27:44.648434:  
2025-10-08 17:27:44.648739: Epoch 162 
2025-10-08 17:27:44.648887: Current learning rate: 0.00224 
2025-10-08 17:30:24.623995: train_loss -0.9014 
2025-10-08 17:30:24.624311: val_loss -0.7323 
2025-10-08 17:30:24.624380: Pseudo dice [0.8343] 
2025-10-08 17:30:24.624457: Epoch time: 159.98 s 
2025-10-08 17:30:25.960847:  
2025-10-08 17:30:25.961038: Epoch 163 
2025-10-08 17:30:25.961152: Current learning rate: 0.00219 
2025-10-08 17:33:06.003308: train_loss -0.901 
2025-10-08 17:33:06.003585: val_loss -0.7257 
2025-10-08 17:33:06.003683: Pseudo dice [0.8324] 
2025-10-08 17:33:06.003774: Epoch time: 160.04 s 
2025-10-08 17:33:07.374432:  
2025-10-08 17:33:07.374627: Epoch 164 
2025-10-08 17:33:07.374742: Current learning rate: 0.00214 
2025-10-08 17:35:47.392341: train_loss -0.9026 
2025-10-08 17:35:47.392672: val_loss -0.7227 
2025-10-08 17:35:47.392747: Pseudo dice [0.8268] 
2025-10-08 17:35:47.392832: Epoch time: 160.02 s 
2025-10-08 17:35:49.166763:  
2025-10-08 17:35:49.166956: Epoch 165 
2025-10-08 17:35:49.167066: Current learning rate: 0.00208 
2025-10-08 17:38:29.022555: train_loss -0.9024 
2025-10-08 17:38:29.023346: val_loss -0.7324 
2025-10-08 17:38:29.023476: Pseudo dice [0.8343] 
2025-10-08 17:38:29.023561: Epoch time: 159.86 s 
2025-10-08 17:38:30.335442:  
2025-10-08 17:38:30.335644: Epoch 166 
2025-10-08 17:38:30.335816: Current learning rate: 0.00203 
2025-10-08 17:41:10.400881: train_loss -0.9021 
2025-10-08 17:41:10.401260: val_loss -0.7072 
2025-10-08 17:41:10.401359: Pseudo dice [0.8164] 
2025-10-08 17:41:10.401475: Epoch time: 160.07 s 
2025-10-08 17:41:11.695338:  
2025-10-08 17:41:11.695535: Epoch 167 
2025-10-08 17:41:11.695670: Current learning rate: 0.00198 
2025-10-08 17:43:51.915227: train_loss -0.9043 
2025-10-08 17:43:51.915542: val_loss -0.731 
2025-10-08 17:43:51.915612: Pseudo dice [0.8307] 
2025-10-08 17:43:51.915722: Epoch time: 160.22 s 
2025-10-08 17:43:53.272303:  
2025-10-08 17:43:53.272523: Epoch 168 
2025-10-08 17:43:53.272625: Current learning rate: 0.00192 
2025-10-08 17:46:33.550745: train_loss -0.9041 
2025-10-08 17:46:33.551111: val_loss -0.7263 
2025-10-08 17:46:33.551198: Pseudo dice [0.8297] 
2025-10-08 17:46:33.551283: Epoch time: 160.28 s 
2025-10-08 17:46:34.876958:  
2025-10-08 17:46:34.877166: Epoch 169 
2025-10-08 17:46:34.877270: Current learning rate: 0.00187 
2025-10-08 17:49:15.104481: train_loss -0.9032 
2025-10-08 17:49:15.104823: val_loss -0.7276 
2025-10-08 17:49:15.104925: Pseudo dice [0.8296] 
2025-10-08 17:49:15.105025: Epoch time: 160.23 s 
2025-10-08 17:49:16.435607:  
2025-10-08 17:49:16.435859: Epoch 170 
2025-10-08 17:49:16.435980: Current learning rate: 0.00181 
2025-10-08 17:51:56.432873: train_loss -0.9015 
2025-10-08 17:51:56.433257: val_loss -0.7177 
2025-10-08 17:51:56.433335: Pseudo dice [0.8238] 
2025-10-08 17:51:56.433418: Epoch time: 160.0 s 
2025-10-08 17:51:57.747068:  
2025-10-08 17:51:57.747266: Epoch 171 
2025-10-08 17:51:57.747375: Current learning rate: 0.00176 
2025-10-08 17:54:37.868173: train_loss -0.9025 
2025-10-08 17:54:37.868522: val_loss -0.7154 
2025-10-08 17:54:37.868605: Pseudo dice [0.8211] 
2025-10-08 17:54:37.868749: Epoch time: 160.12 s 
2025-10-08 17:54:39.193946:  
2025-10-08 17:54:39.194120: Epoch 172 
2025-10-08 17:54:39.194221: Current learning rate: 0.0017 
2025-10-08 17:57:19.305796: train_loss -0.8997 
2025-10-08 17:57:19.306128: val_loss -0.6975 
2025-10-08 17:57:19.306200: Pseudo dice [0.8121] 
2025-10-08 17:57:19.306280: Epoch time: 160.11 s 
2025-10-08 17:57:20.600219:  
2025-10-08 17:57:20.600414: Epoch 173 
2025-10-08 17:57:20.600550: Current learning rate: 0.00165 
2025-10-08 18:00:00.797002: train_loss -0.9043 
2025-10-08 18:00:00.797353: val_loss -0.7382 
2025-10-08 18:00:00.797427: Pseudo dice [0.8362] 
2025-10-08 18:00:00.797506: Epoch time: 160.2 s 
2025-10-08 18:00:02.096284:  
2025-10-08 18:00:02.096493: Epoch 174 
2025-10-08 18:00:02.096602: Current learning rate: 0.00159 
2025-10-08 18:02:42.169511: train_loss -0.9059 
2025-10-08 18:02:42.169847: val_loss -0.6968 
2025-10-08 18:02:42.169920: Pseudo dice [0.8156] 
2025-10-08 18:02:42.169999: Epoch time: 160.07 s 
2025-10-08 18:02:43.467115:  
2025-10-08 18:02:43.467278: Epoch 175 
2025-10-08 18:02:43.467374: Current learning rate: 0.00154 
2025-10-08 18:05:23.643018: train_loss -0.907 
2025-10-08 18:05:23.643351: val_loss -0.7016 
2025-10-08 18:05:23.643429: Pseudo dice [0.82] 
2025-10-08 18:05:23.643561: Epoch time: 160.18 s 
2025-10-08 18:05:25.479292:  
2025-10-08 18:05:25.479552: Epoch 176 
2025-10-08 18:05:25.479668: Current learning rate: 0.00148 
2025-10-08 18:08:05.711355: train_loss -0.9043 
2025-10-08 18:08:05.711721: val_loss -0.7179 
2025-10-08 18:08:05.711804: Pseudo dice [0.8289] 
2025-10-08 18:08:05.711887: Epoch time: 160.23 s 
2025-10-08 18:08:07.021803:  
2025-10-08 18:08:07.022028: Epoch 177 
2025-10-08 18:08:07.022133: Current learning rate: 0.00143 
2025-10-08 18:10:47.269601: train_loss -0.9072 
2025-10-08 18:10:47.269935: val_loss -0.7378 
2025-10-08 18:10:47.270006: Pseudo dice [0.8367] 
2025-10-08 18:10:47.270098: Epoch time: 160.25 s 
2025-10-08 18:10:48.574975:  
2025-10-08 18:10:48.575197: Epoch 178 
2025-10-08 18:10:48.575303: Current learning rate: 0.00137 
2025-10-08 18:13:28.546849: train_loss -0.905 
2025-10-08 18:13:28.547179: val_loss -0.7248 
2025-10-08 18:13:28.547247: Pseudo dice [0.8284] 
2025-10-08 18:13:28.547331: Epoch time: 159.97 s 
2025-10-08 18:13:29.857475:  
2025-10-08 18:13:29.857705: Epoch 179 
2025-10-08 18:13:29.857818: Current learning rate: 0.00132 
2025-10-08 18:16:09.951763: train_loss -0.9049 
2025-10-08 18:16:09.952106: val_loss -0.7116 
2025-10-08 18:16:09.952177: Pseudo dice [0.8186] 
2025-10-08 18:16:09.952258: Epoch time: 160.1 s 
2025-10-08 18:16:11.258619:  
2025-10-08 18:16:11.258866: Epoch 180 
2025-10-08 18:16:11.258994: Current learning rate: 0.00126 
2025-10-08 18:18:51.271066: train_loss -0.9075 
2025-10-08 18:18:51.271386: val_loss -0.7201 
2025-10-08 18:18:51.271463: Pseudo dice [0.8287] 
2025-10-08 18:18:51.271542: Epoch time: 160.01 s 
2025-10-08 18:18:52.566394:  
2025-10-08 18:18:52.566590: Epoch 181 
2025-10-08 18:18:52.566700: Current learning rate: 0.0012 
2025-10-08 18:21:32.652907: train_loss -0.9087 
2025-10-08 18:21:32.653358: val_loss -0.6976 
2025-10-08 18:21:32.653449: Pseudo dice [0.8131] 
2025-10-08 18:21:32.653549: Epoch time: 160.09 s 
2025-10-08 18:21:33.975253:  
2025-10-08 18:21:33.975444: Epoch 182 
2025-10-08 18:21:33.975566: Current learning rate: 0.00115 
2025-10-08 18:24:14.064977: train_loss -0.9067 
2025-10-08 18:24:14.065403: val_loss -0.7105 
2025-10-08 18:24:14.065498: Pseudo dice [0.8249] 
2025-10-08 18:24:14.065579: Epoch time: 160.09 s 
2025-10-08 18:24:15.379076:  
2025-10-08 18:24:15.379303: Epoch 183 
2025-10-08 18:24:15.379423: Current learning rate: 0.00109 
2025-10-08 18:26:55.543788: train_loss -0.9064 
2025-10-08 18:26:55.544152: val_loss -0.7044 
2025-10-08 18:26:55.544292: Pseudo dice [0.8159] 
2025-10-08 18:26:55.544411: Epoch time: 160.17 s 
2025-10-08 18:26:56.853732:  
2025-10-08 18:26:56.853934: Epoch 184 
2025-10-08 18:26:56.854033: Current learning rate: 0.00103 
2025-10-08 18:29:36.990238: train_loss -0.9059 
2025-10-08 18:29:36.990572: val_loss -0.7148 
2025-10-08 18:29:36.990642: Pseudo dice [0.8262] 
2025-10-08 18:29:36.990749: Epoch time: 160.14 s 
2025-10-08 18:29:38.311588:  
2025-10-08 18:29:38.311793: Epoch 185 
2025-10-08 18:29:38.311908: Current learning rate: 0.00097 
2025-10-08 18:32:18.372221: train_loss -0.9069 
2025-10-08 18:32:18.372536: val_loss -0.7094 
2025-10-08 18:32:18.372602: Pseudo dice [0.8227] 
2025-10-08 18:32:18.372691: Epoch time: 160.06 s 
2025-10-08 18:32:19.667413:  
2025-10-08 18:32:19.667615: Epoch 186 
2025-10-08 18:32:19.667753: Current learning rate: 0.00091 
2025-10-08 18:34:59.823026: train_loss -0.9072 
2025-10-08 18:34:59.823358: val_loss -0.7231 
2025-10-08 18:34:59.823427: Pseudo dice [0.829] 
2025-10-08 18:34:59.823505: Epoch time: 160.16 s 
2025-10-08 18:35:01.247057:  
2025-10-08 18:35:01.247282: Epoch 187 
2025-10-08 18:35:01.247399: Current learning rate: 0.00085 
2025-10-08 18:37:41.403811: train_loss -0.9062 
2025-10-08 18:37:41.404116: val_loss -0.7247 
2025-10-08 18:37:41.404185: Pseudo dice [0.8324] 
2025-10-08 18:37:41.404261: Epoch time: 160.16 s 
2025-10-08 18:37:43.085591:  
2025-10-08 18:37:43.085846: Epoch 188 
2025-10-08 18:37:43.085958: Current learning rate: 0.00079 
2025-10-08 18:40:23.260448: train_loss -0.9066 
2025-10-08 18:40:23.260804: val_loss -0.6901 
2025-10-08 18:40:23.260872: Pseudo dice [0.8124] 
2025-10-08 18:40:23.260953: Epoch time: 160.18 s 
2025-10-08 18:40:24.580146:  
2025-10-08 18:40:24.580353: Epoch 189 
2025-10-08 18:40:24.580462: Current learning rate: 0.00074 
2025-10-08 18:43:04.816913: train_loss -0.9076 
2025-10-08 18:43:04.817221: val_loss -0.7329 
2025-10-08 18:43:04.817302: Pseudo dice [0.8324] 
2025-10-08 18:43:04.817386: Epoch time: 160.24 s 
2025-10-08 18:43:06.117026:  
2025-10-08 18:43:06.117211: Epoch 190 
2025-10-08 18:43:06.117315: Current learning rate: 0.00067 
2025-10-08 18:45:46.224312: train_loss -0.909 
2025-10-08 18:45:46.224742: val_loss -0.7093 
2025-10-08 18:45:46.224818: Pseudo dice [0.8226] 
2025-10-08 18:45:46.224900: Epoch time: 160.11 s 
2025-10-08 18:45:47.538415:  
2025-10-08 18:45:47.538672: Epoch 191 
2025-10-08 18:45:47.538776: Current learning rate: 0.00061 
2025-10-08 18:48:27.604196: train_loss -0.9095 
2025-10-08 18:48:27.604667: val_loss -0.7134 
2025-10-08 18:48:27.604751: Pseudo dice [0.8213] 
2025-10-08 18:48:27.604852: Epoch time: 160.07 s 
2025-10-08 18:48:28.982131:  
2025-10-08 18:48:28.982346: Epoch 192 
2025-10-08 18:48:28.982455: Current learning rate: 0.00055 
2025-10-08 18:51:09.192065: train_loss -0.9071 
2025-10-08 18:51:09.192436: val_loss -0.701 
2025-10-08 18:51:09.192505: Pseudo dice [0.8162] 
2025-10-08 18:51:09.192582: Epoch time: 160.21 s 
2025-10-08 18:51:10.561325:  
2025-10-08 18:51:10.561510: Epoch 193 
2025-10-08 18:51:10.561641: Current learning rate: 0.00049 
2025-10-08 18:53:50.778635: train_loss -0.9068 
2025-10-08 18:53:50.779021: val_loss -0.7204 
2025-10-08 18:53:50.779093: Pseudo dice [0.8318] 
2025-10-08 18:53:50.779173: Epoch time: 160.22 s 
2025-10-08 18:53:52.116437:  
2025-10-08 18:53:52.116675: Epoch 194 
2025-10-08 18:53:52.116790: Current learning rate: 0.00043 
2025-10-08 18:56:32.398400: train_loss -0.9101 
2025-10-08 18:56:32.398780: val_loss -0.6969 
2025-10-08 18:56:32.398875: Pseudo dice [0.8153] 
2025-10-08 18:56:32.398982: Epoch time: 160.28 s 
2025-10-08 18:56:33.751153:  
2025-10-08 18:56:33.751395: Epoch 195 
2025-10-08 18:56:33.751510: Current learning rate: 0.00036 
2025-10-08 18:59:14.024880: train_loss -0.9093 
2025-10-08 18:59:14.025204: val_loss -0.7025 
2025-10-08 18:59:14.025275: Pseudo dice [0.8205] 
2025-10-08 18:59:14.025354: Epoch time: 160.28 s 
2025-10-08 18:59:15.387344:  
2025-10-08 18:59:15.387550: Epoch 196 
2025-10-08 18:59:15.387656: Current learning rate: 0.0003 
2025-10-08 19:01:55.521509: train_loss -0.9098 
2025-10-08 19:01:55.521919: val_loss -0.6993 
2025-10-08 19:01:55.521999: Pseudo dice [0.8189] 
2025-10-08 19:01:55.522089: Epoch time: 160.14 s 
2025-10-08 19:01:56.874850:  
2025-10-08 19:01:56.875086: Epoch 197 
2025-10-08 19:01:56.875204: Current learning rate: 0.00023 
2025-10-08 19:04:36.944258: train_loss -0.9097 
2025-10-08 19:04:36.944628: val_loss -0.6985 
2025-10-08 19:04:36.944724: Pseudo dice [0.8201] 
2025-10-08 19:04:36.944806: Epoch time: 160.07 s 
2025-10-08 19:04:38.696506:  
2025-10-08 19:04:38.696725: Epoch 198 
2025-10-08 19:04:38.696858: Current learning rate: 0.00016 
2025-10-08 19:07:18.763898: train_loss -0.9109 
2025-10-08 19:07:18.764221: val_loss -0.6983 
2025-10-08 19:07:18.764288: Pseudo dice [0.8139] 
2025-10-08 19:07:18.764364: Epoch time: 160.07 s 
2025-10-08 19:07:20.096006:  
2025-10-08 19:07:20.096272: Epoch 199 
2025-10-08 19:07:20.096379: Current learning rate: 8e-05 
2025-10-08 19:10:00.080346: train_loss -0.9077 
2025-10-08 19:10:00.080665: val_loss -0.7397 
2025-10-08 19:10:00.080735: Pseudo dice [0.8408] 
2025-10-08 19:10:00.080819: Epoch time: 159.99 s 
2025-10-08 19:10:02.150637: Training done. 
2025-10-08 19:10:02.419846: Using splits from existing split file: /home/rnga/tsdehaan/my-scratch/Data_nnUNet/nnUnet_preprocessed/Dataset001_AAA/splits_final.json 
2025-10-08 19:10:02.420551: The split file contains 5 splits. 
2025-10-08 19:10:02.420623: Desired fold for training: 1 
2025-10-08 19:10:02.420695: This split has 61 training and 16 validation cases. 
2025-10-08 19:10:02.420997: predicting IVIM_025 
2025-10-08 19:10:02.423323: IVIM_025, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-08 19:10:11.877457: predicting IVIM_046 
2025-10-08 19:10:11.879022: IVIM_046, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-08 19:10:12.428367: predicting IVIM_049 
2025-10-08 19:10:12.429863: IVIM_049, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-08 19:10:12.942457: predicting IVIM_052 
2025-10-08 19:10:12.943884: IVIM_052, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-08 19:10:13.452278: predicting IVIM_062 
2025-10-08 19:10:13.453686: IVIM_062, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-08 19:10:13.990849: predicting IVIM_076 
2025-10-08 19:10:13.992151: IVIM_076, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-08 19:10:15.661726: predicting IVIM_082 
2025-10-08 19:10:15.663128: IVIM_082, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-08 19:10:16.198491: predicting IVIM_117 
2025-10-08 19:10:16.199928: IVIM_117, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-08 19:10:16.797431: predicting IVIM_127 
2025-10-08 19:10:16.799377: IVIM_127, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-08 19:10:17.299806: predicting IVIM_130 
2025-10-08 19:10:17.302099: IVIM_130, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-08 19:10:17.801699: predicting IVIM_131 
2025-10-08 19:10:17.803522: IVIM_131, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-08 19:10:18.303323: predicting IVIM_137 
2025-10-08 19:10:18.305004: IVIM_137, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-08 19:10:18.808834: predicting IVIM_147 
2025-10-08 19:10:18.810384: IVIM_147, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-08 19:10:19.311671: predicting IVIM_148 
2025-10-08 19:10:19.313263: IVIM_148, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-08 19:10:19.814558: predicting IVIM_149 
2025-10-08 19:10:19.816173: IVIM_149, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-08 19:10:20.316467: predicting IVIM_153 
2025-10-08 19:10:20.318336: IVIM_153, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-08 19:10:32.385406: Validation complete 
2025-10-08 19:10:32.385531: Mean Validation Dice:  0.810394867363589 
