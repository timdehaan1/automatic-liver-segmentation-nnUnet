
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 
2025-10-09 10:39:41.361806: do_dummy_2d_data_aug: True 
2025-10-09 10:39:41.362683: Using splits from existing split file: /home/rnga/tsdehaan/my-scratch/Data_nnUNet/nnUnet_preprocessed/Dataset001_AAA/splits_final.json 
2025-10-09 10:39:41.362900: The split file contains 5 splits. 
2025-10-09 10:39:41.362950: Desired fold for training: 4 
2025-10-09 10:39:41.362990: This split has 62 training and 15 validation cases. 
2025-10-09 10:40:00.512530: Using torch.compile... 

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 3, 'patch_size': [28, 192, 256], 'median_image_size_in_voxels': [27.0, 168.0, 256.0], 'spacing': [7.0, 1.7578125, 1.7578125], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [True], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[1, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [1, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': False} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset001_AAA', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [7.0, 1.7578125, 1.7578125], 'original_median_shape_after_transp': [27, 168, 256], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 2868.689453125, 'mean': 227.654052734375, 'median': 202.5998992919922, 'min': 16.920146942138672, 'percentile_00_5': 53.70329284667969, 'percentile_99_5': 852.1956787109375, 'std': 123.62262725830078}}} 
 
2025-10-09 10:40:03.088475: unpacking dataset... 
2025-10-09 10:40:15.190558: unpacking done... 
2025-10-09 10:40:15.192901: Unable to plot network architecture: nnUNet_compile is enabled! 
2025-10-09 10:40:15.250775:  
2025-10-09 10:40:15.251103: Epoch 0 
2025-10-09 10:40:15.251304: Current learning rate: 0.01 
2025-10-09 10:43:15.156003: train_loss -0.5425 
2025-10-09 10:43:15.156257: val_loss -0.6293 
2025-10-09 10:43:15.156323: Pseudo dice [0.7544] 
2025-10-09 10:43:15.156389: Epoch time: 179.91 s 
2025-10-09 10:43:15.156443: Yayy! New best EMA pseudo Dice: 0.7544 
2025-10-09 10:43:16.541433:  
2025-10-09 10:43:16.541625: Epoch 1 
2025-10-09 10:43:16.541754: Current learning rate: 0.00995 
2025-10-09 10:45:56.252164: train_loss -0.706 
2025-10-09 10:45:56.282065: val_loss -0.6961 
2025-10-09 10:45:56.282336: Pseudo dice [0.7911] 
2025-10-09 10:45:56.282441: Epoch time: 159.71 s 
2025-10-09 10:45:56.282524: Yayy! New best EMA pseudo Dice: 0.7581 
2025-10-09 10:45:57.952266:  
2025-10-09 10:45:57.952465: Epoch 2 
2025-10-09 10:45:57.952565: Current learning rate: 0.00991 
2025-10-09 10:48:37.698670: train_loss -0.7335 
2025-10-09 10:48:37.698985: val_loss -0.6784 
2025-10-09 10:48:37.699055: Pseudo dice [0.7805] 
2025-10-09 10:48:37.699132: Epoch time: 159.75 s 
2025-10-09 10:48:37.699190: Yayy! New best EMA pseudo Dice: 0.7603 
2025-10-09 10:48:39.348005:  
2025-10-09 10:48:39.348197: Epoch 3 
2025-10-09 10:48:39.348300: Current learning rate: 0.00986 
2025-10-09 10:51:18.873621: train_loss -0.7444 
2025-10-09 10:51:18.873991: val_loss -0.7107 
2025-10-09 10:51:18.874064: Pseudo dice [0.7987] 
2025-10-09 10:51:18.874144: Epoch time: 159.53 s 
2025-10-09 10:51:18.874201: Yayy! New best EMA pseudo Dice: 0.7642 
2025-10-09 10:51:20.596096:  
2025-10-09 10:51:20.596288: Epoch 4 
2025-10-09 10:51:20.596392: Current learning rate: 0.00982 
2025-10-09 10:54:00.338375: train_loss -0.7544 
2025-10-09 10:54:00.338930: val_loss -0.6954 
2025-10-09 10:54:00.339008: Pseudo dice [0.7924] 
2025-10-09 10:54:00.339097: Epoch time: 159.74 s 
2025-10-09 10:54:00.339157: Yayy! New best EMA pseudo Dice: 0.767 
2025-10-09 10:54:02.006869:  
2025-10-09 10:54:02.007051: Epoch 5 
2025-10-09 10:54:02.007157: Current learning rate: 0.00977 
2025-10-09 10:56:42.367619: train_loss -0.755 
2025-10-09 10:56:42.367943: val_loss -0.6666 
2025-10-09 10:56:42.368025: Pseudo dice [0.7724] 
2025-10-09 10:56:42.368105: Epoch time: 160.36 s 
2025-10-09 10:56:42.368161: Yayy! New best EMA pseudo Dice: 0.7675 
2025-10-09 10:56:43.944956:  
2025-10-09 10:56:43.945143: Epoch 6 
2025-10-09 10:56:43.945251: Current learning rate: 0.00973 
2025-10-09 10:59:23.696803: train_loss -0.7633 
2025-10-09 10:59:23.697183: val_loss -0.7358 
2025-10-09 10:59:23.697258: Pseudo dice [0.8159] 
2025-10-09 10:59:23.697344: Epoch time: 159.75 s 
2025-10-09 10:59:23.697406: Yayy! New best EMA pseudo Dice: 0.7724 
2025-10-09 10:59:25.454547:  
2025-10-09 10:59:25.454760: Epoch 7 
2025-10-09 10:59:25.454868: Current learning rate: 0.00968 
2025-10-09 11:02:05.157748: train_loss -0.7701 
2025-10-09 11:02:05.158088: val_loss -0.7358 
2025-10-09 11:02:05.158160: Pseudo dice [0.8163] 
2025-10-09 11:02:05.158250: Epoch time: 159.7 s 
2025-10-09 11:02:05.158387: Yayy! New best EMA pseudo Dice: 0.7768 
2025-10-09 11:02:07.028210:  
2025-10-09 11:02:07.028433: Epoch 8 
2025-10-09 11:02:07.028546: Current learning rate: 0.00964 
2025-10-09 11:04:47.144175: train_loss -0.7675 
2025-10-09 11:04:47.144556: val_loss -0.7177 
2025-10-09 11:04:47.144632: Pseudo dice [0.8077] 
2025-10-09 11:04:47.144728: Epoch time: 160.12 s 
2025-10-09 11:04:47.144789: Yayy! New best EMA pseudo Dice: 0.7799 
2025-10-09 11:04:49.006120:  
2025-10-09 11:04:49.006397: Epoch 9 
2025-10-09 11:04:49.006543: Current learning rate: 0.00959 
2025-10-09 11:07:29.239734: train_loss -0.7825 
2025-10-09 11:07:29.240093: val_loss -0.7265 
2025-10-09 11:07:29.240163: Pseudo dice [0.8105] 
2025-10-09 11:07:29.240307: Epoch time: 160.23 s 
2025-10-09 11:07:29.240373: Yayy! New best EMA pseudo Dice: 0.7829 
2025-10-09 11:07:31.450705:  
2025-10-09 11:07:31.450917: Epoch 10 
2025-10-09 11:07:31.451031: Current learning rate: 0.00955 
2025-10-09 11:10:11.549045: train_loss -0.7845 
2025-10-09 11:10:11.557946: val_loss -0.7201 
2025-10-09 11:10:11.558166: Pseudo dice [0.814] 
2025-10-09 11:10:11.558320: Epoch time: 160.1 s 
2025-10-09 11:10:11.558427: Yayy! New best EMA pseudo Dice: 0.786 
2025-10-09 11:10:13.367450:  
2025-10-09 11:10:13.367786: Epoch 11 
2025-10-09 11:10:13.367928: Current learning rate: 0.0095 
2025-10-09 11:12:53.598954: train_loss -0.7845 
2025-10-09 11:12:53.599286: val_loss -0.709 
2025-10-09 11:12:53.599359: Pseudo dice [0.8003] 
2025-10-09 11:12:53.599475: Epoch time: 160.23 s 
2025-10-09 11:12:53.599570: Yayy! New best EMA pseudo Dice: 0.7875 
2025-10-09 11:12:55.400494:  
2025-10-09 11:12:55.400776: Epoch 12 
2025-10-09 11:12:55.400896: Current learning rate: 0.00946 
2025-10-09 11:15:35.451732: train_loss -0.7847 
2025-10-09 11:15:35.452064: val_loss -0.728 
2025-10-09 11:15:35.452136: Pseudo dice [0.8103] 
2025-10-09 11:15:35.452214: Epoch time: 160.05 s 
2025-10-09 11:15:35.452276: Yayy! New best EMA pseudo Dice: 0.7898 
2025-10-09 11:15:37.258386:  
2025-10-09 11:15:37.258634: Epoch 13 
2025-10-09 11:15:37.258778: Current learning rate: 0.00941 
2025-10-09 11:18:17.557292: train_loss -0.7859 
2025-10-09 11:18:17.557614: val_loss -0.73 
2025-10-09 11:18:17.557709: Pseudo dice [0.8182] 
2025-10-09 11:18:17.557789: Epoch time: 160.3 s 
2025-10-09 11:18:17.557847: Yayy! New best EMA pseudo Dice: 0.7926 
2025-10-09 11:18:19.393940:  
2025-10-09 11:18:19.394148: Epoch 14 
2025-10-09 11:18:19.394259: Current learning rate: 0.00937 
2025-10-09 11:20:59.750993: train_loss -0.7901 
2025-10-09 11:20:59.751335: val_loss -0.7187 
2025-10-09 11:20:59.751410: Pseudo dice [0.8075] 
2025-10-09 11:20:59.751488: Epoch time: 160.36 s 
2025-10-09 11:20:59.751546: Yayy! New best EMA pseudo Dice: 0.7941 
2025-10-09 11:21:01.704077:  
2025-10-09 11:21:01.704323: Epoch 15 
2025-10-09 11:21:01.704453: Current learning rate: 0.00932 
2025-10-09 11:23:42.139258: train_loss -0.7968 
2025-10-09 11:23:42.139623: val_loss -0.7322 
2025-10-09 11:23:42.139873: Pseudo dice [0.8137] 
2025-10-09 11:23:42.139984: Epoch time: 160.44 s 
2025-10-09 11:23:42.140050: Yayy! New best EMA pseudo Dice: 0.796 
2025-10-09 11:23:43.984329:  
2025-10-09 11:23:43.984589: Epoch 16 
2025-10-09 11:23:43.984715: Current learning rate: 0.00928 
2025-10-09 11:26:24.334618: train_loss -0.7976 
2025-10-09 11:26:24.334987: val_loss -0.7534 
2025-10-09 11:26:24.335063: Pseudo dice [0.8309] 
2025-10-09 11:26:24.335145: Epoch time: 160.35 s 
2025-10-09 11:26:24.335207: Yayy! New best EMA pseudo Dice: 0.7995 
2025-10-09 11:26:26.256112:  
2025-10-09 11:26:26.256381: Epoch 17 
2025-10-09 11:26:26.256508: Current learning rate: 0.00923 
2025-10-09 11:29:06.561048: train_loss -0.7928 
2025-10-09 11:29:06.561421: val_loss -0.7269 
2025-10-09 11:29:06.561502: Pseudo dice [0.8156] 
2025-10-09 11:29:06.561584: Epoch time: 160.31 s 
2025-10-09 11:29:06.561672: Yayy! New best EMA pseudo Dice: 0.8011 
2025-10-09 11:29:08.388391:  
2025-10-09 11:29:08.388586: Epoch 18 
2025-10-09 11:29:08.388707: Current learning rate: 0.00919 
2025-10-09 11:31:48.629208: train_loss -0.7976 
2025-10-09 11:31:48.629549: val_loss -0.7295 
2025-10-09 11:31:48.629622: Pseudo dice [0.8119] 
2025-10-09 11:31:48.629777: Epoch time: 160.24 s 
2025-10-09 11:31:48.629851: Yayy! New best EMA pseudo Dice: 0.8022 
2025-10-09 11:31:50.474409:  
2025-10-09 11:31:50.474807: Epoch 19 
2025-10-09 11:31:50.474946: Current learning rate: 0.00914 
2025-10-09 11:34:31.067501: train_loss -0.8016 
2025-10-09 11:34:31.067825: val_loss -0.7338 
2025-10-09 11:34:31.067907: Pseudo dice [0.8172] 
2025-10-09 11:34:31.068012: Epoch time: 160.59 s 
2025-10-09 11:34:31.068079: Yayy! New best EMA pseudo Dice: 0.8037 
2025-10-09 11:34:32.867986:  
2025-10-09 11:34:32.868194: Epoch 20 
2025-10-09 11:34:32.868383: Current learning rate: 0.0091 
2025-10-09 11:37:13.088960: train_loss -0.8029 
2025-10-09 11:37:13.089303: val_loss -0.6919 
2025-10-09 11:37:13.089377: Pseudo dice [0.7934] 
2025-10-09 11:37:13.089457: Epoch time: 160.22 s 
2025-10-09 11:37:14.950601:  
2025-10-09 11:37:14.950843: Epoch 21 
2025-10-09 11:37:14.950975: Current learning rate: 0.00905 
2025-10-09 11:39:55.212065: train_loss -0.8007 
2025-10-09 11:39:55.212819: val_loss -0.7528 
2025-10-09 11:39:55.212907: Pseudo dice [0.8268] 
2025-10-09 11:39:55.212995: Epoch time: 160.26 s 
2025-10-09 11:39:55.213061: Yayy! New best EMA pseudo Dice: 0.8051 
2025-10-09 11:39:57.029831:  
2025-10-09 11:39:57.030082: Epoch 22 
2025-10-09 11:39:57.030198: Current learning rate: 0.009 
2025-10-09 11:42:37.245394: train_loss -0.804 
2025-10-09 11:42:37.245756: val_loss -0.7136 
2025-10-09 11:42:37.245892: Pseudo dice [0.8077] 
2025-10-09 11:42:37.246006: Epoch time: 160.22 s 
2025-10-09 11:42:37.246094: Yayy! New best EMA pseudo Dice: 0.8054 
2025-10-09 11:42:39.009996:  
2025-10-09 11:42:39.010225: Epoch 23 
2025-10-09 11:42:39.010348: Current learning rate: 0.00896 
2025-10-09 11:45:19.140434: train_loss -0.8066 
2025-10-09 11:45:19.140787: val_loss -0.7146 
2025-10-09 11:45:19.140863: Pseudo dice [0.8096] 
2025-10-09 11:45:19.140942: Epoch time: 160.13 s 
2025-10-09 11:45:19.141002: Yayy! New best EMA pseudo Dice: 0.8058 
2025-10-09 11:45:20.973746:  
2025-10-09 11:45:20.974012: Epoch 24 
2025-10-09 11:45:20.974141: Current learning rate: 0.00891 
2025-10-09 11:48:01.230497: train_loss -0.8143 
2025-10-09 11:48:01.230850: val_loss -0.7014 
2025-10-09 11:48:01.230924: Pseudo dice [0.801] 
2025-10-09 11:48:01.231008: Epoch time: 160.26 s 
2025-10-09 11:48:02.572744:  
2025-10-09 11:48:02.573024: Epoch 25 
2025-10-09 11:48:02.573163: Current learning rate: 0.00887 
2025-10-09 11:50:42.852666: train_loss -0.8135 
2025-10-09 11:50:42.853028: val_loss -0.7151 
2025-10-09 11:50:42.853158: Pseudo dice [0.8095] 
2025-10-09 11:50:42.853263: Epoch time: 160.28 s 
2025-10-09 11:50:44.219390:  
2025-10-09 11:50:44.219634: Epoch 26 
2025-10-09 11:50:44.219768: Current learning rate: 0.00882 
2025-10-09 11:53:24.720476: train_loss -0.8165 
2025-10-09 11:53:24.720869: val_loss -0.7307 
2025-10-09 11:53:24.720954: Pseudo dice [0.8144] 
2025-10-09 11:53:24.721038: Epoch time: 160.5 s 
2025-10-09 11:53:24.721108: Yayy! New best EMA pseudo Dice: 0.8066 
2025-10-09 11:53:26.565773:  
2025-10-09 11:53:26.566022: Epoch 27 
2025-10-09 11:53:26.566143: Current learning rate: 0.00878 
2025-10-09 11:56:07.069244: train_loss -0.8107 
2025-10-09 11:56:07.069599: val_loss -0.7362 
2025-10-09 11:56:07.069705: Pseudo dice [0.8201] 
2025-10-09 11:56:07.069858: Epoch time: 160.5 s 
2025-10-09 11:56:07.069968: Yayy! New best EMA pseudo Dice: 0.8079 
2025-10-09 11:56:08.863054:  
2025-10-09 11:56:08.863310: Epoch 28 
2025-10-09 11:56:08.863422: Current learning rate: 0.00873 
2025-10-09 11:58:48.979975: train_loss -0.8155 
2025-10-09 11:58:48.980338: val_loss -0.7275 
2025-10-09 11:58:48.980433: Pseudo dice [0.8163] 
2025-10-09 11:58:48.980536: Epoch time: 160.12 s 
2025-10-09 11:58:48.980601: Yayy! New best EMA pseudo Dice: 0.8088 
2025-10-09 11:58:50.784798:  
2025-10-09 11:58:50.785038: Epoch 29 
2025-10-09 11:58:50.785178: Current learning rate: 0.00868 
2025-10-09 12:01:31.073572: train_loss -0.8161 
2025-10-09 12:01:31.073921: val_loss -0.6941 
2025-10-09 12:01:31.073995: Pseudo dice [0.8008] 
2025-10-09 12:01:31.074133: Epoch time: 160.29 s 
2025-10-09 12:01:32.507143:  
2025-10-09 12:01:32.507347: Epoch 30 
2025-10-09 12:01:32.507457: Current learning rate: 0.00864 
2025-10-09 12:04:12.830243: train_loss -0.815 
2025-10-09 12:04:12.830612: val_loss -0.7279 
2025-10-09 12:04:12.830706: Pseudo dice [0.813] 
2025-10-09 12:04:12.830789: Epoch time: 160.32 s 
2025-10-09 12:04:14.218366:  
2025-10-09 12:04:14.218679: Epoch 31 
2025-10-09 12:04:14.218826: Current learning rate: 0.00859 
2025-10-09 12:06:54.248805: train_loss -0.8212 
2025-10-09 12:06:54.249127: val_loss -0.7341 
2025-10-09 12:06:54.249199: Pseudo dice [0.8168] 
2025-10-09 12:06:54.249276: Epoch time: 160.03 s 
2025-10-09 12:06:54.249338: Yayy! New best EMA pseudo Dice: 0.8093 
2025-10-09 12:06:56.548259:  
2025-10-09 12:06:56.548834: Epoch 32 
2025-10-09 12:06:56.548984: Current learning rate: 0.00855 
2025-10-09 12:09:37.125800: train_loss -0.8246 
2025-10-09 12:09:37.126148: val_loss -0.6963 
2025-10-09 12:09:37.126236: Pseudo dice [0.7992] 
2025-10-09 12:09:37.126348: Epoch time: 160.58 s 
2025-10-09 12:09:38.490889:  
2025-10-09 12:09:38.491124: Epoch 33 
2025-10-09 12:09:38.491231: Current learning rate: 0.0085 
2025-10-09 12:12:18.718740: train_loss -0.821 
2025-10-09 12:12:18.719119: val_loss -0.7248 
2025-10-09 12:12:18.719195: Pseudo dice [0.8133] 
2025-10-09 12:12:18.719279: Epoch time: 160.23 s 
2025-10-09 12:12:20.117975:  
2025-10-09 12:12:20.118199: Epoch 34 
2025-10-09 12:12:20.118306: Current learning rate: 0.00846 
2025-10-09 12:15:00.248511: train_loss -0.8207 
2025-10-09 12:15:00.248909: val_loss -0.7091 
2025-10-09 12:15:00.249001: Pseudo dice [0.8014] 
2025-10-09 12:15:00.249094: Epoch time: 160.13 s 
2025-10-09 12:15:01.652234:  
2025-10-09 12:15:01.652448: Epoch 35 
2025-10-09 12:15:01.652557: Current learning rate: 0.00841 
2025-10-09 12:17:42.036714: train_loss -0.8209 
2025-10-09 12:17:42.037123: val_loss -0.7041 
2025-10-09 12:17:42.037200: Pseudo dice [0.8019] 
2025-10-09 12:17:42.037289: Epoch time: 160.39 s 
2025-10-09 12:17:43.393074:  
2025-10-09 12:17:43.393296: Epoch 36 
2025-10-09 12:17:43.393408: Current learning rate: 0.00836 
2025-10-09 12:20:23.565914: train_loss -0.8217 
2025-10-09 12:20:23.566260: val_loss -0.7247 
2025-10-09 12:20:23.566331: Pseudo dice [0.8128] 
2025-10-09 12:20:23.566412: Epoch time: 160.17 s 
2025-10-09 12:20:25.015567:  
2025-10-09 12:20:25.015968: Epoch 37 
2025-10-09 12:20:25.016141: Current learning rate: 0.00832 
2025-10-09 12:23:05.090184: train_loss -0.8212 
2025-10-09 12:23:05.090522: val_loss -0.694 
2025-10-09 12:23:05.090597: Pseudo dice [0.8012] 
2025-10-09 12:23:05.090688: Epoch time: 160.08 s 
2025-10-09 12:23:06.527867:  
2025-10-09 12:23:06.528158: Epoch 38 
2025-10-09 12:23:06.528295: Current learning rate: 0.00827 
2025-10-09 12:25:46.741733: train_loss -0.8261 
2025-10-09 12:25:46.742061: val_loss -0.6979 
2025-10-09 12:25:46.742130: Pseudo dice [0.7992] 
2025-10-09 12:25:46.742204: Epoch time: 160.22 s 
2025-10-09 12:25:48.118452:  
2025-10-09 12:25:48.118751: Epoch 39 
2025-10-09 12:25:48.118869: Current learning rate: 0.00823 
2025-10-09 12:28:28.357618: train_loss -0.835 
2025-10-09 12:28:28.358124: val_loss -0.7309 
2025-10-09 12:28:28.358266: Pseudo dice [0.8191] 
2025-10-09 12:28:28.358358: Epoch time: 160.24 s 
2025-10-09 12:28:29.768904:  
2025-10-09 12:28:29.769118: Epoch 40 
2025-10-09 12:28:29.769232: Current learning rate: 0.00818 
2025-10-09 12:31:09.951119: train_loss -0.8347 
2025-10-09 12:31:09.951464: val_loss -0.6817 
2025-10-09 12:31:09.951567: Pseudo dice [0.7879] 
2025-10-09 12:31:09.951662: Epoch time: 160.18 s 
2025-10-09 12:31:11.363670:  
2025-10-09 12:31:11.363899: Epoch 41 
2025-10-09 12:31:11.364016: Current learning rate: 0.00813 
2025-10-09 12:33:51.468501: train_loss -0.8305 
2025-10-09 12:33:51.468819: val_loss -0.7143 
2025-10-09 12:33:51.468890: Pseudo dice [0.8064] 
2025-10-09 12:33:51.468978: Epoch time: 160.11 s 
2025-10-09 12:33:52.745054:  
2025-10-09 12:33:52.745256: Epoch 42 
2025-10-09 12:33:52.745386: Current learning rate: 0.00809 
2025-10-09 12:36:33.199292: train_loss -0.8343 
2025-10-09 12:36:33.199689: val_loss -0.6904 
2025-10-09 12:36:33.199779: Pseudo dice [0.7955] 
2025-10-09 12:36:33.199866: Epoch time: 160.46 s 
2025-10-09 12:36:35.039118:  
2025-10-09 12:36:35.039350: Epoch 43 
2025-10-09 12:36:35.039520: Current learning rate: 0.00804 
2025-10-09 12:39:15.211610: train_loss -0.8419 
2025-10-09 12:39:15.211952: val_loss -0.7116 
2025-10-09 12:39:15.212026: Pseudo dice [0.8018] 
2025-10-09 12:39:15.212103: Epoch time: 160.17 s 
2025-10-09 12:39:16.545443:  
2025-10-09 12:39:16.545684: Epoch 44 
2025-10-09 12:39:16.545808: Current learning rate: 0.008 
2025-10-09 12:41:56.681204: train_loss -0.845 
2025-10-09 12:41:56.681526: val_loss -0.7244 
2025-10-09 12:41:56.681597: Pseudo dice [0.8145] 
2025-10-09 12:41:56.681693: Epoch time: 160.14 s 
2025-10-09 12:41:58.016555:  
2025-10-09 12:41:58.016877: Epoch 45 
2025-10-09 12:41:58.017009: Current learning rate: 0.00795 
2025-10-09 12:44:38.243037: train_loss -0.839 
2025-10-09 12:44:38.243382: val_loss -0.7023 
2025-10-09 12:44:38.243461: Pseudo dice [0.8053] 
2025-10-09 12:44:38.243554: Epoch time: 160.23 s 
2025-10-09 12:44:39.579579:  
2025-10-09 12:44:39.580003: Epoch 46 
2025-10-09 12:44:39.580184: Current learning rate: 0.0079 
2025-10-09 12:47:20.079312: train_loss -0.8378 
2025-10-09 12:47:20.079668: val_loss -0.6957 
2025-10-09 12:47:20.079746: Pseudo dice [0.797] 
2025-10-09 12:47:20.079829: Epoch time: 160.5 s 
2025-10-09 12:47:21.393636:  
2025-10-09 12:47:21.393986: Epoch 47 
2025-10-09 12:47:21.394130: Current learning rate: 0.00786 
2025-10-09 12:50:01.902089: train_loss -0.8396 
2025-10-09 12:50:01.902734: val_loss -0.7367 
2025-10-09 12:50:01.902812: Pseudo dice [0.821] 
2025-10-09 12:50:01.902895: Epoch time: 160.51 s 
2025-10-09 12:50:03.241791:  
2025-10-09 12:50:03.241993: Epoch 48 
2025-10-09 12:50:03.242100: Current learning rate: 0.00781 
2025-10-09 12:52:43.410794: train_loss -0.8406 
2025-10-09 12:52:43.411119: val_loss -0.7223 
2025-10-09 12:52:43.411193: Pseudo dice [0.8121] 
2025-10-09 12:52:43.411274: Epoch time: 160.17 s 
2025-10-09 12:52:44.787270:  
2025-10-09 12:52:44.787549: Epoch 49 
2025-10-09 12:52:44.787783: Current learning rate: 0.00777 
2025-10-09 12:55:25.178535: train_loss -0.8436 
2025-10-09 12:55:25.178883: val_loss -0.7203 
2025-10-09 12:55:25.178956: Pseudo dice [0.8125] 
2025-10-09 12:55:25.179038: Epoch time: 160.39 s 
2025-10-09 12:55:26.878287:  
2025-10-09 12:55:26.878492: Epoch 50 
2025-10-09 12:55:26.878602: Current learning rate: 0.00772 
2025-10-09 12:58:06.909751: train_loss -0.8426 
2025-10-09 12:58:06.910082: val_loss -0.675 
2025-10-09 12:58:06.910153: Pseudo dice [0.7876] 
2025-10-09 12:58:06.910229: Epoch time: 160.03 s 
2025-10-09 12:58:08.234090:  
2025-10-09 12:58:08.234298: Epoch 51 
2025-10-09 12:58:08.234405: Current learning rate: 0.00767 
2025-10-09 13:00:48.075204: train_loss -0.848 
2025-10-09 13:00:48.075510: val_loss -0.7358 
2025-10-09 13:00:48.075581: Pseudo dice [0.822] 
2025-10-09 13:00:48.075670: Epoch time: 159.84 s 
2025-10-09 13:00:49.350671:  
2025-10-09 13:00:49.350859: Epoch 52 
2025-10-09 13:00:49.350966: Current learning rate: 0.00763 
2025-10-09 13:03:29.126815: train_loss -0.847 
2025-10-09 13:03:29.127208: val_loss -0.7106 
2025-10-09 13:03:29.127298: Pseudo dice [0.8087] 
2025-10-09 13:03:29.127402: Epoch time: 159.78 s 
2025-10-09 13:03:30.466412:  
2025-10-09 13:03:30.466660: Epoch 53 
2025-10-09 13:03:30.466789: Current learning rate: 0.00758 
2025-10-09 13:06:10.573982: train_loss -0.8462 
2025-10-09 13:06:10.574366: val_loss -0.7423 
2025-10-09 13:06:10.574441: Pseudo dice [0.8247] 
2025-10-09 13:06:10.574544: Epoch time: 160.11 s 
2025-10-09 13:06:11.942121:  
2025-10-09 13:06:11.942351: Epoch 54 
2025-10-09 13:06:11.942471: Current learning rate: 0.00753 
2025-10-09 13:08:52.036508: train_loss -0.8478 
2025-10-09 13:08:52.036862: val_loss -0.738 
2025-10-09 13:08:52.036935: Pseudo dice [0.8223] 
2025-10-09 13:08:52.037018: Epoch time: 160.1 s 
2025-10-09 13:08:52.037077: Yayy! New best EMA pseudo Dice: 0.8103 
2025-10-09 13:08:54.399905:  
2025-10-09 13:08:54.400233: Epoch 55 
2025-10-09 13:08:54.400401: Current learning rate: 0.00749 
2025-10-09 13:11:34.687364: train_loss -0.8491 
2025-10-09 13:11:34.697932: val_loss -0.7515 
2025-10-09 13:11:34.698233: Pseudo dice [0.8316] 
2025-10-09 13:11:34.698417: Epoch time: 160.29 s 
2025-10-09 13:11:34.698496: Yayy! New best EMA pseudo Dice: 0.8125 
2025-10-09 13:11:36.595732:  
2025-10-09 13:11:36.595972: Epoch 56 
2025-10-09 13:11:36.596083: Current learning rate: 0.00744 
2025-10-09 13:14:16.623750: train_loss -0.8494 
2025-10-09 13:14:16.624134: val_loss -0.737 
2025-10-09 13:14:16.624212: Pseudo dice [0.8216] 
2025-10-09 13:14:16.624301: Epoch time: 160.03 s 
2025-10-09 13:14:16.624365: Yayy! New best EMA pseudo Dice: 0.8134 
2025-10-09 13:14:18.588892:  
2025-10-09 13:14:18.589113: Epoch 57 
2025-10-09 13:14:18.589241: Current learning rate: 0.00739 
2025-10-09 13:16:58.840044: train_loss -0.8516 
2025-10-09 13:16:58.840378: val_loss -0.7308 
2025-10-09 13:16:58.840450: Pseudo dice [0.8203] 
2025-10-09 13:16:58.840535: Epoch time: 160.25 s 
2025-10-09 13:16:58.840596: Yayy! New best EMA pseudo Dice: 0.8141 
2025-10-09 13:17:00.647786:  
2025-10-09 13:17:00.648120: Epoch 58 
2025-10-09 13:17:00.648314: Current learning rate: 0.00735 
2025-10-09 13:19:40.911658: train_loss -0.8537 
2025-10-09 13:19:40.912030: val_loss -0.729 
2025-10-09 13:19:40.912103: Pseudo dice [0.8206] 
2025-10-09 13:19:40.912184: Epoch time: 160.27 s 
2025-10-09 13:19:40.912246: Yayy! New best EMA pseudo Dice: 0.8147 
2025-10-09 13:19:42.824693:  
2025-10-09 13:19:42.824922: Epoch 59 
2025-10-09 13:19:42.825031: Current learning rate: 0.0073 
2025-10-09 13:22:23.131757: train_loss -0.8567 
2025-10-09 13:22:23.132121: val_loss -0.7394 
2025-10-09 13:22:23.132198: Pseudo dice [0.8237] 
2025-10-09 13:22:23.132280: Epoch time: 160.31 s 
2025-10-09 13:22:23.132344: Yayy! New best EMA pseudo Dice: 0.8156 
2025-10-09 13:22:24.954468:  
2025-10-09 13:22:24.954711: Epoch 60 
2025-10-09 13:22:24.954821: Current learning rate: 0.00725 
2025-10-09 13:25:05.050465: train_loss -0.8567 
2025-10-09 13:25:05.050833: val_loss -0.7418 
2025-10-09 13:25:05.050921: Pseudo dice [0.8213] 
2025-10-09 13:25:05.051057: Epoch time: 160.1 s 
2025-10-09 13:25:05.051131: Yayy! New best EMA pseudo Dice: 0.8162 
2025-10-09 13:25:06.974904:  
2025-10-09 13:25:06.975294: Epoch 61 
2025-10-09 13:25:06.975413: Current learning rate: 0.00721 
2025-10-09 13:27:47.142541: train_loss -0.8533 
2025-10-09 13:27:47.142885: val_loss -0.7448 
2025-10-09 13:27:47.142963: Pseudo dice [0.8256] 
2025-10-09 13:27:47.143051: Epoch time: 160.17 s 
2025-10-09 13:27:47.143136: Yayy! New best EMA pseudo Dice: 0.8171 
2025-10-09 13:27:49.012506:  
2025-10-09 13:27:49.012769: Epoch 62 
2025-10-09 13:27:49.012948: Current learning rate: 0.00716 
2025-10-09 13:30:28.982402: train_loss -0.8569 
2025-10-09 13:30:28.982778: val_loss -0.7185 
2025-10-09 13:30:28.982854: Pseudo dice [0.8087] 
2025-10-09 13:30:28.982956: Epoch time: 159.97 s 
2025-10-09 13:30:30.396539:  
2025-10-09 13:30:30.396877: Epoch 63 
2025-10-09 13:30:30.397028: Current learning rate: 0.00711 
2025-10-09 13:33:10.418875: train_loss -0.8589 
2025-10-09 13:33:10.419223: val_loss -0.726 
2025-10-09 13:33:10.419306: Pseudo dice [0.8145] 
2025-10-09 13:33:10.419389: Epoch time: 160.02 s 
2025-10-09 13:33:11.808486:  
2025-10-09 13:33:11.808682: Epoch 64 
2025-10-09 13:33:11.808834: Current learning rate: 0.00707 
2025-10-09 13:35:52.001629: train_loss -0.8555 
2025-10-09 13:35:52.001995: val_loss -0.7256 
2025-10-09 13:35:52.002071: Pseudo dice [0.8219] 
2025-10-09 13:35:52.002154: Epoch time: 160.19 s 
2025-10-09 13:35:53.358083:  
2025-10-09 13:35:53.358329: Epoch 65 
2025-10-09 13:35:53.358437: Current learning rate: 0.00702 
2025-10-09 13:38:33.575585: train_loss -0.8593 
2025-10-09 13:38:33.575924: val_loss -0.7292 
2025-10-09 13:38:33.575998: Pseudo dice [0.816] 
2025-10-09 13:38:33.576078: Epoch time: 160.22 s 
2025-10-09 13:38:35.393308:  
2025-10-09 13:38:35.393574: Epoch 66 
2025-10-09 13:38:35.393707: Current learning rate: 0.00697 
2025-10-09 13:41:15.503398: train_loss -0.8605 
2025-10-09 13:41:15.503744: val_loss -0.7362 
2025-10-09 13:41:15.503819: Pseudo dice [0.8208] 
2025-10-09 13:41:15.503899: Epoch time: 160.11 s 
2025-10-09 13:41:16.866587:  
2025-10-09 13:41:16.866855: Epoch 67 
2025-10-09 13:41:16.866968: Current learning rate: 0.00693 
2025-10-09 13:43:57.125239: train_loss -0.8615 
2025-10-09 13:43:57.125591: val_loss -0.7523 
2025-10-09 13:43:57.125738: Pseudo dice [0.8338] 
2025-10-09 13:43:57.125829: Epoch time: 160.26 s 
2025-10-09 13:43:57.125898: Yayy! New best EMA pseudo Dice: 0.8187 
2025-10-09 13:43:59.093120:  
2025-10-09 13:43:59.093396: Epoch 68 
2025-10-09 13:43:59.093532: Current learning rate: 0.00688 
2025-10-09 13:46:39.522233: train_loss -0.8586 
2025-10-09 13:46:39.522581: val_loss -0.754 
2025-10-09 13:46:39.522666: Pseudo dice [0.8322] 
2025-10-09 13:46:39.522750: Epoch time: 160.43 s 
2025-10-09 13:46:39.522809: Yayy! New best EMA pseudo Dice: 0.8201 
2025-10-09 13:46:41.339744:  
2025-10-09 13:46:41.339987: Epoch 69 
2025-10-09 13:46:41.340098: Current learning rate: 0.00683 
2025-10-09 13:49:21.437870: train_loss -0.8639 
2025-10-09 13:49:21.438200: val_loss -0.7372 
2025-10-09 13:49:21.438277: Pseudo dice [0.8225] 
2025-10-09 13:49:21.438359: Epoch time: 160.1 s 
2025-10-09 13:49:21.438731: Yayy! New best EMA pseudo Dice: 0.8203 
2025-10-09 13:49:23.525600:  
2025-10-09 13:49:23.525942: Epoch 70 
2025-10-09 13:49:23.526075: Current learning rate: 0.00679 
2025-10-09 13:52:03.731703: train_loss -0.8621 
2025-10-09 13:52:03.732040: val_loss -0.7279 
2025-10-09 13:52:03.732113: Pseudo dice [0.8178] 
2025-10-09 13:52:03.732193: Epoch time: 160.21 s 
2025-10-09 13:52:05.166936:  
2025-10-09 13:52:05.167212: Epoch 71 
2025-10-09 13:52:05.167322: Current learning rate: 0.00674 
2025-10-09 13:54:45.472128: train_loss -0.8617 
2025-10-09 13:54:45.472465: val_loss -0.7456 
2025-10-09 13:54:45.472542: Pseudo dice [0.8257] 
2025-10-09 13:54:45.472624: Epoch time: 160.31 s 
2025-10-09 13:54:45.472702: Yayy! New best EMA pseudo Dice: 0.8206 
2025-10-09 13:54:47.465778:  
2025-10-09 13:54:47.466013: Epoch 72 
2025-10-09 13:54:47.466133: Current learning rate: 0.00669 
2025-10-09 13:57:27.736208: train_loss -0.8643 
2025-10-09 13:57:27.736729: val_loss -0.7512 
2025-10-09 13:57:27.736809: Pseudo dice [0.8305] 
2025-10-09 13:57:27.736894: Epoch time: 160.27 s 
2025-10-09 13:57:27.736959: Yayy! New best EMA pseudo Dice: 0.8216 
2025-10-09 13:57:29.740091:  
2025-10-09 13:57:29.740293: Epoch 73 
2025-10-09 13:57:29.740453: Current learning rate: 0.00665 
2025-10-09 14:00:10.041665: train_loss -0.8625 
2025-10-09 14:00:10.042059: val_loss -0.729 
2025-10-09 14:00:10.042137: Pseudo dice [0.8171] 
2025-10-09 14:00:10.042223: Epoch time: 160.3 s 
2025-10-09 14:00:11.485423:  
2025-10-09 14:00:11.485625: Epoch 74 
2025-10-09 14:00:11.485774: Current learning rate: 0.0066 
2025-10-09 14:02:52.012529: train_loss -0.8646 
2025-10-09 14:02:52.012875: val_loss -0.7007 
2025-10-09 14:02:52.012961: Pseudo dice [0.8042] 
2025-10-09 14:02:52.013054: Epoch time: 160.53 s 
2025-10-09 14:02:53.441626:  
2025-10-09 14:02:53.442073: Epoch 75 
2025-10-09 14:02:53.442191: Current learning rate: 0.00655 
2025-10-09 14:05:33.625302: train_loss -0.8614 
2025-10-09 14:05:33.625661: val_loss -0.7142 
2025-10-09 14:05:33.625745: Pseudo dice [0.8132] 
2025-10-09 14:05:33.625831: Epoch time: 160.18 s 
2025-10-09 14:05:35.018956:  
2025-10-09 14:05:35.019213: Epoch 76 
2025-10-09 14:05:35.019351: Current learning rate: 0.0065 
2025-10-09 14:08:15.337010: train_loss -0.8633 
2025-10-09 14:08:15.337353: val_loss -0.7416 
2025-10-09 14:08:15.337427: Pseudo dice [0.8258] 
2025-10-09 14:08:15.337508: Epoch time: 160.32 s 
2025-10-09 14:08:17.253587:  
2025-10-09 14:08:17.253902: Epoch 77 
2025-10-09 14:08:17.254021: Current learning rate: 0.00646 
2025-10-09 14:10:57.284342: train_loss -0.8662 
2025-10-09 14:10:57.284681: val_loss -0.7257 
2025-10-09 14:10:57.284758: Pseudo dice [0.8153] 
2025-10-09 14:10:57.284835: Epoch time: 160.03 s 
2025-10-09 14:10:58.731953:  
2025-10-09 14:10:58.732177: Epoch 78 
2025-10-09 14:10:58.732285: Current learning rate: 0.00641 
2025-10-09 14:13:38.967899: train_loss -0.8618 
2025-10-09 14:13:38.968215: val_loss -0.7117 
2025-10-09 14:13:38.968286: Pseudo dice [0.806] 
2025-10-09 14:13:38.968363: Epoch time: 160.24 s 
2025-10-09 14:13:40.351246:  
2025-10-09 14:13:40.351564: Epoch 79 
2025-10-09 14:13:40.351701: Current learning rate: 0.00636 
2025-10-09 14:16:20.562935: train_loss -0.8706 
2025-10-09 14:16:20.563295: val_loss -0.7462 
2025-10-09 14:16:20.563384: Pseudo dice [0.8276] 
2025-10-09 14:16:20.563475: Epoch time: 160.21 s 
2025-10-09 14:16:22.022741:  
2025-10-09 14:16:22.022979: Epoch 80 
2025-10-09 14:16:22.023089: Current learning rate: 0.00631 
2025-10-09 14:19:02.264902: train_loss -0.8699 
2025-10-09 14:19:02.265249: val_loss -0.7322 
2025-10-09 14:19:02.265449: Pseudo dice [0.8232] 
2025-10-09 14:19:02.265541: Epoch time: 160.24 s 
2025-10-09 14:19:03.693434:  
2025-10-09 14:19:03.693686: Epoch 81 
2025-10-09 14:19:03.693800: Current learning rate: 0.00627 
2025-10-09 14:21:43.737533: train_loss -0.8646 
2025-10-09 14:21:43.737915: val_loss -0.7162 
2025-10-09 14:21:43.738003: Pseudo dice [0.8087] 
2025-10-09 14:21:43.738086: Epoch time: 160.05 s 
2025-10-09 14:21:45.201343:  
2025-10-09 14:21:45.201717: Epoch 82 
2025-10-09 14:21:45.201899: Current learning rate: 0.00622 
2025-10-09 14:24:25.455763: train_loss -0.8667 
2025-10-09 14:24:25.456093: val_loss -0.7386 
2025-10-09 14:24:25.456166: Pseudo dice [0.8246] 
2025-10-09 14:24:25.456252: Epoch time: 160.26 s 
2025-10-09 14:24:26.811920:  
2025-10-09 14:24:26.812153: Epoch 83 
2025-10-09 14:24:26.812264: Current learning rate: 0.00617 
2025-10-09 14:27:07.157721: train_loss -0.8661 
2025-10-09 14:27:07.158075: val_loss -0.7185 
2025-10-09 14:27:07.158154: Pseudo dice [0.8137] 
2025-10-09 14:27:07.158277: Epoch time: 160.35 s 
2025-10-09 14:27:08.529796:  
2025-10-09 14:27:08.530071: Epoch 84 
2025-10-09 14:27:08.530195: Current learning rate: 0.00612 
2025-10-09 14:29:48.989046: train_loss -0.869 
2025-10-09 14:29:48.989364: val_loss -0.7315 
2025-10-09 14:29:48.989437: Pseudo dice [0.8206] 
2025-10-09 14:29:48.989522: Epoch time: 160.46 s 
2025-10-09 14:29:50.381148:  
2025-10-09 14:29:50.381368: Epoch 85 
2025-10-09 14:29:50.381485: Current learning rate: 0.00608 
2025-10-09 14:32:30.612442: train_loss -0.8721 
2025-10-09 14:32:30.612805: val_loss -0.7411 
2025-10-09 14:32:30.612878: Pseudo dice [0.8265] 
2025-10-09 14:32:30.612962: Epoch time: 160.23 s 
2025-10-09 14:32:31.954103:  
2025-10-09 14:32:31.954401: Epoch 86 
2025-10-09 14:32:31.954523: Current learning rate: 0.00603 
2025-10-09 14:35:12.202044: train_loss -0.8726 
2025-10-09 14:35:12.202374: val_loss -0.7312 
2025-10-09 14:35:12.202449: Pseudo dice [0.8219] 
2025-10-09 14:35:12.202531: Epoch time: 160.25 s 
2025-10-09 14:35:13.581789:  
2025-10-09 14:35:13.582114: Epoch 87 
2025-10-09 14:35:13.582311: Current learning rate: 0.00598 
2025-10-09 14:37:53.705640: train_loss -0.8728 
2025-10-09 14:37:53.706025: val_loss -0.73 
2025-10-09 14:37:53.706158: Pseudo dice [0.8219] 
2025-10-09 14:37:53.706262: Epoch time: 160.13 s 
2025-10-09 14:37:55.074990:  
2025-10-09 14:37:55.075246: Epoch 88 
2025-10-09 14:37:55.075364: Current learning rate: 0.00593 
2025-10-09 14:40:35.257629: train_loss -0.8697 
2025-10-09 14:40:35.258020: val_loss -0.7142 
2025-10-09 14:40:35.258100: Pseudo dice [0.8129] 
2025-10-09 14:40:35.258235: Epoch time: 160.18 s 
2025-10-09 14:40:37.156568:  
2025-10-09 14:40:37.156918: Epoch 89 
2025-10-09 14:40:37.157053: Current learning rate: 0.00589 
2025-10-09 14:43:17.274197: train_loss -0.8768 
2025-10-09 14:43:17.274547: val_loss -0.7001 
2025-10-09 14:43:17.274622: Pseudo dice [0.8027] 
2025-10-09 14:43:17.274721: Epoch time: 160.12 s 
2025-10-09 14:43:18.629390:  
2025-10-09 14:43:18.629668: Epoch 90 
2025-10-09 14:43:18.629785: Current learning rate: 0.00584 
2025-10-09 14:45:58.845785: train_loss -0.8722 
2025-10-09 14:45:58.846274: val_loss -0.7229 
2025-10-09 14:45:58.846358: Pseudo dice [0.8137] 
2025-10-09 14:45:58.846450: Epoch time: 160.22 s 
2025-10-09 14:46:00.209330:  
2025-10-09 14:46:00.209600: Epoch 91 
2025-10-09 14:46:00.209726: Current learning rate: 0.00579 
2025-10-09 14:48:40.468459: train_loss -0.8741 
2025-10-09 14:48:40.468900: val_loss -0.726 
2025-10-09 14:48:40.468998: Pseudo dice [0.8175] 
2025-10-09 14:48:40.469110: Epoch time: 160.26 s 
2025-10-09 14:48:41.804556:  
2025-10-09 14:48:41.804883: Epoch 92 
2025-10-09 14:48:41.804992: Current learning rate: 0.00574 
2025-10-09 14:51:22.148737: train_loss -0.8727 
2025-10-09 14:51:22.149066: val_loss -0.7166 
2025-10-09 14:51:22.149138: Pseudo dice [0.8133] 
2025-10-09 14:51:22.149220: Epoch time: 160.35 s 
2025-10-09 14:51:23.519809:  
2025-10-09 14:51:23.520169: Epoch 93 
2025-10-09 14:51:23.520291: Current learning rate: 0.0057 
2025-10-09 14:54:03.699167: train_loss -0.872 
2025-10-09 14:54:03.699496: val_loss -0.7036 
2025-10-09 14:54:03.699565: Pseudo dice [0.8039] 
2025-10-09 14:54:03.699655: Epoch time: 160.18 s 
2025-10-09 14:54:05.016608:  
2025-10-09 14:54:05.016989: Epoch 94 
2025-10-09 14:54:05.017109: Current learning rate: 0.00565 
2025-10-09 14:56:45.197852: train_loss -0.8792 
2025-10-09 14:56:45.198200: val_loss -0.7146 
2025-10-09 14:56:45.198280: Pseudo dice [0.8148] 
2025-10-09 14:56:45.198372: Epoch time: 160.18 s 
2025-10-09 14:56:46.581026:  
2025-10-09 14:56:46.581371: Epoch 95 
2025-10-09 14:56:46.581511: Current learning rate: 0.0056 
2025-10-09 14:59:26.719058: train_loss -0.8757 
2025-10-09 14:59:26.719432: val_loss -0.7253 
2025-10-09 14:59:26.719525: Pseudo dice [0.8202] 
2025-10-09 14:59:26.719609: Epoch time: 160.14 s 
2025-10-09 14:59:28.137027:  
2025-10-09 14:59:28.137289: Epoch 96 
2025-10-09 14:59:28.137435: Current learning rate: 0.00555 
2025-10-09 15:02:08.232015: train_loss -0.8752 
2025-10-09 15:02:08.232384: val_loss -0.7149 
2025-10-09 15:02:08.232473: Pseudo dice [0.8118] 
2025-10-09 15:02:08.232574: Epoch time: 160.1 s 
2025-10-09 15:02:09.574465:  
2025-10-09 15:02:09.574724: Epoch 97 
2025-10-09 15:02:09.574859: Current learning rate: 0.0055 
2025-10-09 15:04:49.690657: train_loss -0.8748 
2025-10-09 15:04:49.690968: val_loss -0.7259 
2025-10-09 15:04:49.691036: Pseudo dice [0.8212] 
2025-10-09 15:04:49.691113: Epoch time: 160.12 s 
2025-10-09 15:04:50.982622:  
2025-10-09 15:04:50.982847: Epoch 98 
2025-10-09 15:04:50.982961: Current learning rate: 0.00546 
2025-10-09 15:07:31.008656: train_loss -0.8821 
2025-10-09 15:07:31.008963: val_loss -0.7164 
2025-10-09 15:07:31.009031: Pseudo dice [0.8125] 
2025-10-09 15:07:31.009111: Epoch time: 160.03 s 
2025-10-09 15:07:32.278550:  
2025-10-09 15:07:32.278762: Epoch 99 
2025-10-09 15:07:32.278876: Current learning rate: 0.00541 
2025-10-09 15:10:12.358853: train_loss -0.8754 
2025-10-09 15:10:12.359185: val_loss -0.7326 
2025-10-09 15:10:12.359259: Pseudo dice [0.8207] 
2025-10-09 15:10:12.359340: Epoch time: 160.08 s 
2025-10-09 15:10:14.792153:  
2025-10-09 15:10:14.792382: Epoch 100 
2025-10-09 15:10:14.792506: Current learning rate: 0.00536 
2025-10-09 15:12:54.963378: train_loss -0.8754 
2025-10-09 15:12:54.963755: val_loss -0.7319 
2025-10-09 15:12:54.963829: Pseudo dice [0.8198] 
2025-10-09 15:12:54.963913: Epoch time: 160.17 s 
2025-10-09 15:12:56.299604:  
2025-10-09 15:12:56.299909: Epoch 101 
2025-10-09 15:12:56.300027: Current learning rate: 0.00531 
2025-10-09 15:15:36.235122: train_loss -0.8787 
2025-10-09 15:15:36.235522: val_loss -0.7383 
2025-10-09 15:15:36.235670: Pseudo dice [0.8252] 
2025-10-09 15:15:36.235771: Epoch time: 159.94 s 
2025-10-09 15:15:37.639629:  
2025-10-09 15:15:37.639892: Epoch 102 
2025-10-09 15:15:37.640022: Current learning rate: 0.00526 
2025-10-09 15:18:17.649230: train_loss -0.8791 
2025-10-09 15:18:17.649583: val_loss -0.7238 
2025-10-09 15:18:17.649678: Pseudo dice [0.8186] 
2025-10-09 15:18:17.649775: Epoch time: 160.01 s 
2025-10-09 15:18:19.027109:  
2025-10-09 15:18:19.027486: Epoch 103 
2025-10-09 15:18:19.027617: Current learning rate: 0.00521 
2025-10-09 15:20:59.233898: train_loss -0.8811 
2025-10-09 15:20:59.234246: val_loss -0.717 
2025-10-09 15:20:59.234317: Pseudo dice [0.8148] 
2025-10-09 15:20:59.234396: Epoch time: 160.21 s 
2025-10-09 15:21:00.592574:  
2025-10-09 15:21:00.592855: Epoch 104 
2025-10-09 15:21:00.592970: Current learning rate: 0.00517 
2025-10-09 15:23:40.603144: train_loss -0.8812 
2025-10-09 15:23:40.603499: val_loss -0.7144 
2025-10-09 15:23:40.603591: Pseudo dice [0.8151] 
2025-10-09 15:23:40.603725: Epoch time: 160.01 s 
2025-10-09 15:23:41.968941:  
2025-10-09 15:23:41.969205: Epoch 105 
2025-10-09 15:23:41.969322: Current learning rate: 0.00512 
2025-10-09 15:26:22.203175: train_loss -0.8817 
2025-10-09 15:26:22.203536: val_loss -0.7152 
2025-10-09 15:26:22.203613: Pseudo dice [0.8126] 
2025-10-09 15:26:22.203715: Epoch time: 160.24 s 
2025-10-09 15:26:23.576926:  
2025-10-09 15:26:23.577375: Epoch 106 
2025-10-09 15:26:23.577520: Current learning rate: 0.00507 
2025-10-09 15:29:03.941900: train_loss -0.8807 
2025-10-09 15:29:03.942317: val_loss -0.7384 
2025-10-09 15:29:03.942418: Pseudo dice [0.8262] 
2025-10-09 15:29:03.942531: Epoch time: 160.37 s 
2025-10-09 15:29:05.368919:  
2025-10-09 15:29:05.369263: Epoch 107 
2025-10-09 15:29:05.369406: Current learning rate: 0.00502 
2025-10-09 15:31:45.567442: train_loss -0.883 
2025-10-09 15:31:45.567804: val_loss -0.7188 
2025-10-09 15:31:45.567876: Pseudo dice [0.8153] 
2025-10-09 15:31:45.567958: Epoch time: 160.2 s 
2025-10-09 15:31:46.931212:  
2025-10-09 15:31:46.931564: Epoch 108 
2025-10-09 15:31:46.931703: Current learning rate: 0.00497 
2025-10-09 15:34:27.459354: train_loss -0.8803 
2025-10-09 15:34:27.459705: val_loss -0.7397 
2025-10-09 15:34:27.459794: Pseudo dice [0.8276] 
2025-10-09 15:34:27.459894: Epoch time: 160.53 s 
2025-10-09 15:34:28.831290:  
2025-10-09 15:34:28.831580: Epoch 109 
2025-10-09 15:34:28.831723: Current learning rate: 0.00492 
2025-10-09 15:37:09.126793: train_loss -0.8832 
2025-10-09 15:37:09.127150: val_loss -0.7429 
2025-10-09 15:37:09.127228: Pseudo dice [0.8291] 
2025-10-09 15:37:09.127310: Epoch time: 160.3 s 
2025-10-09 15:37:10.530131:  
2025-10-09 15:37:10.530360: Epoch 110 
2025-10-09 15:37:10.530488: Current learning rate: 0.00487 
2025-10-09 15:39:50.678517: train_loss -0.8823 
2025-10-09 15:39:50.678894: val_loss -0.7145 
2025-10-09 15:39:50.678995: Pseudo dice [0.8165] 
2025-10-09 15:39:50.679078: Epoch time: 160.15 s 
2025-10-09 15:39:52.034206:  
2025-10-09 15:39:52.034414: Epoch 111 
2025-10-09 15:39:52.034528: Current learning rate: 0.00483 
2025-10-09 15:42:32.260877: train_loss -0.8825 
2025-10-09 15:42:32.261242: val_loss -0.7316 
2025-10-09 15:42:32.261331: Pseudo dice [0.8218] 
2025-10-09 15:42:32.261435: Epoch time: 160.23 s 
2025-10-09 15:42:34.128414:  
2025-10-09 15:42:34.128712: Epoch 112 
2025-10-09 15:42:34.128843: Current learning rate: 0.00478 
2025-10-09 15:45:14.514993: train_loss -0.8839 
2025-10-09 15:45:14.515323: val_loss -0.7251 
2025-10-09 15:45:14.515398: Pseudo dice [0.8197] 
2025-10-09 15:45:14.515481: Epoch time: 160.39 s 
2025-10-09 15:45:15.965509:  
2025-10-09 15:45:15.965793: Epoch 113 
2025-10-09 15:45:15.965913: Current learning rate: 0.00473 
2025-10-09 15:47:56.092729: train_loss -0.883 
2025-10-09 15:47:56.093077: val_loss -0.7213 
2025-10-09 15:47:56.093152: Pseudo dice [0.8153] 
2025-10-09 15:47:56.093233: Epoch time: 160.13 s 
2025-10-09 15:47:57.449373:  
2025-10-09 15:47:57.449613: Epoch 114 
2025-10-09 15:47:57.449733: Current learning rate: 0.00468 
2025-10-09 15:50:37.531841: train_loss -0.8859 
2025-10-09 15:50:37.532221: val_loss -0.7147 
2025-10-09 15:50:37.532294: Pseudo dice [0.8123] 
2025-10-09 15:50:37.532377: Epoch time: 160.08 s 
2025-10-09 15:50:38.889782:  
2025-10-09 15:50:38.890144: Epoch 115 
2025-10-09 15:50:38.890268: Current learning rate: 0.00463 
2025-10-09 15:53:19.128438: train_loss -0.8826 
2025-10-09 15:53:19.128811: val_loss -0.7258 
2025-10-09 15:53:19.128904: Pseudo dice [0.8194] 
2025-10-09 15:53:19.128998: Epoch time: 160.24 s 
2025-10-09 15:53:20.511905:  
2025-10-09 15:53:20.512158: Epoch 116 
2025-10-09 15:53:20.512270: Current learning rate: 0.00458 
2025-10-09 15:56:00.646132: train_loss -0.8843 
2025-10-09 15:56:00.646470: val_loss -0.7157 
2025-10-09 15:56:00.646545: Pseudo dice [0.8132] 
2025-10-09 15:56:00.646626: Epoch time: 160.14 s 
2025-10-09 15:56:01.999633:  
2025-10-09 15:56:01.999950: Epoch 117 
2025-10-09 15:56:02.000094: Current learning rate: 0.00453 
2025-10-09 15:58:42.226161: train_loss -0.8868 
2025-10-09 15:58:42.226485: val_loss -0.7431 
2025-10-09 15:58:42.226562: Pseudo dice [0.8289] 
2025-10-09 15:58:42.226642: Epoch time: 160.23 s 
2025-10-09 15:58:43.585939:  
2025-10-09 15:58:43.586183: Epoch 118 
2025-10-09 15:58:43.586294: Current learning rate: 0.00448 
2025-10-09 16:01:23.871030: train_loss -0.8852 
2025-10-09 16:01:23.871370: val_loss -0.709 
2025-10-09 16:01:23.871439: Pseudo dice [0.8127] 
2025-10-09 16:01:23.871517: Epoch time: 160.29 s 
2025-10-09 16:01:25.258775:  
2025-10-09 16:01:25.259040: Epoch 119 
2025-10-09 16:01:25.259166: Current learning rate: 0.00443 
2025-10-09 16:04:05.469833: train_loss -0.8833 
2025-10-09 16:04:05.470163: val_loss -0.7297 
2025-10-09 16:04:05.470256: Pseudo dice [0.8234] 
2025-10-09 16:04:05.470362: Epoch time: 160.21 s 
2025-10-09 16:04:06.980305:  
2025-10-09 16:04:06.980523: Epoch 120 
2025-10-09 16:04:06.980632: Current learning rate: 0.00438 
2025-10-09 16:06:47.004984: train_loss -0.8877 
2025-10-09 16:06:47.005361: val_loss -0.7151 
2025-10-09 16:06:47.005443: Pseudo dice [0.8124] 
2025-10-09 16:06:47.005526: Epoch time: 160.03 s 
2025-10-09 16:06:48.380343:  
2025-10-09 16:06:48.380543: Epoch 121 
2025-10-09 16:06:48.397714: Current learning rate: 0.00433 
2025-10-09 16:09:28.591014: train_loss -0.8891 
2025-10-09 16:09:28.591398: val_loss -0.7339 
2025-10-09 16:09:28.591465: Pseudo dice [0.8236] 
2025-10-09 16:09:28.591545: Epoch time: 160.21 s 
2025-10-09 16:09:29.987484:  
2025-10-09 16:09:29.987766: Epoch 122 
2025-10-09 16:09:29.987927: Current learning rate: 0.00429 
2025-10-09 16:12:10.196248: train_loss -0.8875 
2025-10-09 16:12:10.196844: val_loss -0.7199 
2025-10-09 16:12:10.196918: Pseudo dice [0.8164] 
2025-10-09 16:12:10.197000: Epoch time: 160.21 s 
2025-10-09 16:12:12.080008:  
2025-10-09 16:12:12.080229: Epoch 123 
2025-10-09 16:12:12.080354: Current learning rate: 0.00424 
2025-10-09 16:14:52.222677: train_loss -0.8872 
2025-10-09 16:14:52.223044: val_loss -0.7201 
2025-10-09 16:14:52.223130: Pseudo dice [0.8138] 
2025-10-09 16:14:52.223216: Epoch time: 160.14 s 
2025-10-09 16:14:53.568250:  
2025-10-09 16:14:53.568549: Epoch 124 
2025-10-09 16:14:53.568677: Current learning rate: 0.00419 
2025-10-09 16:17:33.643925: train_loss -0.8901 
2025-10-09 16:17:33.644289: val_loss -0.7115 
2025-10-09 16:17:33.644361: Pseudo dice [0.813] 
2025-10-09 16:17:33.644440: Epoch time: 160.08 s 
2025-10-09 16:17:34.990523:  
2025-10-09 16:17:34.990752: Epoch 125 
2025-10-09 16:17:34.990860: Current learning rate: 0.00414 
2025-10-09 16:20:15.209409: train_loss -0.8884 
2025-10-09 16:20:15.209790: val_loss -0.7312 
2025-10-09 16:20:15.209867: Pseudo dice [0.8228] 
2025-10-09 16:20:15.209949: Epoch time: 160.22 s 
2025-10-09 16:20:16.562127:  
2025-10-09 16:20:16.562412: Epoch 126 
2025-10-09 16:20:16.562529: Current learning rate: 0.00409 
2025-10-09 16:22:56.588451: train_loss -0.8908 
2025-10-09 16:22:56.588824: val_loss -0.7294 
2025-10-09 16:22:56.588901: Pseudo dice [0.822] 
2025-10-09 16:22:56.588985: Epoch time: 160.03 s 
2025-10-09 16:22:57.969769:  
2025-10-09 16:22:57.970107: Epoch 127 
2025-10-09 16:22:57.970265: Current learning rate: 0.00404 
2025-10-09 16:25:38.145668: train_loss -0.8901 
2025-10-09 16:25:38.146055: val_loss -0.7131 
2025-10-09 16:25:38.146130: Pseudo dice [0.8121] 
2025-10-09 16:25:38.146212: Epoch time: 160.18 s 
2025-10-09 16:25:39.536658:  
2025-10-09 16:25:39.536867: Epoch 128 
2025-10-09 16:25:39.536974: Current learning rate: 0.00399 
2025-10-09 16:28:19.667898: train_loss -0.8952 
2025-10-09 16:28:19.668217: val_loss -0.7045 
2025-10-09 16:28:19.668289: Pseudo dice [0.8033] 
2025-10-09 16:28:19.668368: Epoch time: 160.13 s 
2025-10-09 16:28:21.043390:  
2025-10-09 16:28:21.043687: Epoch 129 
2025-10-09 16:28:21.043844: Current learning rate: 0.00394 
2025-10-09 16:31:01.373569: train_loss -0.8915 
2025-10-09 16:31:01.373944: val_loss -0.717 
2025-10-09 16:31:01.374025: Pseudo dice [0.8187] 
2025-10-09 16:31:01.374105: Epoch time: 160.33 s 
2025-10-09 16:31:02.701841:  
2025-10-09 16:31:02.702103: Epoch 130 
2025-10-09 16:31:02.702220: Current learning rate: 0.00389 
2025-10-09 16:33:42.864988: train_loss -0.8918 
2025-10-09 16:33:42.865370: val_loss -0.721 
2025-10-09 16:33:42.865445: Pseudo dice [0.8186] 
2025-10-09 16:33:42.865528: Epoch time: 160.16 s 
2025-10-09 16:33:44.207565:  
2025-10-09 16:33:44.207834: Epoch 131 
2025-10-09 16:33:44.207947: Current learning rate: 0.00384 
2025-10-09 16:36:24.391641: train_loss -0.8918 
2025-10-09 16:36:24.391984: val_loss -0.7169 
2025-10-09 16:36:24.392051: Pseudo dice [0.8153] 
2025-10-09 16:36:24.392127: Epoch time: 160.19 s 
2025-10-09 16:36:25.739428:  
2025-10-09 16:36:25.739677: Epoch 132 
2025-10-09 16:36:25.739803: Current learning rate: 0.00379 
2025-10-09 16:39:06.034687: train_loss -0.8913 
2025-10-09 16:39:06.035064: val_loss -0.7132 
2025-10-09 16:39:06.035174: Pseudo dice [0.8132] 
2025-10-09 16:39:06.035285: Epoch time: 160.3 s 
2025-10-09 16:39:07.373311:  
2025-10-09 16:39:07.373523: Epoch 133 
2025-10-09 16:39:07.373626: Current learning rate: 0.00374 
2025-10-09 16:41:47.718538: train_loss -0.8935 
2025-10-09 16:41:47.718876: val_loss -0.7176 
2025-10-09 16:41:47.718956: Pseudo dice [0.8141] 
2025-10-09 16:41:47.719032: Epoch time: 160.35 s 
2025-10-09 16:41:49.519353:  
2025-10-09 16:41:49.519599: Epoch 134 
2025-10-09 16:41:49.519715: Current learning rate: 0.00369 
2025-10-09 16:44:29.586395: train_loss -0.895 
2025-10-09 16:44:29.586730: val_loss -0.7111 
2025-10-09 16:44:29.586810: Pseudo dice [0.8138] 
2025-10-09 16:44:29.586889: Epoch time: 160.07 s 
2025-10-09 16:44:30.988079:  
2025-10-09 16:44:30.988316: Epoch 135 
2025-10-09 16:44:30.988447: Current learning rate: 0.00364 
2025-10-09 16:47:11.084101: train_loss -0.8931 
2025-10-09 16:47:11.084448: val_loss -0.7321 
2025-10-09 16:47:11.084572: Pseudo dice [0.8241] 
2025-10-09 16:47:11.084667: Epoch time: 160.1 s 
2025-10-09 16:47:12.430446:  
2025-10-09 16:47:12.430843: Epoch 136 
2025-10-09 16:47:12.430983: Current learning rate: 0.00359 
2025-10-09 16:49:52.906095: train_loss -0.8917 
2025-10-09 16:49:52.906448: val_loss -0.723 
2025-10-09 16:49:52.906518: Pseudo dice [0.8167] 
2025-10-09 16:49:52.906598: Epoch time: 160.48 s 
2025-10-09 16:49:54.262958:  
2025-10-09 16:49:54.263315: Epoch 137 
2025-10-09 16:49:54.263454: Current learning rate: 0.00354 
2025-10-09 16:52:34.568447: train_loss -0.8946 
2025-10-09 16:52:34.568855: val_loss -0.7266 
2025-10-09 16:52:34.568928: Pseudo dice [0.8212] 
2025-10-09 16:52:34.569037: Epoch time: 160.31 s 
2025-10-09 16:52:35.996703:  
2025-10-09 16:52:35.996967: Epoch 138 
2025-10-09 16:52:35.997119: Current learning rate: 0.00349 
2025-10-09 16:55:16.435436: train_loss -0.8965 
2025-10-09 16:55:16.435781: val_loss -0.7125 
2025-10-09 16:55:16.435852: Pseudo dice [0.8151] 
2025-10-09 16:55:16.435931: Epoch time: 160.44 s 
2025-10-09 16:55:17.826611:  
2025-10-09 16:55:17.826987: Epoch 139 
2025-10-09 16:55:17.827109: Current learning rate: 0.00343 
2025-10-09 16:57:58.409193: train_loss -0.8935 
2025-10-09 16:57:58.409561: val_loss -0.7062 
2025-10-09 16:57:58.409632: Pseudo dice [0.8082] 
2025-10-09 16:57:58.409729: Epoch time: 160.58 s 
2025-10-09 16:57:59.776253:  
2025-10-09 16:57:59.776642: Epoch 140 
2025-10-09 16:57:59.776791: Current learning rate: 0.00338 
2025-10-09 17:00:40.354963: train_loss -0.8935 
2025-10-09 17:00:40.355312: val_loss -0.7256 
2025-10-09 17:00:40.355382: Pseudo dice [0.8246] 
2025-10-09 17:00:40.355458: Epoch time: 160.58 s 
2025-10-09 17:00:41.751684:  
2025-10-09 17:00:41.751945: Epoch 141 
2025-10-09 17:00:41.752051: Current learning rate: 0.00333 
2025-10-09 17:03:22.333485: train_loss -0.8947 
2025-10-09 17:03:22.333823: val_loss -0.715 
2025-10-09 17:03:22.333889: Pseudo dice [0.8122] 
2025-10-09 17:03:22.333965: Epoch time: 160.58 s 
2025-10-09 17:03:23.704701:  
2025-10-09 17:03:23.705018: Epoch 142 
2025-10-09 17:03:23.705154: Current learning rate: 0.00328 
2025-10-09 17:06:04.154804: train_loss -0.8951 
2025-10-09 17:06:04.155117: val_loss -0.7334 
2025-10-09 17:06:04.155186: Pseudo dice [0.8273] 
2025-10-09 17:06:04.155264: Epoch time: 160.45 s 
2025-10-09 17:06:05.460064:  
2025-10-09 17:06:05.460257: Epoch 143 
2025-10-09 17:06:05.460393: Current learning rate: 0.00323 
2025-10-09 17:08:45.673918: train_loss -0.8972 
2025-10-09 17:08:45.674201: val_loss -0.7294 
2025-10-09 17:08:45.674268: Pseudo dice [0.8239] 
2025-10-09 17:08:45.674343: Epoch time: 160.22 s 
2025-10-09 17:08:46.931285:  
2025-10-09 17:08:46.931478: Epoch 144 
2025-10-09 17:08:46.931584: Current learning rate: 0.00318 
2025-10-09 17:11:26.320422: train_loss -0.8934 
2025-10-09 17:11:26.320735: val_loss -0.7212 
2025-10-09 17:11:26.320804: Pseudo dice [0.8212] 
2025-10-09 17:11:26.320881: Epoch time: 159.39 s 
2025-10-09 17:11:28.013918:  
2025-10-09 17:11:28.014104: Epoch 145 
2025-10-09 17:11:28.014223: Current learning rate: 0.00313 
2025-10-09 17:14:07.429205: train_loss -0.8974 
2025-10-09 17:14:07.429517: val_loss -0.7118 
2025-10-09 17:14:07.429585: Pseudo dice [0.8135] 
2025-10-09 17:14:07.429665: Epoch time: 159.42 s 
2025-10-09 17:14:08.759292:  
2025-10-09 17:14:08.759485: Epoch 146 
2025-10-09 17:14:08.759589: Current learning rate: 0.00308 
2025-10-09 17:16:48.234102: train_loss -0.8998 
2025-10-09 17:16:48.234410: val_loss -0.7266 
2025-10-09 17:16:48.234478: Pseudo dice [0.8198] 
2025-10-09 17:16:48.234554: Epoch time: 159.48 s 
2025-10-09 17:16:49.507468:  
2025-10-09 17:16:49.507693: Epoch 147 
2025-10-09 17:16:49.508038: Current learning rate: 0.00303 
2025-10-09 17:19:28.993241: train_loss -0.9029 
2025-10-09 17:19:28.993527: val_loss -0.7221 
2025-10-09 17:19:28.993603: Pseudo dice [0.8188] 
2025-10-09 17:19:28.993694: Epoch time: 159.49 s 
2025-10-09 17:19:30.258802:  
2025-10-09 17:19:30.259001: Epoch 148 
2025-10-09 17:19:30.259107: Current learning rate: 0.00297 
2025-10-09 17:22:09.621210: train_loss -0.8976 
2025-10-09 17:22:09.621497: val_loss -0.7356 
2025-10-09 17:22:09.621567: Pseudo dice [0.8245] 
2025-10-09 17:22:09.621643: Epoch time: 159.36 s 
2025-10-09 17:22:10.910841:  
2025-10-09 17:22:10.911056: Epoch 149 
2025-10-09 17:22:10.911160: Current learning rate: 0.00292 
2025-10-09 17:24:50.415795: train_loss -0.8988 
2025-10-09 17:24:50.416088: val_loss -0.7207 
2025-10-09 17:24:50.416173: Pseudo dice [0.8214] 
2025-10-09 17:24:50.416317: Epoch time: 159.51 s 
2025-10-09 17:24:52.044210:  
2025-10-09 17:24:52.044410: Epoch 150 
2025-10-09 17:24:52.044514: Current learning rate: 0.00287 
2025-10-09 17:27:31.703069: train_loss -0.9016 
2025-10-09 17:27:31.703403: val_loss -0.7191 
2025-10-09 17:27:31.703472: Pseudo dice [0.816] 
2025-10-09 17:27:31.703551: Epoch time: 159.66 s 
2025-10-09 17:27:32.962293:  
2025-10-09 17:27:32.962500: Epoch 151 
2025-10-09 17:27:32.962604: Current learning rate: 0.00282 
2025-10-09 17:30:12.494060: train_loss -0.8985 
2025-10-09 17:30:12.494375: val_loss -0.7275 
2025-10-09 17:30:12.494441: Pseudo dice [0.8233] 
2025-10-09 17:30:12.494515: Epoch time: 159.53 s 
2025-10-09 17:30:13.751792:  
2025-10-09 17:30:13.752033: Epoch 152 
2025-10-09 17:30:13.752169: Current learning rate: 0.00277 
2025-10-09 17:32:53.432676: train_loss -0.8998 
2025-10-09 17:32:53.432964: val_loss -0.7169 
2025-10-09 17:32:53.433029: Pseudo dice [0.8185] 
2025-10-09 17:32:53.433105: Epoch time: 159.68 s 
2025-10-09 17:32:54.686172:  
2025-10-09 17:32:54.686358: Epoch 153 
2025-10-09 17:32:54.686459: Current learning rate: 0.00272 
2025-10-09 17:35:34.392044: train_loss -0.8995 
2025-10-09 17:35:34.392335: val_loss -0.7266 
2025-10-09 17:35:34.392399: Pseudo dice [0.8253] 
2025-10-09 17:35:34.392474: Epoch time: 159.71 s 
2025-10-09 17:35:35.668376:  
2025-10-09 17:35:35.668561: Epoch 154 
2025-10-09 17:35:35.668673: Current learning rate: 0.00266 
2025-10-09 17:38:15.948653: train_loss -0.902 
2025-10-09 17:38:15.949003: val_loss -0.7035 
2025-10-09 17:38:15.949090: Pseudo dice [0.8099] 
2025-10-09 17:38:15.949173: Epoch time: 160.28 s 
2025-10-09 17:38:17.432122:  
2025-10-09 17:38:17.432368: Epoch 155 
2025-10-09 17:38:17.432473: Current learning rate: 0.00261 
2025-10-09 17:40:58.511410: train_loss -0.8993 
2025-10-09 17:40:58.511779: val_loss -0.7115 
2025-10-09 17:40:58.511852: Pseudo dice [0.8141] 
2025-10-09 17:40:58.511950: Epoch time: 161.08 s 
2025-10-09 17:41:00.407473:  
2025-10-09 17:41:00.407788: Epoch 156 
2025-10-09 17:41:00.407929: Current learning rate: 0.00256 
2025-10-09 17:43:40.997112: train_loss -0.9 
2025-10-09 17:43:40.997458: val_loss -0.713 
2025-10-09 17:43:40.997532: Pseudo dice [0.8146] 
2025-10-09 17:43:40.997610: Epoch time: 160.59 s 
2025-10-09 17:43:42.374162:  
2025-10-09 17:43:42.374429: Epoch 157 
2025-10-09 17:43:42.374538: Current learning rate: 0.00251 
2025-10-09 17:46:22.511879: train_loss -0.9015 
2025-10-09 17:46:22.512232: val_loss -0.7136 
2025-10-09 17:46:22.512312: Pseudo dice [0.8154] 
2025-10-09 17:46:22.512391: Epoch time: 160.14 s 
2025-10-09 17:46:23.855066:  
2025-10-09 17:46:23.855305: Epoch 158 
2025-10-09 17:46:23.855417: Current learning rate: 0.00245 
2025-10-09 17:49:04.424806: train_loss -0.9049 
2025-10-09 17:49:04.425147: val_loss -0.72 
2025-10-09 17:49:04.425217: Pseudo dice [0.8197] 
2025-10-09 17:49:04.425299: Epoch time: 160.57 s 
2025-10-09 17:49:05.809818:  
2025-10-09 17:49:05.810186: Epoch 159 
2025-10-09 17:49:05.810336: Current learning rate: 0.0024 
2025-10-09 17:51:46.207345: train_loss -0.8997 
2025-10-09 17:51:46.207739: val_loss -0.7077 
2025-10-09 17:51:46.207813: Pseudo dice [0.8132] 
2025-10-09 17:51:46.207891: Epoch time: 160.4 s 
2025-10-09 17:51:47.694543:  
2025-10-09 17:51:47.694943: Epoch 160 
2025-10-09 17:51:47.695081: Current learning rate: 0.00235 
2025-10-09 17:54:28.313912: train_loss -0.9038 
2025-10-09 17:54:28.314238: val_loss -0.7189 
2025-10-09 17:54:28.314311: Pseudo dice [0.8189] 
2025-10-09 17:54:28.314395: Epoch time: 160.62 s 
2025-10-09 17:54:29.689017:  
2025-10-09 17:54:29.689271: Epoch 161 
2025-10-09 17:54:29.689380: Current learning rate: 0.0023 
2025-10-09 17:57:10.383682: train_loss -0.9009 
2025-10-09 17:57:10.384037: val_loss -0.7247 
2025-10-09 17:57:10.384108: Pseudo dice [0.8199] 
2025-10-09 17:57:10.384189: Epoch time: 160.7 s 
2025-10-09 17:57:11.748784:  
2025-10-09 17:57:11.749011: Epoch 162 
2025-10-09 17:57:11.749125: Current learning rate: 0.00224 
2025-10-09 17:59:52.083504: train_loss -0.9056 
2025-10-09 17:59:52.083833: val_loss -0.7253 
2025-10-09 17:59:52.083900: Pseudo dice [0.8218] 
2025-10-09 17:59:52.083973: Epoch time: 160.34 s 
2025-10-09 17:59:53.495241:  
2025-10-09 17:59:53.495545: Epoch 163 
2025-10-09 17:59:53.495721: Current learning rate: 0.00219 
2025-10-09 18:02:33.883436: train_loss -0.9025 
2025-10-09 18:02:33.883781: val_loss -0.6907 
2025-10-09 18:02:33.883851: Pseudo dice [0.8042] 
2025-10-09 18:02:33.883932: Epoch time: 160.39 s 
2025-10-09 18:02:35.245343:  
2025-10-09 18:02:35.245604: Epoch 164 
2025-10-09 18:02:35.245740: Current learning rate: 0.00214 
2025-10-09 18:05:16.033801: train_loss -0.9057 
2025-10-09 18:05:16.034164: val_loss -0.7242 
2025-10-09 18:05:16.034240: Pseudo dice [0.8219] 
2025-10-09 18:05:16.034324: Epoch time: 160.79 s 
2025-10-09 18:05:17.580330:  
2025-10-09 18:05:17.580644: Epoch 165 
2025-10-09 18:05:17.580810: Current learning rate: 0.00208 
2025-10-09 18:07:58.048592: train_loss -0.9061 
2025-10-09 18:07:58.048935: val_loss -0.6962 
2025-10-09 18:07:58.049001: Pseudo dice [0.8072] 
2025-10-09 18:07:58.049087: Epoch time: 160.47 s 
2025-10-09 18:07:59.508336:  
2025-10-09 18:07:59.508562: Epoch 166 
2025-10-09 18:07:59.508679: Current learning rate: 0.00203 
2025-10-09 18:10:40.571569: train_loss -0.9029 
2025-10-09 18:10:40.571964: val_loss -0.7246 
2025-10-09 18:10:40.572044: Pseudo dice [0.8207] 
2025-10-09 18:10:40.572130: Epoch time: 161.06 s 
2025-10-09 18:10:42.364766:  
2025-10-09 18:10:42.364976: Epoch 167 
2025-10-09 18:10:42.365111: Current learning rate: 0.00198 
2025-10-09 18:13:22.706611: train_loss -0.908 
2025-10-09 18:13:22.706971: val_loss -0.7305 
2025-10-09 18:13:22.707043: Pseudo dice [0.8255] 
2025-10-09 18:13:22.707124: Epoch time: 160.34 s 
2025-10-09 18:13:24.038126:  
2025-10-09 18:13:24.038442: Epoch 168 
2025-10-09 18:13:24.038582: Current learning rate: 0.00192 
2025-10-09 18:16:04.459005: train_loss -0.9063 
2025-10-09 18:16:04.459335: val_loss -0.7115 
2025-10-09 18:16:04.459408: Pseudo dice [0.818] 
2025-10-09 18:16:04.459499: Epoch time: 160.42 s 
2025-10-09 18:16:05.772498:  
2025-10-09 18:16:05.772713: Epoch 169 
2025-10-09 18:16:05.772814: Current learning rate: 0.00187 
2025-10-09 18:18:46.467886: train_loss -0.9065 
2025-10-09 18:18:46.468416: val_loss -0.7148 
2025-10-09 18:18:46.468507: Pseudo dice [0.8183] 
2025-10-09 18:18:46.468621: Epoch time: 160.7 s 
2025-10-09 18:18:47.881900:  
2025-10-09 18:18:47.882151: Epoch 170 
2025-10-09 18:18:47.882257: Current learning rate: 0.00181 
2025-10-09 18:21:28.200775: train_loss -0.906 
2025-10-09 18:21:28.201146: val_loss -0.7131 
2025-10-09 18:21:28.201253: Pseudo dice [0.8122] 
2025-10-09 18:21:28.201350: Epoch time: 160.32 s 
2025-10-09 18:21:29.549323:  
2025-10-09 18:21:29.549550: Epoch 171 
2025-10-09 18:21:29.549665: Current learning rate: 0.00176 
2025-10-09 18:24:10.136975: train_loss -0.9072 
2025-10-09 18:24:10.137347: val_loss -0.7226 
2025-10-09 18:24:10.137419: Pseudo dice [0.8213] 
2025-10-09 18:24:10.137529: Epoch time: 160.59 s 
2025-10-09 18:24:11.474576:  
2025-10-09 18:24:11.474878: Epoch 172 
2025-10-09 18:24:11.474983: Current learning rate: 0.0017 
2025-10-09 18:26:51.856225: train_loss -0.9116 
2025-10-09 18:26:51.856620: val_loss -0.7329 
2025-10-09 18:26:51.856714: Pseudo dice [0.8277] 
2025-10-09 18:26:51.856810: Epoch time: 160.38 s 
2025-10-09 18:26:53.145876:  
2025-10-09 18:26:53.146351: Epoch 173 
2025-10-09 18:26:53.146469: Current learning rate: 0.00165 
2025-10-09 18:29:33.502486: train_loss -0.9063 
2025-10-09 18:29:33.502884: val_loss -0.6983 
2025-10-09 18:29:33.502951: Pseudo dice [0.8068] 
2025-10-09 18:29:33.503025: Epoch time: 160.36 s 
2025-10-09 18:29:34.824470:  
2025-10-09 18:29:34.824704: Epoch 174 
2025-10-09 18:29:34.824807: Current learning rate: 0.00159 
2025-10-09 18:32:15.493394: train_loss -0.9083 
2025-10-09 18:32:15.493859: val_loss -0.7085 
2025-10-09 18:32:15.493948: Pseudo dice [0.817] 
2025-10-09 18:32:15.494031: Epoch time: 160.67 s 
2025-10-09 18:32:16.814370:  
2025-10-09 18:32:16.814742: Epoch 175 
2025-10-09 18:32:16.814957: Current learning rate: 0.00154 
2025-10-09 18:34:57.165027: train_loss -0.9065 
2025-10-09 18:34:57.165447: val_loss -0.7103 
2025-10-09 18:34:57.165520: Pseudo dice [0.8125] 
2025-10-09 18:34:57.165599: Epoch time: 160.35 s 
2025-10-09 18:34:58.601801:  
2025-10-09 18:34:58.602141: Epoch 176 
2025-10-09 18:34:58.602279: Current learning rate: 0.00148 
2025-10-09 18:37:38.937080: train_loss -0.9078 
2025-10-09 18:37:38.937398: val_loss -0.7259 
2025-10-09 18:37:38.937468: Pseudo dice [0.8265] 
2025-10-09 18:37:38.937572: Epoch time: 160.34 s 
2025-10-09 18:37:40.259837:  
2025-10-09 18:37:40.260072: Epoch 177 
2025-10-09 18:37:40.260186: Current learning rate: 0.00143 
2025-10-09 18:40:20.476711: train_loss -0.9072 
2025-10-09 18:40:20.477049: val_loss -0.7375 
2025-10-09 18:40:20.477132: Pseudo dice [0.8312] 
2025-10-09 18:40:20.477254: Epoch time: 160.22 s 
2025-10-09 18:40:22.193344:  
2025-10-09 18:40:22.193570: Epoch 178 
2025-10-09 18:40:22.193839: Current learning rate: 0.00137 
2025-10-09 18:43:02.510052: train_loss -0.9058 
2025-10-09 18:43:02.510401: val_loss -0.7207 
2025-10-09 18:43:02.510490: Pseudo dice [0.8193] 
2025-10-09 18:43:02.510594: Epoch time: 160.32 s 
2025-10-09 18:43:03.798338:  
2025-10-09 18:43:03.798611: Epoch 179 
2025-10-09 18:43:03.798753: Current learning rate: 0.00132 
2025-10-09 18:45:44.346282: train_loss -0.9102 
2025-10-09 18:45:44.346658: val_loss -0.7032 
2025-10-09 18:45:44.346758: Pseudo dice [0.8142] 
2025-10-09 18:45:44.346856: Epoch time: 160.55 s 
2025-10-09 18:45:45.632727:  
2025-10-09 18:45:45.632961: Epoch 180 
2025-10-09 18:45:45.633069: Current learning rate: 0.00126 
2025-10-09 18:48:26.055184: train_loss -0.9102 
2025-10-09 18:48:26.055725: val_loss -0.7323 
2025-10-09 18:48:26.055816: Pseudo dice [0.8309] 
2025-10-09 18:48:26.055916: Epoch time: 160.42 s 
2025-10-09 18:48:27.357090:  
2025-10-09 18:48:27.357299: Epoch 181 
2025-10-09 18:48:27.357518: Current learning rate: 0.0012 
2025-10-09 18:51:07.658576: train_loss -0.9053 
2025-10-09 18:51:07.658933: val_loss -0.7245 
2025-10-09 18:51:07.659003: Pseudo dice [0.8217] 
2025-10-09 18:51:07.659078: Epoch time: 160.3 s 
2025-10-09 18:51:08.981226:  
2025-10-09 18:51:08.981480: Epoch 182 
2025-10-09 18:51:08.981584: Current learning rate: 0.00115 
2025-10-09 18:53:49.282527: train_loss -0.9116 
2025-10-09 18:53:49.282894: val_loss -0.7228 
2025-10-09 18:53:49.282964: Pseudo dice [0.8222] 
2025-10-09 18:53:49.283059: Epoch time: 160.3 s 
2025-10-09 18:53:50.564306:  
2025-10-09 18:53:50.564581: Epoch 183 
2025-10-09 18:53:50.564698: Current learning rate: 0.00109 
2025-10-09 18:56:31.107924: train_loss -0.9095 
2025-10-09 18:56:31.108271: val_loss -0.7071 
2025-10-09 18:56:31.108341: Pseudo dice [0.8115] 
2025-10-09 18:56:31.108419: Epoch time: 160.55 s 
2025-10-09 18:56:32.458540:  
2025-10-09 18:56:32.458868: Epoch 184 
2025-10-09 18:56:32.459013: Current learning rate: 0.00103 
2025-10-09 18:59:13.220073: train_loss -0.9104 
2025-10-09 18:59:13.220453: val_loss -0.7105 
2025-10-09 18:59:13.220539: Pseudo dice [0.8155] 
2025-10-09 18:59:13.220624: Epoch time: 160.76 s 
2025-10-09 18:59:14.598963:  
2025-10-09 18:59:14.599278: Epoch 185 
2025-10-09 18:59:14.599417: Current learning rate: 0.00097 
2025-10-09 19:01:54.823180: train_loss -0.9074 
2025-10-09 19:01:54.823519: val_loss -0.7239 
2025-10-09 19:01:54.823598: Pseudo dice [0.8241] 
2025-10-09 19:01:54.823696: Epoch time: 160.23 s 
2025-10-09 19:01:56.135146:  
2025-10-09 19:01:56.135357: Epoch 186 
2025-10-09 19:01:56.135468: Current learning rate: 0.00091 
2025-10-09 19:04:36.541346: train_loss -0.9108 
2025-10-09 19:04:36.541673: val_loss -0.6886 
2025-10-09 19:04:36.541745: Pseudo dice [0.8061] 
2025-10-09 19:04:36.541821: Epoch time: 160.41 s 
2025-10-09 19:04:37.823098:  
2025-10-09 19:04:37.823313: Epoch 187 
2025-10-09 19:04:37.823416: Current learning rate: 0.00085 
2025-10-09 19:07:18.042497: train_loss -0.9121 
2025-10-09 19:07:18.042863: val_loss -0.7093 
2025-10-09 19:07:18.042932: Pseudo dice [0.8183] 
2025-10-09 19:07:18.043010: Epoch time: 160.22 s 
2025-10-09 19:07:19.463259:  
2025-10-09 19:07:19.463703: Epoch 188 
2025-10-09 19:07:19.463808: Current learning rate: 0.00079 
2025-10-09 19:09:59.677836: train_loss -0.9098 
2025-10-09 19:09:59.678184: val_loss -0.6884 
2025-10-09 19:09:59.678344: Pseudo dice [0.8013] 
2025-10-09 19:09:59.678454: Epoch time: 160.22 s 
2025-10-09 19:10:01.053561:  
2025-10-09 19:10:01.053785: Epoch 189 
2025-10-09 19:10:01.053892: Current learning rate: 0.00074 
2025-10-09 19:12:41.745852: train_loss -0.91 
2025-10-09 19:12:41.746367: val_loss -0.7234 
2025-10-09 19:12:41.746444: Pseudo dice [0.8266] 
2025-10-09 19:12:41.746537: Epoch time: 160.69 s 
2025-10-09 19:12:43.722189:  
2025-10-09 19:12:43.722412: Epoch 190 
2025-10-09 19:12:43.722524: Current learning rate: 0.00067 
2025-10-09 19:15:24.392198: train_loss -0.915 
2025-10-09 19:15:24.392539: val_loss -0.6981 
2025-10-09 19:15:24.392608: Pseudo dice [0.8096] 
2025-10-09 19:15:24.392694: Epoch time: 160.67 s 
2025-10-09 19:15:25.702083:  
2025-10-09 19:15:25.702295: Epoch 191 
2025-10-09 19:15:25.702398: Current learning rate: 0.00061 
2025-10-09 19:18:06.278744: train_loss -0.9102 
2025-10-09 19:18:06.279070: val_loss -0.7003 
2025-10-09 19:18:06.279137: Pseudo dice [0.8113] 
2025-10-09 19:18:06.279218: Epoch time: 160.58 s 
2025-10-09 19:18:07.775475:  
2025-10-09 19:18:07.775728: Epoch 192 
2025-10-09 19:18:07.775838: Current learning rate: 0.00055 
2025-10-09 19:20:48.084304: train_loss -0.9118 
2025-10-09 19:20:48.084640: val_loss -0.705 
2025-10-09 19:20:48.084731: Pseudo dice [0.8123] 
2025-10-09 19:20:48.084810: Epoch time: 160.31 s 
2025-10-09 19:20:49.424293:  
2025-10-09 19:20:49.424579: Epoch 193 
2025-10-09 19:20:49.424698: Current learning rate: 0.00049 
2025-10-09 19:23:29.801302: train_loss -0.9135 
2025-10-09 19:23:29.801638: val_loss -0.6962 
2025-10-09 19:23:29.801721: Pseudo dice [0.8113] 
2025-10-09 19:23:29.801802: Epoch time: 160.38 s 
2025-10-09 19:23:31.142941:  
2025-10-09 19:23:31.143173: Epoch 194 
2025-10-09 19:23:31.143277: Current learning rate: 0.00043 
2025-10-09 19:26:11.424914: train_loss -0.9088 
2025-10-09 19:26:11.425235: val_loss -0.7034 
2025-10-09 19:26:11.425301: Pseudo dice [0.8148] 
2025-10-09 19:26:11.425376: Epoch time: 160.28 s 
2025-10-09 19:26:12.785690:  
2025-10-09 19:26:12.785938: Epoch 195 
2025-10-09 19:26:12.786055: Current learning rate: 0.00036 
2025-10-09 19:28:52.914211: train_loss -0.9155 
2025-10-09 19:28:52.914536: val_loss -0.7066 
2025-10-09 19:28:52.914608: Pseudo dice [0.8178] 
2025-10-09 19:28:52.914706: Epoch time: 160.13 s 
2025-10-09 19:28:54.211125:  
2025-10-09 19:28:54.211359: Epoch 196 
2025-10-09 19:28:54.211505: Current learning rate: 0.0003 
2025-10-09 19:31:34.324921: train_loss -0.9104 
2025-10-09 19:31:34.325268: val_loss -0.7081 
2025-10-09 19:31:34.325357: Pseudo dice [0.8149] 
2025-10-09 19:31:34.325448: Epoch time: 160.12 s 
2025-10-09 19:31:35.677285:  
2025-10-09 19:31:35.677499: Epoch 197 
2025-10-09 19:31:35.677612: Current learning rate: 0.00023 
2025-10-09 19:34:15.537750: train_loss -0.9127 
2025-10-09 19:34:15.538079: val_loss -0.7173 
2025-10-09 19:34:15.538148: Pseudo dice [0.8169] 
2025-10-09 19:34:15.538224: Epoch time: 159.86 s 
2025-10-09 19:34:16.821566:  
2025-10-09 19:34:16.821760: Epoch 198 
2025-10-09 19:34:16.821859: Current learning rate: 0.00016 
2025-10-09 19:36:56.706341: train_loss -0.9133 
2025-10-09 19:36:56.706680: val_loss -0.7246 
2025-10-09 19:36:56.706753: Pseudo dice [0.8231] 
2025-10-09 19:36:56.706828: Epoch time: 159.89 s 
2025-10-09 19:36:58.007840:  
2025-10-09 19:36:58.008033: Epoch 199 
2025-10-09 19:36:58.008137: Current learning rate: 8e-05 
2025-10-09 19:39:37.629914: train_loss -0.9128 
2025-10-09 19:39:37.630229: val_loss -0.6953 
2025-10-09 19:39:37.630296: Pseudo dice [0.8102] 
2025-10-09 19:39:37.630369: Epoch time: 159.62 s 
2025-10-09 19:39:39.833237: Training done. 
2025-10-09 19:39:40.239302: Using splits from existing split file: /home/rnga/tsdehaan/my-scratch/Data_nnUNet/nnUnet_preprocessed/Dataset001_AAA/splits_final.json 
2025-10-09 19:39:40.239860: The split file contains 5 splits. 
2025-10-09 19:39:40.239932: Desired fold for training: 4 
2025-10-09 19:39:40.239991: This split has 62 training and 15 validation cases. 
2025-10-09 19:39:40.240252: predicting IVIM_003 
2025-10-09 19:39:40.242095: IVIM_003, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-09 19:39:50.000304: predicting IVIM_021 
2025-10-09 19:39:50.034804: IVIM_021, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-09 19:39:50.576235: predicting IVIM_031 
2025-10-09 19:39:50.577665: IVIM_031, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-09 19:39:51.094838: predicting IVIM_034 
2025-10-09 19:39:51.095990: IVIM_034, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-09 19:39:51.645396: predicting IVIM_039 
2025-10-09 19:39:51.646749: IVIM_039, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-09 19:39:52.169022: predicting IVIM_066 
2025-10-09 19:39:52.170558: IVIM_066, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-09 19:39:53.405902: predicting IVIM_081 
2025-10-09 19:39:53.407248: IVIM_081, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-09 19:39:54.025228: predicting IVIM_083 
2025-10-09 19:39:54.026629: IVIM_083, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-09 19:39:54.608185: predicting IVIM_089 
2025-10-09 19:39:54.609956: IVIM_089, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-09 19:39:55.110549: predicting IVIM_090 
2025-10-09 19:39:55.113208: IVIM_090, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-09 19:39:55.608943: predicting IVIM_092 
2025-10-09 19:39:55.610904: IVIM_092, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-09 19:39:56.109539: predicting IVIM_094 
2025-10-09 19:39:56.111858: IVIM_094, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-09 19:39:56.612718: predicting IVIM_140 
2025-10-09 19:39:56.614411: IVIM_140, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-09 19:39:57.113148: predicting IVIM_160 
2025-10-09 19:39:57.117656: IVIM_160, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-09 19:39:57.619663: predicting IVIM_162 
2025-10-09 19:39:57.621131: IVIM_162, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-09 19:40:09.481991: Validation complete 
2025-10-09 19:40:09.482126: Mean Validation Dice:  0.8198251301919278 
