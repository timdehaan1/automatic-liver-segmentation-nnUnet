
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 
2025-10-08 10:10:41.936584: do_dummy_2d_data_aug: True 
2025-10-08 10:10:41.937345: Using splits from existing split file: /home/rnga/tsdehaan/my-scratch/Data_nnUNet/nnUnet_preprocessed/Dataset001_AAA/splits_final.json 
2025-10-08 10:10:41.937531: The split file contains 5 splits. 
2025-10-08 10:10:41.937575: Desired fold for training: 2 
2025-10-08 10:10:41.937615: This split has 62 training and 15 validation cases. 
2025-10-08 10:10:47.634308: Using torch.compile... 

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 3, 'patch_size': [28, 192, 256], 'median_image_size_in_voxels': [27.0, 168.0, 256.0], 'spacing': [7.0, 1.7578125, 1.7578125], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [True], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[1, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [1, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': False} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset001_AAA', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [7.0, 1.7578125, 1.7578125], 'original_median_shape_after_transp': [27, 168, 256], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 2868.689453125, 'mean': 227.654052734375, 'median': 202.5998992919922, 'min': 16.920146942138672, 'percentile_00_5': 53.70329284667969, 'percentile_99_5': 852.1956787109375, 'std': 123.62262725830078}}} 
 
2025-10-08 10:10:50.477611: unpacking dataset... 
2025-10-08 10:11:02.598814: unpacking done... 
2025-10-08 10:11:02.600553: Unable to plot network architecture: nnUNet_compile is enabled! 
2025-10-08 10:11:02.659059:  
2025-10-08 10:11:02.659187: Epoch 0 
2025-10-08 10:11:02.659478: Current learning rate: 0.01 
2025-10-08 10:14:07.200393: train_loss -0.5209 
2025-10-08 10:14:07.200659: val_loss -0.6514 
2025-10-08 10:14:07.200730: Pseudo dice [0.7548] 
2025-10-08 10:14:07.200796: Epoch time: 184.54 s 
2025-10-08 10:14:07.200851: Yayy! New best EMA pseudo Dice: 0.7548 
2025-10-08 10:14:08.600135:  
2025-10-08 10:14:08.600313: Epoch 1 
2025-10-08 10:14:08.600445: Current learning rate: 0.00995 
2025-10-08 10:16:48.321134: train_loss -0.6887 
2025-10-08 10:16:48.321410: val_loss -0.7158 
2025-10-08 10:16:48.321484: Pseudo dice [0.797] 
2025-10-08 10:16:48.321566: Epoch time: 159.72 s 
2025-10-08 10:16:48.321642: Yayy! New best EMA pseudo Dice: 0.759 
2025-10-08 10:16:50.079822:  
2025-10-08 10:16:50.080041: Epoch 2 
2025-10-08 10:16:50.080181: Current learning rate: 0.00991 
2025-10-08 10:19:30.000044: train_loss -0.7149 
2025-10-08 10:19:30.000304: val_loss -0.7473 
2025-10-08 10:19:30.000383: Pseudo dice [0.8212] 
2025-10-08 10:19:30.000460: Epoch time: 159.92 s 
2025-10-08 10:19:30.000517: Yayy! New best EMA pseudo Dice: 0.7652 
2025-10-08 10:19:31.681354:  
2025-10-08 10:19:31.681539: Epoch 3 
2025-10-08 10:19:31.681669: Current learning rate: 0.00986 
2025-10-08 10:22:11.610350: train_loss -0.7219 
2025-10-08 10:22:11.610669: val_loss -0.7432 
2025-10-08 10:22:11.610739: Pseudo dice [0.8207] 
2025-10-08 10:22:11.610817: Epoch time: 159.93 s 
2025-10-08 10:22:11.610875: Yayy! New best EMA pseudo Dice: 0.7708 
2025-10-08 10:22:13.240385:  
2025-10-08 10:22:13.240572: Epoch 4 
2025-10-08 10:22:13.240702: Current learning rate: 0.00982 
2025-10-08 10:24:53.233701: train_loss -0.7284 
2025-10-08 10:24:53.234023: val_loss -0.7645 
2025-10-08 10:24:53.234090: Pseudo dice [0.8293] 
2025-10-08 10:24:53.234169: Epoch time: 159.99 s 
2025-10-08 10:24:53.234236: Yayy! New best EMA pseudo Dice: 0.7766 
2025-10-08 10:24:55.048748:  
2025-10-08 10:24:55.048965: Epoch 5 
2025-10-08 10:24:55.049083: Current learning rate: 0.00977 
2025-10-08 10:27:34.950401: train_loss -0.738 
2025-10-08 10:27:34.950731: val_loss -0.7691 
2025-10-08 10:27:34.950799: Pseudo dice [0.8319] 
2025-10-08 10:27:34.950877: Epoch time: 159.9 s 
2025-10-08 10:27:34.950933: Yayy! New best EMA pseudo Dice: 0.7822 
2025-10-08 10:27:36.594305:  
2025-10-08 10:27:36.594516: Epoch 6 
2025-10-08 10:27:36.594615: Current learning rate: 0.00973 
2025-10-08 10:30:16.330249: train_loss -0.7438 
2025-10-08 10:30:16.330659: val_loss -0.7532 
2025-10-08 10:30:16.330755: Pseudo dice [0.8232] 
2025-10-08 10:30:16.330839: Epoch time: 159.74 s 
2025-10-08 10:30:16.330899: Yayy! New best EMA pseudo Dice: 0.7863 
2025-10-08 10:30:17.970456:  
2025-10-08 10:30:17.970680: Epoch 7 
2025-10-08 10:30:17.970773: Current learning rate: 0.00968 
2025-10-08 10:32:57.783946: train_loss -0.7535 
2025-10-08 10:32:57.784266: val_loss -0.747 
2025-10-08 10:32:57.784423: Pseudo dice [0.8158] 
2025-10-08 10:32:57.784513: Epoch time: 159.81 s 
2025-10-08 10:32:57.784589: Yayy! New best EMA pseudo Dice: 0.7892 
2025-10-08 10:32:59.405511:  
2025-10-08 10:32:59.405716: Epoch 8 
2025-10-08 10:32:59.405830: Current learning rate: 0.00964 
2025-10-08 10:35:39.541306: train_loss -0.7464 
2025-10-08 10:35:39.541627: val_loss -0.7355 
2025-10-08 10:35:39.541711: Pseudo dice [0.8064] 
2025-10-08 10:35:39.541789: Epoch time: 160.14 s 
2025-10-08 10:35:39.541922: Yayy! New best EMA pseudo Dice: 0.7909 
2025-10-08 10:35:41.231000:  
2025-10-08 10:35:41.231197: Epoch 9 
2025-10-08 10:35:41.231316: Current learning rate: 0.00959 
2025-10-08 10:38:21.090322: train_loss -0.7617 
2025-10-08 10:38:21.090635: val_loss -0.7633 
2025-10-08 10:38:21.090717: Pseudo dice [0.8267] 
2025-10-08 10:38:21.090793: Epoch time: 159.86 s 
2025-10-08 10:38:21.090850: Yayy! New best EMA pseudo Dice: 0.7945 
2025-10-08 10:38:23.017087:  
2025-10-08 10:38:23.017344: Epoch 10 
2025-10-08 10:38:23.017468: Current learning rate: 0.00955 
2025-10-08 10:41:02.862744: train_loss -0.773 
2025-10-08 10:41:02.863082: val_loss -0.7842 
2025-10-08 10:41:02.863156: Pseudo dice [0.8421] 
2025-10-08 10:41:02.863235: Epoch time: 159.85 s 
2025-10-08 10:41:02.863293: Yayy! New best EMA pseudo Dice: 0.7993 
2025-10-08 10:41:04.462706:  
2025-10-08 10:41:04.462915: Epoch 11 
2025-10-08 10:41:04.463045: Current learning rate: 0.0095 
2025-10-08 10:43:44.497915: train_loss -0.7737 
2025-10-08 10:43:44.498232: val_loss -0.786 
2025-10-08 10:43:44.498314: Pseudo dice [0.8469] 
2025-10-08 10:43:44.498392: Epoch time: 160.04 s 
2025-10-08 10:43:44.498452: Yayy! New best EMA pseudo Dice: 0.804 
2025-10-08 10:43:46.110282:  
2025-10-08 10:43:46.110497: Epoch 12 
2025-10-08 10:43:46.110614: Current learning rate: 0.00946 
2025-10-08 10:46:26.207063: train_loss -0.7712 
2025-10-08 10:46:26.207400: val_loss -0.7808 
2025-10-08 10:46:26.207507: Pseudo dice [0.8432] 
2025-10-08 10:46:26.207611: Epoch time: 160.1 s 
2025-10-08 10:46:26.207690: Yayy! New best EMA pseudo Dice: 0.808 
2025-10-08 10:46:27.820067:  
2025-10-08 10:46:27.820246: Epoch 13 
2025-10-08 10:46:27.820365: Current learning rate: 0.00941 
2025-10-08 10:49:07.542514: train_loss -0.7721 
2025-10-08 10:49:07.542856: val_loss -0.7808 
2025-10-08 10:49:07.542932: Pseudo dice [0.8428] 
2025-10-08 10:49:07.543010: Epoch time: 159.72 s 
2025-10-08 10:49:07.543069: Yayy! New best EMA pseudo Dice: 0.8114 
2025-10-08 10:49:09.183290:  
2025-10-08 10:49:09.183465: Epoch 14 
2025-10-08 10:49:09.183570: Current learning rate: 0.00937 
2025-10-08 10:51:48.963574: train_loss -0.7705 
2025-10-08 10:51:48.963868: val_loss -0.7758 
2025-10-08 10:51:48.963972: Pseudo dice [0.8361] 
2025-10-08 10:51:48.964109: Epoch time: 159.78 s 
2025-10-08 10:51:48.964176: Yayy! New best EMA pseudo Dice: 0.8139 
2025-10-08 10:51:50.593436:  
2025-10-08 10:51:50.593719: Epoch 15 
2025-10-08 10:51:50.593833: Current learning rate: 0.00932 
2025-10-08 10:54:30.441943: train_loss -0.7816 
2025-10-08 10:54:30.442262: val_loss -0.7745 
2025-10-08 10:54:30.442333: Pseudo dice [0.8319] 
2025-10-08 10:54:30.442424: Epoch time: 159.85 s 
2025-10-08 10:54:30.442486: Yayy! New best EMA pseudo Dice: 0.8157 
2025-10-08 10:54:32.093778:  
2025-10-08 10:54:32.094072: Epoch 16 
2025-10-08 10:54:32.094209: Current learning rate: 0.00928 
2025-10-08 10:57:11.900603: train_loss -0.7762 
2025-10-08 10:57:11.900973: val_loss -0.7659 
2025-10-08 10:57:11.901044: Pseudo dice [0.8293] 
2025-10-08 10:57:11.901126: Epoch time: 159.81 s 
2025-10-08 10:57:11.901193: Yayy! New best EMA pseudo Dice: 0.8171 
2025-10-08 10:57:13.572628:  
2025-10-08 10:57:13.572822: Epoch 17 
2025-10-08 10:57:13.572939: Current learning rate: 0.00923 
2025-10-08 10:59:53.610436: train_loss -0.7866 
2025-10-08 10:59:53.610809: val_loss -0.7831 
2025-10-08 10:59:53.610883: Pseudo dice [0.8414] 
2025-10-08 10:59:53.611001: Epoch time: 160.04 s 
2025-10-08 10:59:53.611089: Yayy! New best EMA pseudo Dice: 0.8195 
2025-10-08 10:59:55.264513:  
2025-10-08 10:59:55.264800: Epoch 18 
2025-10-08 10:59:55.264906: Current learning rate: 0.00919 
2025-10-08 11:02:35.277844: train_loss -0.7842 
2025-10-08 11:02:35.278153: val_loss -0.7813 
2025-10-08 11:02:35.278218: Pseudo dice [0.8403] 
2025-10-08 11:02:35.278294: Epoch time: 160.01 s 
2025-10-08 11:02:35.278355: Yayy! New best EMA pseudo Dice: 0.8216 
2025-10-08 11:02:36.946137:  
2025-10-08 11:02:36.946320: Epoch 19 
2025-10-08 11:02:36.946428: Current learning rate: 0.00914 
2025-10-08 11:05:16.900761: train_loss -0.7881 
2025-10-08 11:05:16.901137: val_loss -0.7816 
2025-10-08 11:05:16.901208: Pseudo dice [0.8385] 
2025-10-08 11:05:16.901353: Epoch time: 159.96 s 
2025-10-08 11:05:16.901417: Yayy! New best EMA pseudo Dice: 0.8233 
2025-10-08 11:05:18.568257:  
2025-10-08 11:05:18.568530: Epoch 20 
2025-10-08 11:05:18.568678: Current learning rate: 0.0091 
2025-10-08 11:07:58.274246: train_loss -0.7835 
2025-10-08 11:07:58.274565: val_loss -0.7842 
2025-10-08 11:07:58.274632: Pseudo dice [0.8415] 
2025-10-08 11:07:58.274725: Epoch time: 159.71 s 
2025-10-08 11:07:58.274782: Yayy! New best EMA pseudo Dice: 0.8251 
2025-10-08 11:07:59.928468:  
2025-10-08 11:07:59.928690: Epoch 21 
2025-10-08 11:07:59.928989: Current learning rate: 0.00905 
2025-10-08 11:10:39.709220: train_loss -0.8 
2025-10-08 11:10:39.709578: val_loss -0.7859 
2025-10-08 11:10:39.709662: Pseudo dice [0.8414] 
2025-10-08 11:10:39.709741: Epoch time: 159.78 s 
2025-10-08 11:10:39.709798: Yayy! New best EMA pseudo Dice: 0.8267 
2025-10-08 11:10:41.634088:  
2025-10-08 11:10:41.634289: Epoch 22 
2025-10-08 11:10:41.634400: Current learning rate: 0.009 
2025-10-08 11:13:21.560348: train_loss -0.7964 
2025-10-08 11:13:21.560674: val_loss -0.7761 
2025-10-08 11:13:21.560746: Pseudo dice [0.8362] 
2025-10-08 11:13:21.560821: Epoch time: 159.93 s 
2025-10-08 11:13:21.560888: Yayy! New best EMA pseudo Dice: 0.8277 
2025-10-08 11:13:23.121459:  
2025-10-08 11:13:23.121684: Epoch 23 
2025-10-08 11:13:23.121856: Current learning rate: 0.00896 
2025-10-08 11:16:03.001083: train_loss -0.7941 
2025-10-08 11:16:03.001394: val_loss -0.7776 
2025-10-08 11:16:03.001462: Pseudo dice [0.8377] 
2025-10-08 11:16:03.001536: Epoch time: 159.88 s 
2025-10-08 11:16:03.001592: Yayy! New best EMA pseudo Dice: 0.8287 
2025-10-08 11:16:04.532770:  
2025-10-08 11:16:04.532968: Epoch 24 
2025-10-08 11:16:04.533083: Current learning rate: 0.00891 
2025-10-08 11:18:44.231636: train_loss -0.8006 
2025-10-08 11:18:44.231956: val_loss -0.781 
2025-10-08 11:18:44.232025: Pseudo dice [0.8391] 
2025-10-08 11:18:44.232102: Epoch time: 159.7 s 
2025-10-08 11:18:44.232158: Yayy! New best EMA pseudo Dice: 0.8297 
2025-10-08 11:18:45.827687:  
2025-10-08 11:18:45.827875: Epoch 25 
2025-10-08 11:18:45.828005: Current learning rate: 0.00887 
2025-10-08 11:21:25.637296: train_loss -0.7986 
2025-10-08 11:21:25.637584: val_loss -0.7832 
2025-10-08 11:21:25.637661: Pseudo dice [0.8405] 
2025-10-08 11:21:25.637797: Epoch time: 159.81 s 
2025-10-08 11:21:25.637873: Yayy! New best EMA pseudo Dice: 0.8308 
2025-10-08 11:21:27.236772:  
2025-10-08 11:21:27.236945: Epoch 26 
2025-10-08 11:21:27.237037: Current learning rate: 0.00882 
2025-10-08 11:24:06.989481: train_loss -0.8 
2025-10-08 11:24:06.989902: val_loss -0.772 
2025-10-08 11:24:06.990019: Pseudo dice [0.832] 
2025-10-08 11:24:06.990115: Epoch time: 159.75 s 
2025-10-08 11:24:06.990181: Yayy! New best EMA pseudo Dice: 0.8309 
2025-10-08 11:24:08.601148:  
2025-10-08 11:24:08.601313: Epoch 27 
2025-10-08 11:24:08.601426: Current learning rate: 0.00878 
2025-10-08 11:26:48.247706: train_loss -0.8083 
2025-10-08 11:26:48.248084: val_loss -0.7852 
2025-10-08 11:26:48.248187: Pseudo dice [0.8428] 
2025-10-08 11:26:48.248277: Epoch time: 159.65 s 
2025-10-08 11:26:48.248336: Yayy! New best EMA pseudo Dice: 0.8321 
2025-10-08 11:26:49.924742:  
2025-10-08 11:26:49.924936: Epoch 28 
2025-10-08 11:26:49.925049: Current learning rate: 0.00873 
2025-10-08 11:29:29.783932: train_loss -0.8055 
2025-10-08 11:29:29.784224: val_loss -0.7783 
2025-10-08 11:29:29.784296: Pseudo dice [0.8369] 
2025-10-08 11:29:29.784418: Epoch time: 159.86 s 
2025-10-08 11:29:29.784499: Yayy! New best EMA pseudo Dice: 0.8326 
2025-10-08 11:29:31.385536:  
2025-10-08 11:29:31.385735: Epoch 29 
2025-10-08 11:29:31.385864: Current learning rate: 0.00868 
2025-10-08 11:32:11.005966: train_loss -0.8038 
2025-10-08 11:32:11.006289: val_loss -0.7784 
2025-10-08 11:32:11.006408: Pseudo dice [0.8371] 
2025-10-08 11:32:11.006510: Epoch time: 159.62 s 
2025-10-08 11:32:11.006571: Yayy! New best EMA pseudo Dice: 0.833 
2025-10-08 11:32:12.616073:  
2025-10-08 11:32:12.616278: Epoch 30 
2025-10-08 11:32:12.616396: Current learning rate: 0.00864 
2025-10-08 11:34:52.451572: train_loss -0.8081 
2025-10-08 11:34:52.451918: val_loss -0.7835 
2025-10-08 11:34:52.451998: Pseudo dice [0.8412] 
2025-10-08 11:34:52.452078: Epoch time: 159.84 s 
2025-10-08 11:34:52.452134: Yayy! New best EMA pseudo Dice: 0.8339 
2025-10-08 11:34:54.057773:  
2025-10-08 11:34:54.057970: Epoch 31 
2025-10-08 11:34:54.058096: Current learning rate: 0.00859 
2025-10-08 11:37:33.698738: train_loss -0.8096 
2025-10-08 11:37:33.699044: val_loss -0.787 
2025-10-08 11:37:33.699127: Pseudo dice [0.8454] 
2025-10-08 11:37:33.699226: Epoch time: 159.64 s 
2025-10-08 11:37:33.699303: Yayy! New best EMA pseudo Dice: 0.835 
2025-10-08 11:37:35.350705:  
2025-10-08 11:37:35.350902: Epoch 32 
2025-10-08 11:37:35.351036: Current learning rate: 0.00855 
2025-10-08 11:40:15.180177: train_loss -0.8128 
2025-10-08 11:40:15.180509: val_loss -0.7846 
2025-10-08 11:40:15.180583: Pseudo dice [0.8425] 
2025-10-08 11:40:15.180673: Epoch time: 159.83 s 
2025-10-08 11:40:15.180737: Yayy! New best EMA pseudo Dice: 0.8358 
2025-10-08 11:40:17.132340:  
2025-10-08 11:40:17.132589: Epoch 33 
2025-10-08 11:40:17.132791: Current learning rate: 0.0085 
2025-10-08 11:42:57.036325: train_loss -0.8091 
2025-10-08 11:42:57.036618: val_loss -0.7914 
2025-10-08 11:42:57.036696: Pseudo dice [0.8508] 
2025-10-08 11:42:57.036812: Epoch time: 159.91 s 
2025-10-08 11:42:57.036883: Yayy! New best EMA pseudo Dice: 0.8373 
2025-10-08 11:42:58.645489:  
2025-10-08 11:42:58.645708: Epoch 34 
2025-10-08 11:42:58.645826: Current learning rate: 0.00846 
2025-10-08 11:45:38.610896: train_loss -0.8137 
2025-10-08 11:45:38.611282: val_loss -0.7957 
2025-10-08 11:45:38.611357: Pseudo dice [0.8502] 
2025-10-08 11:45:38.611438: Epoch time: 159.97 s 
2025-10-08 11:45:38.611503: Yayy! New best EMA pseudo Dice: 0.8386 
2025-10-08 11:45:40.221117:  
2025-10-08 11:45:40.221310: Epoch 35 
2025-10-08 11:45:40.221427: Current learning rate: 0.00841 
2025-10-08 11:48:20.008878: train_loss -0.8181 
2025-10-08 11:48:20.009196: val_loss -0.791 
2025-10-08 11:48:20.009265: Pseudo dice [0.8473] 
2025-10-08 11:48:20.009340: Epoch time: 159.79 s 
2025-10-08 11:48:20.009399: Yayy! New best EMA pseudo Dice: 0.8394 
2025-10-08 11:48:21.722527:  
2025-10-08 11:48:21.722852: Epoch 36 
2025-10-08 11:48:21.722991: Current learning rate: 0.00836 
2025-10-08 11:51:01.440632: train_loss -0.8178 
2025-10-08 11:51:01.441066: val_loss -0.7894 
2025-10-08 11:51:01.441151: Pseudo dice [0.8477] 
2025-10-08 11:51:01.441244: Epoch time: 159.72 s 
2025-10-08 11:51:01.441305: Yayy! New best EMA pseudo Dice: 0.8403 
2025-10-08 11:51:03.071711:  
2025-10-08 11:51:03.071940: Epoch 37 
2025-10-08 11:51:03.072053: Current learning rate: 0.00832 
2025-10-08 11:53:43.014043: train_loss -0.8153 
2025-10-08 11:53:43.014383: val_loss -0.7875 
2025-10-08 11:53:43.014462: Pseudo dice [0.8458] 
2025-10-08 11:53:43.014560: Epoch time: 159.94 s 
2025-10-08 11:53:43.014627: Yayy! New best EMA pseudo Dice: 0.8408 
2025-10-08 11:53:44.671260:  
2025-10-08 11:53:44.671763: Epoch 38 
2025-10-08 11:53:44.671915: Current learning rate: 0.00827 
2025-10-08 11:56:24.447145: train_loss -0.8118 
2025-10-08 11:56:24.447474: val_loss -0.785 
2025-10-08 11:56:24.447541: Pseudo dice [0.8417] 
2025-10-08 11:56:24.447619: Epoch time: 159.78 s 
2025-10-08 11:56:24.447695: Yayy! New best EMA pseudo Dice: 0.8409 
2025-10-08 11:56:26.085075:  
2025-10-08 11:56:26.085296: Epoch 39 
2025-10-08 11:56:26.085415: Current learning rate: 0.00823 
2025-10-08 11:59:05.710736: train_loss -0.8164 
2025-10-08 11:59:05.711037: val_loss -0.7996 
2025-10-08 11:59:05.711109: Pseudo dice [0.851] 
2025-10-08 11:59:05.711183: Epoch time: 159.63 s 
2025-10-08 11:59:05.711240: Yayy! New best EMA pseudo Dice: 0.8419 
2025-10-08 11:59:07.367225:  
2025-10-08 11:59:07.367423: Epoch 40 
2025-10-08 11:59:07.367536: Current learning rate: 0.00818 
2025-10-08 12:01:47.152567: train_loss -0.8194 
2025-10-08 12:01:47.152882: val_loss -0.7885 
2025-10-08 12:01:47.152962: Pseudo dice [0.845] 
2025-10-08 12:01:47.153040: Epoch time: 159.79 s 
2025-10-08 12:01:47.153098: Yayy! New best EMA pseudo Dice: 0.8422 
2025-10-08 12:01:48.850425:  
2025-10-08 12:01:48.850596: Epoch 41 
2025-10-08 12:01:48.850713: Current learning rate: 0.00813 
2025-10-08 12:04:28.586990: train_loss -0.8152 
2025-10-08 12:04:28.587298: val_loss -0.7882 
2025-10-08 12:04:28.587369: Pseudo dice [0.8444] 
2025-10-08 12:04:28.587449: Epoch time: 159.74 s 
2025-10-08 12:04:28.587507: Yayy! New best EMA pseudo Dice: 0.8424 
2025-10-08 12:04:30.164365:  
2025-10-08 12:04:30.164661: Epoch 42 
2025-10-08 12:04:30.164804: Current learning rate: 0.00809 
2025-10-08 12:07:10.021280: train_loss -0.8141 
2025-10-08 12:07:10.021569: val_loss -0.7813 
2025-10-08 12:07:10.021642: Pseudo dice [0.8399] 
2025-10-08 12:07:10.021925: Epoch time: 159.86 s 
2025-10-08 12:07:11.212964:  
2025-10-08 12:07:11.213163: Epoch 43 
2025-10-08 12:07:11.213274: Current learning rate: 0.00804 
2025-10-08 12:09:51.009356: train_loss -0.8225 
2025-10-08 12:09:51.009662: val_loss -0.7779 
2025-10-08 12:09:51.009734: Pseudo dice [0.8371] 
2025-10-08 12:09:51.009813: Epoch time: 159.8 s 
2025-10-08 12:09:52.576798:  
2025-10-08 12:09:52.577061: Epoch 44 
2025-10-08 12:09:52.577201: Current learning rate: 0.008 
2025-10-08 12:12:32.313080: train_loss -0.8231 
2025-10-08 12:12:32.313379: val_loss -0.7832 
2025-10-08 12:12:32.313466: Pseudo dice [0.8402] 
2025-10-08 12:12:32.313546: Epoch time: 159.74 s 
2025-10-08 12:12:33.519768:  
2025-10-08 12:12:33.519966: Epoch 45 
2025-10-08 12:12:33.520073: Current learning rate: 0.00795 
2025-10-08 12:15:13.181286: train_loss -0.8223 
2025-10-08 12:15:13.181585: val_loss -0.7848 
2025-10-08 12:15:13.181661: Pseudo dice [0.8427] 
2025-10-08 12:15:13.181743: Epoch time: 159.66 s 
2025-10-08 12:15:14.377185:  
2025-10-08 12:15:14.377450: Epoch 46 
2025-10-08 12:15:14.377576: Current learning rate: 0.0079 
2025-10-08 12:17:54.120344: train_loss -0.8233 
2025-10-08 12:17:54.120644: val_loss -0.7873 
2025-10-08 12:17:54.120732: Pseudo dice [0.8451] 
2025-10-08 12:17:54.120836: Epoch time: 159.74 s 
2025-10-08 12:17:55.284031:  
2025-10-08 12:17:55.284282: Epoch 47 
2025-10-08 12:17:55.284397: Current learning rate: 0.00786 
2025-10-08 12:20:34.993086: train_loss -0.8291 
2025-10-08 12:20:34.993421: val_loss -0.7838 
2025-10-08 12:20:34.993497: Pseudo dice [0.8439] 
2025-10-08 12:20:34.993574: Epoch time: 159.71 s 
2025-10-08 12:20:36.171041:  
2025-10-08 12:20:36.171277: Epoch 48 
2025-10-08 12:20:36.171400: Current learning rate: 0.00781 
2025-10-08 12:23:15.798123: train_loss -0.833 
2025-10-08 12:23:15.798500: val_loss -0.7883 
2025-10-08 12:23:15.798574: Pseudo dice [0.845] 
2025-10-08 12:23:15.798664: Epoch time: 159.63 s 
2025-10-08 12:23:15.798743: Yayy! New best EMA pseudo Dice: 0.8425 
2025-10-08 12:23:17.393215:  
2025-10-08 12:23:17.393408: Epoch 49 
2025-10-08 12:23:17.393524: Current learning rate: 0.00777 
2025-10-08 12:25:57.015207: train_loss -0.8341 
2025-10-08 12:25:57.015524: val_loss -0.7955 
2025-10-08 12:25:57.015596: Pseudo dice [0.851] 
2025-10-08 12:25:57.015686: Epoch time: 159.62 s 
2025-10-08 12:25:57.306520: Yayy! New best EMA pseudo Dice: 0.8433 
2025-10-08 12:25:58.901243:  
2025-10-08 12:25:58.901431: Epoch 50 
2025-10-08 12:25:58.901549: Current learning rate: 0.00772 
2025-10-08 12:28:38.461584: train_loss -0.8296 
2025-10-08 12:28:38.461907: val_loss -0.7904 
2025-10-08 12:28:38.461980: Pseudo dice [0.8469] 
2025-10-08 12:28:38.462077: Epoch time: 159.56 s 
2025-10-08 12:28:38.462191: Yayy! New best EMA pseudo Dice: 0.8437 
2025-10-08 12:28:40.068994:  
2025-10-08 12:28:40.069356: Epoch 51 
2025-10-08 12:28:40.069533: Current learning rate: 0.00767 
2025-10-08 12:31:19.725556: train_loss -0.8364 
2025-10-08 12:31:19.725867: val_loss -0.7843 
2025-10-08 12:31:19.725934: Pseudo dice [0.8415] 
2025-10-08 12:31:19.726012: Epoch time: 159.66 s 
2025-10-08 12:31:20.936640:  
2025-10-08 12:31:20.936816: Epoch 52 
2025-10-08 12:31:20.936929: Current learning rate: 0.00763 
2025-10-08 12:34:00.737662: train_loss -0.8368 
2025-10-08 12:34:00.737967: val_loss -0.794 
2025-10-08 12:34:00.738051: Pseudo dice [0.8511] 
2025-10-08 12:34:00.738132: Epoch time: 159.8 s 
2025-10-08 12:34:00.738188: Yayy! New best EMA pseudo Dice: 0.8442 
2025-10-08 12:34:02.324367:  
2025-10-08 12:34:02.324566: Epoch 53 
2025-10-08 12:34:02.324688: Current learning rate: 0.00758 
2025-10-08 12:36:42.068252: train_loss -0.8383 
2025-10-08 12:36:42.068604: val_loss -0.7846 
2025-10-08 12:36:42.068690: Pseudo dice [0.8412] 
2025-10-08 12:36:42.068768: Epoch time: 159.75 s 
2025-10-08 12:36:43.288670:  
2025-10-08 12:36:43.288851: Epoch 54 
2025-10-08 12:36:43.288986: Current learning rate: 0.00753 
2025-10-08 12:39:23.433004: train_loss -0.8394 
2025-10-08 12:39:23.433297: val_loss -0.794 
2025-10-08 12:39:23.433365: Pseudo dice [0.8517] 
2025-10-08 12:39:23.433444: Epoch time: 160.15 s 
2025-10-08 12:39:23.433502: Yayy! New best EMA pseudo Dice: 0.8447 
2025-10-08 12:39:25.076905:  
2025-10-08 12:39:25.077097: Epoch 55 
2025-10-08 12:39:25.077211: Current learning rate: 0.00749 
2025-10-08 12:42:05.241067: train_loss -0.8397 
2025-10-08 12:42:05.241410: val_loss -0.7949 
2025-10-08 12:42:05.241589: Pseudo dice [0.8498] 
2025-10-08 12:42:05.241692: Epoch time: 160.17 s 
2025-10-08 12:42:05.241773: Yayy! New best EMA pseudo Dice: 0.8452 
2025-10-08 12:42:07.348745:  
2025-10-08 12:42:07.349018: Epoch 56 
2025-10-08 12:42:07.349154: Current learning rate: 0.00744 
2025-10-08 12:44:47.316693: train_loss -0.8439 
2025-10-08 12:44:47.317024: val_loss -0.7893 
2025-10-08 12:44:47.317091: Pseudo dice [0.8462] 
2025-10-08 12:44:47.317170: Epoch time: 159.97 s 
2025-10-08 12:44:47.317229: Yayy! New best EMA pseudo Dice: 0.8453 
2025-10-08 12:44:48.987721:  
2025-10-08 12:44:48.987959: Epoch 57 
2025-10-08 12:44:48.988076: Current learning rate: 0.00739 
2025-10-08 12:47:29.060749: train_loss -0.8411 
2025-10-08 12:47:29.061058: val_loss -0.7942 
2025-10-08 12:47:29.061132: Pseudo dice [0.8496] 
2025-10-08 12:47:29.061211: Epoch time: 160.07 s 
2025-10-08 12:47:29.061270: Yayy! New best EMA pseudo Dice: 0.8457 
2025-10-08 12:47:30.740184:  
2025-10-08 12:47:30.740371: Epoch 58 
2025-10-08 12:47:30.740483: Current learning rate: 0.00735 
2025-10-08 12:50:10.900181: train_loss -0.8415 
2025-10-08 12:50:10.900521: val_loss -0.8001 
2025-10-08 12:50:10.900593: Pseudo dice [0.8562] 
2025-10-08 12:50:10.900683: Epoch time: 160.16 s 
2025-10-08 12:50:10.900750: Yayy! New best EMA pseudo Dice: 0.8468 
2025-10-08 12:50:12.561050:  
2025-10-08 12:50:12.561254: Epoch 59 
2025-10-08 12:50:12.561397: Current learning rate: 0.0073 
2025-10-08 12:52:52.524611: train_loss -0.8395 
2025-10-08 12:52:52.524934: val_loss -0.787 
2025-10-08 12:52:52.525016: Pseudo dice [0.8438] 
2025-10-08 12:52:52.525099: Epoch time: 159.96 s 
2025-10-08 12:52:53.831423:  
2025-10-08 12:52:53.831636: Epoch 60 
2025-10-08 12:52:53.831759: Current learning rate: 0.00725 
2025-10-08 12:55:33.926640: train_loss -0.8423 
2025-10-08 12:55:33.926955: val_loss -0.7672 
2025-10-08 12:55:33.927029: Pseudo dice [0.8331] 
2025-10-08 12:55:33.927104: Epoch time: 160.1 s 
2025-10-08 12:55:35.168975:  
2025-10-08 12:55:35.169195: Epoch 61 
2025-10-08 12:55:35.169413: Current learning rate: 0.00721 
2025-10-08 12:58:15.044405: train_loss -0.8401 
2025-10-08 12:58:15.044734: val_loss -0.7945 
2025-10-08 12:58:15.044821: Pseudo dice [0.8497] 
2025-10-08 12:58:15.044919: Epoch time: 159.88 s 
2025-10-08 12:58:16.294621:  
2025-10-08 12:58:16.294846: Epoch 62 
2025-10-08 12:58:16.294937: Current learning rate: 0.00716 
2025-10-08 13:00:56.507624: train_loss -0.8427 
2025-10-08 13:00:56.507968: val_loss -0.8037 
2025-10-08 13:00:56.508040: Pseudo dice [0.8565] 
2025-10-08 13:00:56.508121: Epoch time: 160.21 s 
2025-10-08 13:00:57.750108:  
2025-10-08 13:00:57.750301: Epoch 63 
2025-10-08 13:00:57.750438: Current learning rate: 0.00711 
2025-10-08 13:03:37.971462: train_loss -0.8473 
2025-10-08 13:03:37.971818: val_loss -0.7979 
2025-10-08 13:03:37.971891: Pseudo dice [0.8525] 
2025-10-08 13:03:37.971971: Epoch time: 160.22 s 
2025-10-08 13:03:37.972030: Yayy! New best EMA pseudo Dice: 0.8473 
2025-10-08 13:03:39.611909:  
2025-10-08 13:03:39.612125: Epoch 64 
2025-10-08 13:03:39.612262: Current learning rate: 0.00707 
2025-10-08 13:06:19.524203: train_loss -0.8481 
2025-10-08 13:06:19.524532: val_loss -0.7926 
2025-10-08 13:06:19.524710: Pseudo dice [0.8508] 
2025-10-08 13:06:19.524800: Epoch time: 159.91 s 
2025-10-08 13:06:19.524860: Yayy! New best EMA pseudo Dice: 0.8476 
2025-10-08 13:06:21.172903:  
2025-10-08 13:06:21.173189: Epoch 65 
2025-10-08 13:06:21.173323: Current learning rate: 0.00702 
2025-10-08 13:09:01.338296: train_loss -0.844 
2025-10-08 13:09:01.338609: val_loss -0.7818 
2025-10-08 13:09:01.338728: Pseudo dice [0.8425] 
2025-10-08 13:09:01.338845: Epoch time: 160.17 s 
2025-10-08 13:09:02.701523:  
2025-10-08 13:09:02.701747: Epoch 66 
2025-10-08 13:09:02.701883: Current learning rate: 0.00697 
2025-10-08 13:11:42.787672: train_loss -0.845 
2025-10-08 13:11:42.788230: val_loss -0.7975 
2025-10-08 13:11:42.788309: Pseudo dice [0.8536] 
2025-10-08 13:11:42.788390: Epoch time: 160.09 s 
2025-10-08 13:11:42.788454: Yayy! New best EMA pseudo Dice: 0.8477 
2025-10-08 13:11:44.925788:  
2025-10-08 13:11:44.926085: Epoch 67 
2025-10-08 13:11:44.926204: Current learning rate: 0.00693 
2025-10-08 13:14:25.017061: train_loss -0.8483 
2025-10-08 13:14:25.017376: val_loss -0.7893 
2025-10-08 13:14:25.017445: Pseudo dice [0.8466] 
2025-10-08 13:14:25.017523: Epoch time: 160.09 s 
2025-10-08 13:14:26.292636:  
2025-10-08 13:14:26.292876: Epoch 68 
2025-10-08 13:14:26.293071: Current learning rate: 0.00688 
2025-10-08 13:17:06.347301: train_loss -0.8531 
2025-10-08 13:17:06.347638: val_loss -0.793 
2025-10-08 13:17:06.347723: Pseudo dice [0.8521] 
2025-10-08 13:17:06.347803: Epoch time: 160.06 s 
2025-10-08 13:17:06.347867: Yayy! New best EMA pseudo Dice: 0.8481 
2025-10-08 13:17:08.075893:  
2025-10-08 13:17:08.076109: Epoch 69 
2025-10-08 13:17:08.076221: Current learning rate: 0.00683 
2025-10-08 13:19:48.191952: train_loss -0.8534 
2025-10-08 13:19:48.192267: val_loss -0.7875 
2025-10-08 13:19:48.192338: Pseudo dice [0.8469] 
2025-10-08 13:19:48.192416: Epoch time: 160.12 s 
2025-10-08 13:19:49.466111:  
2025-10-08 13:19:49.466342: Epoch 70 
2025-10-08 13:19:49.466454: Current learning rate: 0.00679 
2025-10-08 13:22:29.723062: train_loss -0.857 
2025-10-08 13:22:29.723389: val_loss -0.79 
2025-10-08 13:22:29.723460: Pseudo dice [0.8466] 
2025-10-08 13:22:29.723536: Epoch time: 160.26 s 
2025-10-08 13:22:31.079250:  
2025-10-08 13:22:31.079665: Epoch 71 
2025-10-08 13:22:31.079848: Current learning rate: 0.00674 
2025-10-08 13:25:11.021379: train_loss -0.8533 
2025-10-08 13:25:11.021707: val_loss -0.7814 
2025-10-08 13:25:11.021784: Pseudo dice [0.8424] 
2025-10-08 13:25:11.021861: Epoch time: 159.94 s 
2025-10-08 13:25:12.403479:  
2025-10-08 13:25:12.403723: Epoch 72 
2025-10-08 13:25:12.403850: Current learning rate: 0.00669 
2025-10-08 13:27:52.211133: train_loss -0.855 
2025-10-08 13:27:52.211426: val_loss -0.7918 
2025-10-08 13:27:52.211498: Pseudo dice [0.8477] 
2025-10-08 13:27:52.211581: Epoch time: 159.81 s 
2025-10-08 13:27:53.492810:  
2025-10-08 13:27:53.493016: Epoch 73 
2025-10-08 13:27:53.493177: Current learning rate: 0.00665 
2025-10-08 13:30:33.455329: train_loss -0.854 
2025-10-08 13:30:33.455610: val_loss -0.7851 
2025-10-08 13:30:33.455695: Pseudo dice [0.8469] 
2025-10-08 13:30:33.455787: Epoch time: 159.96 s 
2025-10-08 13:30:34.718605:  
2025-10-08 13:30:34.718956: Epoch 74 
2025-10-08 13:30:34.719064: Current learning rate: 0.0066 
2025-10-08 13:33:14.824690: train_loss -0.8522 
2025-10-08 13:33:14.824987: val_loss -0.7971 
2025-10-08 13:33:14.825063: Pseudo dice [0.855] 
2025-10-08 13:33:14.825142: Epoch time: 160.11 s 
2025-10-08 13:33:16.090216:  
2025-10-08 13:33:16.090405: Epoch 75 
2025-10-08 13:33:16.090516: Current learning rate: 0.00655 
2025-10-08 13:35:56.048268: train_loss -0.8512 
2025-10-08 13:35:56.048614: val_loss -0.7848 
2025-10-08 13:35:56.048700: Pseudo dice [0.8421] 
2025-10-08 13:35:56.048791: Epoch time: 159.96 s 
2025-10-08 13:35:57.326858:  
2025-10-08 13:35:57.327150: Epoch 76 
2025-10-08 13:35:57.327301: Current learning rate: 0.0065 
2025-10-08 13:38:37.055369: train_loss -0.8615 
2025-10-08 13:38:37.055669: val_loss -0.7955 
2025-10-08 13:38:37.055743: Pseudo dice [0.8529] 
2025-10-08 13:38:37.055841: Epoch time: 159.73 s 
2025-10-08 13:38:38.350187:  
2025-10-08 13:38:38.350415: Epoch 77 
2025-10-08 13:38:38.350534: Current learning rate: 0.00646 
2025-10-08 13:41:17.966658: train_loss -0.8606 
2025-10-08 13:41:17.966988: val_loss -0.809 
2025-10-08 13:41:17.967055: Pseudo dice [0.8632] 
2025-10-08 13:41:17.967153: Epoch time: 159.62 s 
2025-10-08 13:41:17.967244: Yayy! New best EMA pseudo Dice: 0.8495 
2025-10-08 13:41:20.072448:  
2025-10-08 13:41:20.072672: Epoch 78 
2025-10-08 13:41:20.072793: Current learning rate: 0.00641 
2025-10-08 13:43:59.789759: train_loss -0.8585 
2025-10-08 13:43:59.790033: val_loss -0.7981 
2025-10-08 13:43:59.790096: Pseudo dice [0.8528] 
2025-10-08 13:43:59.790170: Epoch time: 159.72 s 
2025-10-08 13:43:59.790227: Yayy! New best EMA pseudo Dice: 0.8498 
2025-10-08 13:44:01.549270:  
2025-10-08 13:44:01.549479: Epoch 79 
2025-10-08 13:44:01.549605: Current learning rate: 0.00636 
2025-10-08 13:46:41.254490: train_loss -0.8567 
2025-10-08 13:46:41.254807: val_loss -0.7905 
2025-10-08 13:46:41.254875: Pseudo dice [0.8487] 
2025-10-08 13:46:41.254952: Epoch time: 159.71 s 
2025-10-08 13:46:42.540908:  
2025-10-08 13:46:42.541137: Epoch 80 
2025-10-08 13:46:42.541252: Current learning rate: 0.00631 
2025-10-08 13:49:22.364733: train_loss -0.8563 
2025-10-08 13:49:22.365022: val_loss -0.7818 
2025-10-08 13:49:22.365090: Pseudo dice [0.8427] 
2025-10-08 13:49:22.365166: Epoch time: 159.83 s 
2025-10-08 13:49:23.668612:  
2025-10-08 13:49:23.668847: Epoch 81 
2025-10-08 13:49:23.668962: Current learning rate: 0.00627 
2025-10-08 13:52:03.387601: train_loss -0.8565 
2025-10-08 13:52:03.388130: val_loss -0.7938 
2025-10-08 13:52:03.388211: Pseudo dice [0.8501] 
2025-10-08 13:52:03.388337: Epoch time: 159.72 s 
2025-10-08 13:52:04.698312:  
2025-10-08 13:52:04.698523: Epoch 82 
2025-10-08 13:52:04.698635: Current learning rate: 0.00622 
2025-10-08 13:54:44.371179: train_loss -0.8585 
2025-10-08 13:54:44.371533: val_loss -0.7889 
2025-10-08 13:54:44.371605: Pseudo dice [0.8477] 
2025-10-08 13:54:44.371704: Epoch time: 159.67 s 
2025-10-08 13:54:45.580430:  
2025-10-08 13:54:45.580637: Epoch 83 
2025-10-08 13:54:45.580752: Current learning rate: 0.00617 
2025-10-08 13:57:25.467973: train_loss -0.8592 
2025-10-08 13:57:25.468537: val_loss -0.7846 
2025-10-08 13:57:25.468618: Pseudo dice [0.8418] 
2025-10-08 13:57:25.468712: Epoch time: 159.89 s 
2025-10-08 13:57:26.720896:  
2025-10-08 13:57:26.721309: Epoch 84 
2025-10-08 13:57:26.721489: Current learning rate: 0.00612 
2025-10-08 14:00:06.574718: train_loss -0.8624 
2025-10-08 14:00:06.575040: val_loss -0.7861 
2025-10-08 14:00:06.575110: Pseudo dice [0.8441] 
2025-10-08 14:00:06.575186: Epoch time: 159.86 s 
2025-10-08 14:00:07.807589:  
2025-10-08 14:00:07.807780: Epoch 85 
2025-10-08 14:00:07.807899: Current learning rate: 0.00608 
2025-10-08 14:02:47.561093: train_loss -0.8611 
2025-10-08 14:02:47.561399: val_loss -0.7952 
2025-10-08 14:02:47.561466: Pseudo dice [0.851] 
2025-10-08 14:02:47.561547: Epoch time: 159.75 s 
2025-10-08 14:02:48.782741:  
2025-10-08 14:02:48.782939: Epoch 86 
2025-10-08 14:02:48.783056: Current learning rate: 0.00603 
2025-10-08 14:05:28.576344: train_loss -0.8613 
2025-10-08 14:05:28.576749: val_loss -0.7931 
2025-10-08 14:05:28.576822: Pseudo dice [0.8514] 
2025-10-08 14:05:28.576900: Epoch time: 159.79 s 
2025-10-08 14:05:29.772726:  
2025-10-08 14:05:29.772906: Epoch 87 
2025-10-08 14:05:29.773074: Current learning rate: 0.00598 
2025-10-08 14:08:09.470355: train_loss -0.8689 
2025-10-08 14:08:09.470850: val_loss -0.7878 
2025-10-08 14:08:09.470956: Pseudo dice [0.8474] 
2025-10-08 14:08:09.471082: Epoch time: 159.7 s 
2025-10-08 14:08:10.677573:  
2025-10-08 14:08:10.677882: Epoch 88 
2025-10-08 14:08:10.678022: Current learning rate: 0.00593 
2025-10-08 14:10:50.361425: train_loss -0.8623 
2025-10-08 14:10:50.361752: val_loss -0.7972 
2025-10-08 14:10:50.361828: Pseudo dice [0.8529] 
2025-10-08 14:10:50.361905: Epoch time: 159.69 s 
2025-10-08 14:10:51.930514:  
2025-10-08 14:10:51.930736: Epoch 89 
2025-10-08 14:10:51.930851: Current learning rate: 0.00589 
2025-10-08 14:13:31.646024: train_loss -0.8634 
2025-10-08 14:13:31.646317: val_loss -0.79 
2025-10-08 14:13:31.646614: Pseudo dice [0.8472] 
2025-10-08 14:13:31.646711: Epoch time: 159.72 s 
2025-10-08 14:13:32.947527:  
2025-10-08 14:13:32.947702: Epoch 90 
2025-10-08 14:13:32.947816: Current learning rate: 0.00584 
2025-10-08 14:16:12.773050: train_loss -0.8634 
2025-10-08 14:16:12.773352: val_loss -0.7923 
2025-10-08 14:16:12.773418: Pseudo dice [0.8508] 
2025-10-08 14:16:12.773497: Epoch time: 159.83 s 
2025-10-08 14:16:13.977957:  
2025-10-08 14:16:13.978179: Epoch 91 
2025-10-08 14:16:13.978289: Current learning rate: 0.00579 
2025-10-08 14:18:53.520174: train_loss -0.8659 
2025-10-08 14:18:53.520516: val_loss -0.7871 
2025-10-08 14:18:53.520921: Pseudo dice [0.8471] 
2025-10-08 14:18:53.521058: Epoch time: 159.54 s 
2025-10-08 14:18:54.737041:  
2025-10-08 14:18:54.737238: Epoch 92 
2025-10-08 14:18:54.737349: Current learning rate: 0.00574 
2025-10-08 14:21:34.313946: train_loss -0.8656 
2025-10-08 14:21:34.314326: val_loss -0.7942 
2025-10-08 14:21:34.314414: Pseudo dice [0.8521] 
2025-10-08 14:21:34.314501: Epoch time: 159.58 s 
2025-10-08 14:21:35.535798:  
2025-10-08 14:21:35.536020: Epoch 93 
2025-10-08 14:21:35.536139: Current learning rate: 0.0057 
2025-10-08 14:24:15.109171: train_loss -0.8668 
2025-10-08 14:24:15.109545: val_loss -0.792 
2025-10-08 14:24:15.109629: Pseudo dice [0.8499] 
2025-10-08 14:24:15.109740: Epoch time: 159.57 s 
2025-10-08 14:24:16.447248:  
2025-10-08 14:24:16.447523: Epoch 94 
2025-10-08 14:24:16.447706: Current learning rate: 0.00565 
2025-10-08 14:26:56.113064: train_loss -0.8671 
2025-10-08 14:26:56.113416: val_loss -0.7925 
2025-10-08 14:26:56.113494: Pseudo dice [0.8491] 
2025-10-08 14:26:56.113579: Epoch time: 159.67 s 
2025-10-08 14:26:57.350023:  
2025-10-08 14:26:57.350227: Epoch 95 
2025-10-08 14:26:57.350343: Current learning rate: 0.0056 
2025-10-08 14:29:37.219557: train_loss -0.8627 
2025-10-08 14:29:37.219962: val_loss -0.7893 
2025-10-08 14:29:37.220039: Pseudo dice [0.8483] 
2025-10-08 14:29:37.220125: Epoch time: 159.87 s 
2025-10-08 14:29:38.460614:  
2025-10-08 14:29:38.460892: Epoch 96 
2025-10-08 14:29:38.461030: Current learning rate: 0.00555 
2025-10-08 14:32:18.242566: train_loss -0.8686 
2025-10-08 14:32:18.242918: val_loss -0.7883 
2025-10-08 14:32:18.243010: Pseudo dice [0.8448] 
2025-10-08 14:32:18.243160: Epoch time: 159.78 s 
2025-10-08 14:32:19.478857:  
2025-10-08 14:32:19.479018: Epoch 97 
2025-10-08 14:32:19.479151: Current learning rate: 0.0055 
2025-10-08 14:34:59.362973: train_loss -0.8711 
2025-10-08 14:34:59.363311: val_loss -0.7981 
2025-10-08 14:34:59.363394: Pseudo dice [0.8552] 
2025-10-08 14:34:59.363478: Epoch time: 159.89 s 
2025-10-08 14:35:00.595162:  
2025-10-08 14:35:00.595337: Epoch 98 
2025-10-08 14:35:00.595462: Current learning rate: 0.00546 
2025-10-08 14:37:40.240396: train_loss -0.8667 
2025-10-08 14:37:40.240707: val_loss -0.7928 
2025-10-08 14:37:40.240779: Pseudo dice [0.8491] 
2025-10-08 14:37:40.240858: Epoch time: 159.65 s 
2025-10-08 14:37:41.473527:  
2025-10-08 14:37:41.473901: Epoch 99 
2025-10-08 14:37:41.474061: Current learning rate: 0.00541 
2025-10-08 14:40:21.271702: train_loss -0.8713 
2025-10-08 14:40:21.272059: val_loss -0.7862 
2025-10-08 14:40:21.272133: Pseudo dice [0.8461] 
2025-10-08 14:40:21.272212: Epoch time: 159.8 s 
2025-10-08 14:40:22.919712:  
2025-10-08 14:40:22.919930: Epoch 100 
2025-10-08 14:40:22.920048: Current learning rate: 0.00536 
2025-10-08 14:43:02.405777: train_loss -0.8738 
2025-10-08 14:43:02.406111: val_loss -0.7904 
2025-10-08 14:43:02.406185: Pseudo dice [0.8487] 
2025-10-08 14:43:02.406268: Epoch time: 159.49 s 
2025-10-08 14:43:04.023101:  
2025-10-08 14:43:04.023461: Epoch 101 
2025-10-08 14:43:04.023606: Current learning rate: 0.00531 
2025-10-08 14:45:43.798565: train_loss -0.8689 
2025-10-08 14:45:43.798892: val_loss -0.781 
2025-10-08 14:45:43.798958: Pseudo dice [0.8414] 
2025-10-08 14:45:43.799032: Epoch time: 159.78 s 
2025-10-08 14:45:45.030869:  
2025-10-08 14:45:45.031247: Epoch 102 
2025-10-08 14:45:45.031404: Current learning rate: 0.00526 
2025-10-08 14:48:24.999246: train_loss -0.8714 
2025-10-08 14:48:24.999528: val_loss -0.7817 
2025-10-08 14:48:24.999598: Pseudo dice [0.8421] 
2025-10-08 14:48:24.999689: Epoch time: 159.97 s 
2025-10-08 14:48:26.243896:  
2025-10-08 14:48:26.244130: Epoch 103 
2025-10-08 14:48:26.244253: Current learning rate: 0.00521 
2025-10-08 14:51:06.021359: train_loss -0.8715 
2025-10-08 14:51:06.021693: val_loss -0.8073 
2025-10-08 14:51:06.021770: Pseudo dice [0.8601] 
2025-10-08 14:51:06.021851: Epoch time: 159.78 s 
2025-10-08 14:51:07.251193:  
2025-10-08 14:51:07.251418: Epoch 104 
2025-10-08 14:51:07.251544: Current learning rate: 0.00517 
2025-10-08 14:53:47.028481: train_loss -0.8738 
2025-10-08 14:53:47.028803: val_loss -0.7943 
2025-10-08 14:53:47.028872: Pseudo dice [0.8525] 
2025-10-08 14:53:47.029086: Epoch time: 159.78 s 
2025-10-08 14:53:48.301035:  
2025-10-08 14:53:48.301389: Epoch 105 
2025-10-08 14:53:48.301498: Current learning rate: 0.00512 
2025-10-08 14:56:28.209736: train_loss -0.8743 
2025-10-08 14:56:28.210090: val_loss -0.785 
2025-10-08 14:56:28.210166: Pseudo dice [0.8457] 
2025-10-08 14:56:28.210249: Epoch time: 159.91 s 
2025-10-08 14:56:29.453538:  
2025-10-08 14:56:29.453776: Epoch 106 
2025-10-08 14:56:29.453917: Current learning rate: 0.00507 
2025-10-08 14:59:09.116305: train_loss -0.8725 
2025-10-08 14:59:09.116623: val_loss -0.7863 
2025-10-08 14:59:09.116782: Pseudo dice [0.8481] 
2025-10-08 14:59:09.116875: Epoch time: 159.66 s 
2025-10-08 14:59:10.356131:  
2025-10-08 14:59:10.356346: Epoch 107 
2025-10-08 14:59:10.356458: Current learning rate: 0.00502 
2025-10-08 15:01:50.169078: train_loss -0.8701 
2025-10-08 15:01:50.169415: val_loss -0.794 
2025-10-08 15:01:50.169485: Pseudo dice [0.8515] 
2025-10-08 15:01:50.169561: Epoch time: 159.81 s 
2025-10-08 15:01:51.416042:  
2025-10-08 15:01:51.416238: Epoch 108 
2025-10-08 15:01:51.416363: Current learning rate: 0.00497 
2025-10-08 15:04:31.217630: train_loss -0.8739 
2025-10-08 15:04:31.218209: val_loss -0.789 
2025-10-08 15:04:31.218299: Pseudo dice [0.8498] 
2025-10-08 15:04:31.218393: Epoch time: 159.8 s 
2025-10-08 15:04:32.515117:  
2025-10-08 15:04:32.515309: Epoch 109 
2025-10-08 15:04:32.515444: Current learning rate: 0.00492 
2025-10-08 15:07:12.085749: train_loss -0.8733 
2025-10-08 15:07:12.086077: val_loss -0.7866 
2025-10-08 15:07:12.086152: Pseudo dice [0.8478] 
2025-10-08 15:07:12.086231: Epoch time: 159.57 s 
2025-10-08 15:07:13.412038:  
2025-10-08 15:07:13.412266: Epoch 110 
2025-10-08 15:07:13.412492: Current learning rate: 0.00487 
2025-10-08 15:09:52.986372: train_loss -0.875 
2025-10-08 15:09:52.986675: val_loss -0.7897 
2025-10-08 15:09:52.986749: Pseudo dice [0.8474] 
2025-10-08 15:09:52.986827: Epoch time: 159.58 s 
2025-10-08 15:09:54.222832:  
2025-10-08 15:09:54.223053: Epoch 111 
2025-10-08 15:09:54.223167: Current learning rate: 0.00483 
2025-10-08 15:12:33.833392: train_loss -0.8745 
2025-10-08 15:12:33.833789: val_loss -0.7973 
2025-10-08 15:12:33.833867: Pseudo dice [0.8557] 
2025-10-08 15:12:33.833954: Epoch time: 159.61 s 
2025-10-08 15:12:35.495104:  
2025-10-08 15:12:35.495295: Epoch 112 
2025-10-08 15:12:35.495420: Current learning rate: 0.00478 
2025-10-08 15:15:15.336506: train_loss -0.8776 
2025-10-08 15:15:15.336842: val_loss -0.8058 
2025-10-08 15:15:15.336912: Pseudo dice [0.8617] 
2025-10-08 15:15:15.336992: Epoch time: 159.84 s 
2025-10-08 15:15:15.337056: Yayy! New best EMA pseudo Dice: 0.8507 
2025-10-08 15:15:17.073856:  
2025-10-08 15:15:17.074223: Epoch 113 
2025-10-08 15:15:17.074347: Current learning rate: 0.00473 
2025-10-08 15:17:56.764819: train_loss -0.878 
2025-10-08 15:17:56.765154: val_loss -0.7876 
2025-10-08 15:17:56.765223: Pseudo dice [0.8479] 
2025-10-08 15:17:56.765303: Epoch time: 159.69 s 
2025-10-08 15:17:57.997096:  
2025-10-08 15:17:57.997315: Epoch 114 
2025-10-08 15:17:57.997425: Current learning rate: 0.00468 
2025-10-08 15:20:37.499866: train_loss -0.8759 
2025-10-08 15:20:37.500161: val_loss -0.7842 
2025-10-08 15:20:37.500229: Pseudo dice [0.8457] 
2025-10-08 15:20:37.500307: Epoch time: 159.5 s 
2025-10-08 15:20:38.764582:  
2025-10-08 15:20:38.764778: Epoch 115 
2025-10-08 15:20:38.764896: Current learning rate: 0.00463 
2025-10-08 15:23:18.431303: train_loss -0.8753 
2025-10-08 15:23:18.431624: val_loss -0.7901 
2025-10-08 15:23:18.431708: Pseudo dice [0.8497] 
2025-10-08 15:23:18.431787: Epoch time: 159.67 s 
2025-10-08 15:23:19.706518:  
2025-10-08 15:23:19.706854: Epoch 116 
2025-10-08 15:23:19.706971: Current learning rate: 0.00458 
2025-10-08 15:25:59.651962: train_loss -0.8766 
2025-10-08 15:25:59.652260: val_loss -0.7896 
2025-10-08 15:25:59.652328: Pseudo dice [0.8492] 
2025-10-08 15:25:59.652407: Epoch time: 159.95 s 
2025-10-08 15:26:00.907994:  
2025-10-08 15:26:00.908218: Epoch 117 
2025-10-08 15:26:00.908334: Current learning rate: 0.00453 
2025-10-08 15:28:40.582616: train_loss -0.879 
2025-10-08 15:28:40.626424: val_loss -0.7839 
2025-10-08 15:28:40.626637: Pseudo dice [0.8453] 
2025-10-08 15:28:40.626765: Epoch time: 159.68 s 
2025-10-08 15:28:41.953658:  
2025-10-08 15:28:41.953883: Epoch 118 
2025-10-08 15:28:41.954028: Current learning rate: 0.00448 
2025-10-08 15:31:21.629301: train_loss -0.8758 
2025-10-08 15:31:21.629585: val_loss -0.7801 
2025-10-08 15:31:21.629688: Pseudo dice [0.8434] 
2025-10-08 15:31:21.629849: Epoch time: 159.68 s 
2025-10-08 15:31:22.938574:  
2025-10-08 15:31:22.938829: Epoch 119 
2025-10-08 15:31:22.939001: Current learning rate: 0.00443 
2025-10-08 15:34:02.765593: train_loss -0.8811 
2025-10-08 15:34:02.765947: val_loss -0.7892 
2025-10-08 15:34:02.766075: Pseudo dice [0.8485] 
2025-10-08 15:34:02.766203: Epoch time: 159.83 s 
2025-10-08 15:34:04.050114:  
2025-10-08 15:34:04.050326: Epoch 120 
2025-10-08 15:34:04.050450: Current learning rate: 0.00438 
2025-10-08 15:36:44.051356: train_loss -0.8806 
2025-10-08 15:36:44.051705: val_loss -0.7783 
2025-10-08 15:36:44.051785: Pseudo dice [0.8391] 
2025-10-08 15:36:44.051880: Epoch time: 160.0 s 
2025-10-08 15:36:45.403675:  
2025-10-08 15:36:45.403910: Epoch 121 
2025-10-08 15:36:45.404060: Current learning rate: 0.00433 
2025-10-08 15:39:25.465382: train_loss -0.8797 
2025-10-08 15:39:25.465703: val_loss -0.7857 
2025-10-08 15:39:25.465784: Pseudo dice [0.8466] 
2025-10-08 15:39:25.465874: Epoch time: 160.06 s 
2025-10-08 15:39:26.778056:  
2025-10-08 15:39:26.778299: Epoch 122 
2025-10-08 15:39:26.778418: Current learning rate: 0.00429 
2025-10-08 15:42:06.639588: train_loss -0.8784 
2025-10-08 15:42:06.639935: val_loss -0.7901 
2025-10-08 15:42:06.640010: Pseudo dice [0.8505] 
2025-10-08 15:42:06.640093: Epoch time: 159.86 s 
2025-10-08 15:42:08.090399:  
2025-10-08 15:42:08.090593: Epoch 123 
2025-10-08 15:42:08.090721: Current learning rate: 0.00424 
2025-10-08 15:44:48.128434: train_loss -0.8803 
2025-10-08 15:44:48.128741: val_loss -0.7903 
2025-10-08 15:44:48.128813: Pseudo dice [0.8518] 
2025-10-08 15:44:48.128896: Epoch time: 160.04 s 
2025-10-08 15:44:49.867397:  
2025-10-08 15:44:49.867663: Epoch 124 
2025-10-08 15:44:49.867830: Current learning rate: 0.00419 
2025-10-08 15:47:29.721437: train_loss -0.882 
2025-10-08 15:47:29.721807: val_loss -0.7883 
2025-10-08 15:47:29.721905: Pseudo dice [0.8498] 
2025-10-08 15:47:29.721998: Epoch time: 159.86 s 
2025-10-08 15:47:31.065061:  
2025-10-08 15:47:31.065314: Epoch 125 
2025-10-08 15:47:31.065444: Current learning rate: 0.00414 
2025-10-08 15:50:10.852021: train_loss -0.8814 
2025-10-08 15:50:10.852349: val_loss -0.788 
2025-10-08 15:50:10.852421: Pseudo dice [0.8487] 
2025-10-08 15:50:10.852503: Epoch time: 159.79 s 
2025-10-08 15:50:12.213011:  
2025-10-08 15:50:12.213239: Epoch 126 
2025-10-08 15:50:12.213360: Current learning rate: 0.00409 
2025-10-08 15:52:52.318389: train_loss -0.8798 
2025-10-08 15:52:52.318732: val_loss -0.7971 
2025-10-08 15:52:52.318815: Pseudo dice [0.8566] 
2025-10-08 15:52:52.318934: Epoch time: 160.11 s 
2025-10-08 15:52:53.603421:  
2025-10-08 15:52:53.603683: Epoch 127 
2025-10-08 15:52:53.603805: Current learning rate: 0.00404 
2025-10-08 15:55:33.246774: train_loss -0.8831 
2025-10-08 15:55:33.247100: val_loss -0.778 
2025-10-08 15:55:33.247171: Pseudo dice [0.841] 
2025-10-08 15:55:33.247252: Epoch time: 159.64 s 
2025-10-08 15:55:34.519903:  
2025-10-08 15:55:34.520069: Epoch 128 
2025-10-08 15:55:34.520196: Current learning rate: 0.00399 
2025-10-08 15:58:14.289600: train_loss -0.8836 
2025-10-08 15:58:14.289937: val_loss -0.7924 
2025-10-08 15:58:14.290009: Pseudo dice [0.8524] 
2025-10-08 15:58:14.290088: Epoch time: 159.77 s 
2025-10-08 15:58:15.600034:  
2025-10-08 15:58:15.600291: Epoch 129 
2025-10-08 15:58:15.600483: Current learning rate: 0.00394 
2025-10-08 16:00:55.121899: train_loss -0.8835 
2025-10-08 16:00:55.122229: val_loss -0.7896 
2025-10-08 16:00:55.122300: Pseudo dice [0.8478] 
2025-10-08 16:00:55.122386: Epoch time: 159.52 s 
2025-10-08 16:00:56.407911:  
2025-10-08 16:00:56.408164: Epoch 130 
2025-10-08 16:00:56.408329: Current learning rate: 0.00389 
2025-10-08 16:03:36.081074: train_loss -0.8872 
2025-10-08 16:03:36.081387: val_loss -0.7951 
2025-10-08 16:03:36.081474: Pseudo dice [0.8525] 
2025-10-08 16:03:36.081557: Epoch time: 159.67 s 
2025-10-08 16:03:37.367562:  
2025-10-08 16:03:37.367822: Epoch 131 
2025-10-08 16:03:37.367951: Current learning rate: 0.00384 
2025-10-08 16:06:17.203105: train_loss -0.8878 
2025-10-08 16:06:17.203574: val_loss -0.7843 
2025-10-08 16:06:17.203661: Pseudo dice [0.8464] 
2025-10-08 16:06:17.203750: Epoch time: 159.84 s 
2025-10-08 16:06:18.490110:  
2025-10-08 16:06:18.490302: Epoch 132 
2025-10-08 16:06:18.490407: Current learning rate: 0.00379 
2025-10-08 16:08:58.352982: train_loss -0.8851 
2025-10-08 16:08:58.353293: val_loss -0.7908 
2025-10-08 16:08:58.353370: Pseudo dice [0.8504] 
2025-10-08 16:08:58.353495: Epoch time: 159.86 s 
2025-10-08 16:08:59.649824:  
2025-10-08 16:08:59.650026: Epoch 133 
2025-10-08 16:08:59.650142: Current learning rate: 0.00374 
2025-10-08 16:11:39.329126: train_loss -0.8841 
2025-10-08 16:11:39.329427: val_loss -0.7908 
2025-10-08 16:11:39.329501: Pseudo dice [0.8533] 
2025-10-08 16:11:39.329583: Epoch time: 159.68 s 
2025-10-08 16:11:40.623016:  
2025-10-08 16:11:40.623193: Epoch 134 
2025-10-08 16:11:40.623311: Current learning rate: 0.00369 
2025-10-08 16:14:20.378455: train_loss -0.886 
2025-10-08 16:14:20.378817: val_loss -0.779 
2025-10-08 16:14:20.378891: Pseudo dice [0.8428] 
2025-10-08 16:14:20.378973: Epoch time: 159.76 s 
2025-10-08 16:14:22.046260:  
2025-10-08 16:14:22.046462: Epoch 135 
2025-10-08 16:14:22.046629: Current learning rate: 0.00364 
2025-10-08 16:17:01.788696: train_loss -0.8821 
2025-10-08 16:17:01.788996: val_loss -0.7799 
2025-10-08 16:17:01.789066: Pseudo dice [0.845] 
2025-10-08 16:17:01.789143: Epoch time: 159.74 s 
2025-10-08 16:17:03.105777:  
2025-10-08 16:17:03.105979: Epoch 136 
2025-10-08 16:17:03.106092: Current learning rate: 0.00359 
2025-10-08 16:19:42.999238: train_loss -0.8914 
2025-10-08 16:19:42.999618: val_loss -0.7786 
2025-10-08 16:19:42.999704: Pseudo dice [0.8436] 
2025-10-08 16:19:42.999784: Epoch time: 159.89 s 
2025-10-08 16:19:44.330062:  
2025-10-08 16:19:44.330270: Epoch 137 
2025-10-08 16:19:44.330385: Current learning rate: 0.00354 
2025-10-08 16:22:24.199755: train_loss -0.8877 
2025-10-08 16:22:24.200032: val_loss -0.7859 
2025-10-08 16:22:24.200096: Pseudo dice [0.8504] 
2025-10-08 16:22:24.200172: Epoch time: 159.87 s 
2025-10-08 16:22:25.471390:  
2025-10-08 16:22:25.471605: Epoch 138 
2025-10-08 16:22:25.471725: Current learning rate: 0.00349 
2025-10-08 16:25:05.264228: train_loss -0.8893 
2025-10-08 16:25:05.264727: val_loss -0.782 
2025-10-08 16:25:05.264798: Pseudo dice [0.8487] 
2025-10-08 16:25:05.264879: Epoch time: 159.79 s 
2025-10-08 16:25:06.547879:  
2025-10-08 16:25:06.548249: Epoch 139 
2025-10-08 16:25:06.548415: Current learning rate: 0.00343 
2025-10-08 16:27:46.018439: train_loss -0.8901 
2025-10-08 16:27:46.018768: val_loss -0.7861 
2025-10-08 16:27:46.018837: Pseudo dice [0.849] 
2025-10-08 16:27:46.018917: Epoch time: 159.47 s 
2025-10-08 16:27:47.314034:  
2025-10-08 16:27:47.314208: Epoch 140 
2025-10-08 16:27:47.314320: Current learning rate: 0.00338 
2025-10-08 16:30:26.833303: train_loss -0.889 
2025-10-08 16:30:26.833606: val_loss -0.7885 
2025-10-08 16:30:26.833686: Pseudo dice [0.8493] 
2025-10-08 16:30:26.833786: Epoch time: 159.52 s 
2025-10-08 16:30:28.157088:  
2025-10-08 16:30:28.157300: Epoch 141 
2025-10-08 16:30:28.157416: Current learning rate: 0.00333 
2025-10-08 16:33:07.592530: train_loss -0.8862 
2025-10-08 16:33:07.592857: val_loss -0.7808 
2025-10-08 16:33:07.592931: Pseudo dice [0.8434] 
2025-10-08 16:33:07.593009: Epoch time: 159.44 s 
2025-10-08 16:33:08.883843:  
2025-10-08 16:33:08.884046: Epoch 142 
2025-10-08 16:33:08.884155: Current learning rate: 0.00328 
2025-10-08 16:35:48.303763: train_loss -0.8867 
2025-10-08 16:35:48.304106: val_loss -0.7903 
2025-10-08 16:35:48.304194: Pseudo dice [0.8515] 
2025-10-08 16:35:48.304323: Epoch time: 159.42 s 
2025-10-08 16:35:49.614719:  
2025-10-08 16:35:49.614925: Epoch 143 
2025-10-08 16:35:49.615039: Current learning rate: 0.00323 
2025-10-08 16:38:29.125431: train_loss -0.8887 
2025-10-08 16:38:29.125731: val_loss -0.7904 
2025-10-08 16:38:29.125800: Pseudo dice [0.8518] 
2025-10-08 16:38:29.125881: Epoch time: 159.51 s 
2025-10-08 16:38:30.396960:  
2025-10-08 16:38:30.397203: Epoch 144 
2025-10-08 16:38:30.397348: Current learning rate: 0.00318 
2025-10-08 16:41:10.109608: train_loss -0.8909 
2025-10-08 16:41:10.110021: val_loss -0.7813 
2025-10-08 16:41:10.110099: Pseudo dice [0.8454] 
2025-10-08 16:41:10.110183: Epoch time: 159.71 s 
2025-10-08 16:41:11.390368:  
2025-10-08 16:41:11.390572: Epoch 145 
2025-10-08 16:41:11.390754: Current learning rate: 0.00313 
2025-10-08 16:43:50.975990: train_loss -0.8899 
2025-10-08 16:43:50.976278: val_loss -0.7871 
2025-10-08 16:43:50.976346: Pseudo dice [0.8471] 
2025-10-08 16:43:50.976423: Epoch time: 159.59 s 
2025-10-08 16:43:52.256437:  
2025-10-08 16:43:52.256676: Epoch 146 
2025-10-08 16:43:52.256812: Current learning rate: 0.00308 
2025-10-08 16:46:31.760455: train_loss -0.8877 
2025-10-08 16:46:31.760807: val_loss -0.7815 
2025-10-08 16:46:31.760926: Pseudo dice [0.8446] 
2025-10-08 16:46:31.761008: Epoch time: 159.51 s 
2025-10-08 16:46:33.460783:  
2025-10-08 16:46:33.460981: Epoch 147 
2025-10-08 16:46:33.461096: Current learning rate: 0.00303 
2025-10-08 16:49:13.063788: train_loss -0.8917 
2025-10-08 16:49:13.064109: val_loss -0.7869 
2025-10-08 16:49:13.064188: Pseudo dice [0.8486] 
2025-10-08 16:49:13.064270: Epoch time: 159.6 s 
2025-10-08 16:49:14.375300:  
2025-10-08 16:49:14.375504: Epoch 148 
2025-10-08 16:49:14.375686: Current learning rate: 0.00297 
2025-10-08 16:51:54.099690: train_loss -0.8895 
2025-10-08 16:51:54.099991: val_loss -0.776 
2025-10-08 16:51:54.100071: Pseudo dice [0.8401] 
2025-10-08 16:51:54.100149: Epoch time: 159.73 s 
2025-10-08 16:51:55.389291:  
2025-10-08 16:51:55.389665: Epoch 149 
2025-10-08 16:51:55.389824: Current learning rate: 0.00292 
2025-10-08 16:54:34.941463: train_loss -0.8906 
2025-10-08 16:54:34.941809: val_loss -0.7758 
2025-10-08 16:54:34.941878: Pseudo dice [0.8406] 
2025-10-08 16:54:34.941958: Epoch time: 159.55 s 
2025-10-08 16:54:36.617114:  
2025-10-08 16:54:36.617299: Epoch 150 
2025-10-08 16:54:36.617440: Current learning rate: 0.00287 
2025-10-08 16:57:16.299693: train_loss -0.8925 
2025-10-08 16:57:16.299985: val_loss -0.7835 
2025-10-08 16:57:16.300051: Pseudo dice [0.8459] 
2025-10-08 16:57:16.300127: Epoch time: 159.68 s 
2025-10-08 16:57:17.584217:  
2025-10-08 16:57:17.584440: Epoch 151 
2025-10-08 16:57:17.584554: Current learning rate: 0.00282 
2025-10-08 16:59:57.541266: train_loss -0.8964 
2025-10-08 16:59:57.541747: val_loss -0.781 
2025-10-08 16:59:57.541846: Pseudo dice [0.8467] 
2025-10-08 16:59:57.541939: Epoch time: 159.96 s 
2025-10-08 16:59:58.829635:  
2025-10-08 16:59:58.829821: Epoch 152 
2025-10-08 16:59:58.829936: Current learning rate: 0.00277 
2025-10-08 17:02:38.516909: train_loss -0.8893 
2025-10-08 17:02:38.517213: val_loss -0.7814 
2025-10-08 17:02:38.517283: Pseudo dice [0.8455] 
2025-10-08 17:02:38.517361: Epoch time: 159.69 s 
2025-10-08 17:02:39.790808:  
2025-10-08 17:02:39.791024: Epoch 153 
2025-10-08 17:02:39.791115: Current learning rate: 0.00272 
2025-10-08 17:05:19.611891: train_loss -0.892 
2025-10-08 17:05:19.612194: val_loss -0.7728 
2025-10-08 17:05:19.612295: Pseudo dice [0.8398] 
2025-10-08 17:05:19.612450: Epoch time: 159.82 s 
2025-10-08 17:05:20.917220:  
2025-10-08 17:05:20.917494: Epoch 154 
2025-10-08 17:05:20.917681: Current learning rate: 0.00266 
2025-10-08 17:08:00.274269: train_loss -0.8938 
2025-10-08 17:08:00.274561: val_loss -0.7786 
2025-10-08 17:08:00.274628: Pseudo dice [0.8434] 
2025-10-08 17:08:00.274723: Epoch time: 159.36 s 
2025-10-08 17:08:01.594247:  
2025-10-08 17:08:01.594577: Epoch 155 
2025-10-08 17:08:01.594774: Current learning rate: 0.00261 
2025-10-08 17:10:41.004922: train_loss -0.8914 
2025-10-08 17:10:41.005311: val_loss -0.7771 
2025-10-08 17:10:41.005396: Pseudo dice [0.8455] 
2025-10-08 17:10:41.005482: Epoch time: 159.41 s 
2025-10-08 17:10:42.332257:  
2025-10-08 17:10:42.332600: Epoch 156 
2025-10-08 17:10:42.332742: Current learning rate: 0.00256 
2025-10-08 17:13:22.231240: train_loss -0.8918 
2025-10-08 17:13:22.231586: val_loss -0.7775 
2025-10-08 17:13:22.231677: Pseudo dice [0.8415] 
2025-10-08 17:13:22.231771: Epoch time: 159.9 s 
2025-10-08 17:13:23.561447:  
2025-10-08 17:13:23.561639: Epoch 157 
2025-10-08 17:13:23.561768: Current learning rate: 0.00251 
2025-10-08 17:16:03.145833: train_loss -0.8938 
2025-10-08 17:16:03.146135: val_loss -0.778 
2025-10-08 17:16:03.146205: Pseudo dice [0.8441] 
2025-10-08 17:16:03.146287: Epoch time: 159.59 s 
2025-10-08 17:16:04.819710:  
2025-10-08 17:16:04.820100: Epoch 158 
2025-10-08 17:16:04.820233: Current learning rate: 0.00245 
2025-10-08 17:18:44.364405: train_loss -0.894 
2025-10-08 17:18:44.364942: val_loss -0.7841 
2025-10-08 17:18:44.365015: Pseudo dice [0.8477] 
2025-10-08 17:18:44.365096: Epoch time: 159.55 s 
2025-10-08 17:18:45.681698:  
2025-10-08 17:18:45.681909: Epoch 159 
2025-10-08 17:18:45.682030: Current learning rate: 0.0024 
2025-10-08 17:21:25.151749: train_loss -0.8949 
2025-10-08 17:21:25.152056: val_loss -0.7826 
2025-10-08 17:21:25.152148: Pseudo dice [0.8467] 
2025-10-08 17:21:25.152267: Epoch time: 159.47 s 
2025-10-08 17:21:26.448451:  
2025-10-08 17:21:26.448693: Epoch 160 
2025-10-08 17:21:26.448833: Current learning rate: 0.00235 
2025-10-08 17:24:05.884096: train_loss -0.8947 
2025-10-08 17:24:05.884377: val_loss -0.7825 
2025-10-08 17:24:05.884445: Pseudo dice [0.846] 
2025-10-08 17:24:05.884524: Epoch time: 159.44 s 
2025-10-08 17:24:07.228907:  
2025-10-08 17:24:07.229145: Epoch 161 
2025-10-08 17:24:07.229262: Current learning rate: 0.0023 
2025-10-08 17:26:46.640970: train_loss -0.8962 
2025-10-08 17:26:46.641280: val_loss -0.7741 
2025-10-08 17:26:46.641351: Pseudo dice [0.8431] 
2025-10-08 17:26:46.641429: Epoch time: 159.41 s 
2025-10-08 17:26:47.961042:  
2025-10-08 17:26:47.961262: Epoch 162 
2025-10-08 17:26:47.961379: Current learning rate: 0.00224 
2025-10-08 17:29:27.407793: train_loss -0.8972 
2025-10-08 17:29:27.408122: val_loss -0.7825 
2025-10-08 17:29:27.408222: Pseudo dice [0.8474] 
2025-10-08 17:29:27.408306: Epoch time: 159.45 s 
2025-10-08 17:29:28.726430:  
2025-10-08 17:29:28.729405: Epoch 163 
2025-10-08 17:29:28.729536: Current learning rate: 0.00219 
2025-10-08 17:32:08.221879: train_loss -0.8951 
2025-10-08 17:32:08.222185: val_loss -0.7789 
2025-10-08 17:32:08.222254: Pseudo dice [0.8457] 
2025-10-08 17:32:08.222333: Epoch time: 159.5 s 
2025-10-08 17:32:09.610081:  
2025-10-08 17:32:09.610308: Epoch 164 
2025-10-08 17:32:09.610423: Current learning rate: 0.00214 
2025-10-08 17:34:49.168156: train_loss -0.8966 
2025-10-08 17:34:49.168457: val_loss -0.7752 
2025-10-08 17:34:49.168527: Pseudo dice [0.8431] 
2025-10-08 17:34:49.168603: Epoch time: 159.56 s 
2025-10-08 17:34:50.436690:  
2025-10-08 17:34:50.436973: Epoch 165 
2025-10-08 17:34:50.437094: Current learning rate: 0.00208 
2025-10-08 17:37:30.199895: train_loss -0.8966 
2025-10-08 17:37:30.200224: val_loss -0.7813 
2025-10-08 17:37:30.200297: Pseudo dice [0.8458] 
2025-10-08 17:37:30.200382: Epoch time: 159.76 s 
2025-10-08 17:37:31.545164:  
2025-10-08 17:37:31.545366: Epoch 166 
2025-10-08 17:37:31.545466: Current learning rate: 0.00203 
2025-10-08 17:40:11.047106: train_loss -0.8954 
2025-10-08 17:40:11.047417: val_loss -0.7798 
2025-10-08 17:40:11.047485: Pseudo dice [0.8462] 
2025-10-08 17:40:11.047564: Epoch time: 159.5 s 
2025-10-08 17:40:12.342493:  
2025-10-08 17:40:12.342732: Epoch 167 
2025-10-08 17:40:12.342846: Current learning rate: 0.00198 
2025-10-08 17:42:52.116628: train_loss -0.8966 
2025-10-08 17:42:52.116991: val_loss -0.7844 
2025-10-08 17:42:52.117069: Pseudo dice [0.8477] 
2025-10-08 17:42:52.117158: Epoch time: 159.78 s 
2025-10-08 17:42:53.393794:  
2025-10-08 17:42:53.393994: Epoch 168 
2025-10-08 17:42:53.394099: Current learning rate: 0.00192 
2025-10-08 17:45:32.888677: train_loss -0.8985 
2025-10-08 17:45:32.888996: val_loss -0.7775 
2025-10-08 17:45:32.889067: Pseudo dice [0.8434] 
2025-10-08 17:45:32.889143: Epoch time: 159.5 s 
2025-10-08 17:45:34.596495:  
2025-10-08 17:45:34.596770: Epoch 169 
2025-10-08 17:45:34.596890: Current learning rate: 0.00187 
2025-10-08 17:48:14.133161: train_loss -0.9018 
2025-10-08 17:48:14.133551: val_loss -0.778 
2025-10-08 17:48:14.133627: Pseudo dice [0.8442] 
2025-10-08 17:48:14.133719: Epoch time: 159.54 s 
2025-10-08 17:48:15.413675:  
2025-10-08 17:48:15.413863: Epoch 170 
2025-10-08 17:48:15.413960: Current learning rate: 0.00181 
2025-10-08 17:50:54.947002: train_loss -0.8973 
2025-10-08 17:50:54.947327: val_loss -0.7894 
2025-10-08 17:50:54.947393: Pseudo dice [0.8513] 
2025-10-08 17:50:54.947470: Epoch time: 159.53 s 
2025-10-08 17:50:56.220080:  
2025-10-08 17:50:56.220264: Epoch 171 
2025-10-08 17:50:56.220363: Current learning rate: 0.00176 
2025-10-08 17:53:35.767710: train_loss -0.8957 
2025-10-08 17:53:35.767971: val_loss -0.7764 
2025-10-08 17:53:35.768037: Pseudo dice [0.8406] 
2025-10-08 17:53:35.768113: Epoch time: 159.55 s 
2025-10-08 17:53:37.065172:  
2025-10-08 17:53:37.065381: Epoch 172 
2025-10-08 17:53:37.065485: Current learning rate: 0.0017 
2025-10-08 17:56:16.622043: train_loss -0.8972 
2025-10-08 17:56:16.622366: val_loss -0.7723 
2025-10-08 17:56:16.622436: Pseudo dice [0.842] 
2025-10-08 17:56:16.622519: Epoch time: 159.56 s 
2025-10-08 17:56:17.903848:  
2025-10-08 17:56:17.904051: Epoch 173 
2025-10-08 17:56:17.904164: Current learning rate: 0.00165 
2025-10-08 17:58:57.468452: train_loss -0.8997 
2025-10-08 17:58:57.468774: val_loss -0.7859 
2025-10-08 17:58:57.468847: Pseudo dice [0.8487] 
2025-10-08 17:58:57.468924: Epoch time: 159.57 s 
2025-10-08 17:58:58.753395:  
2025-10-08 17:58:58.753596: Epoch 174 
2025-10-08 17:58:58.753726: Current learning rate: 0.00159 
2025-10-08 18:01:38.289366: train_loss -0.8993 
2025-10-08 18:01:38.289732: val_loss -0.7786 
2025-10-08 18:01:38.289810: Pseudo dice [0.8453] 
2025-10-08 18:01:38.289920: Epoch time: 159.54 s 
2025-10-08 18:01:39.567428:  
2025-10-08 18:01:39.567633: Epoch 175 
2025-10-08 18:01:39.567746: Current learning rate: 0.00154 
2025-10-08 18:04:19.184174: train_loss -0.9002 
2025-10-08 18:04:19.184465: val_loss -0.7912 
2025-10-08 18:04:19.184531: Pseudo dice [0.8542] 
2025-10-08 18:04:19.184608: Epoch time: 159.62 s 
2025-10-08 18:04:20.463947:  
2025-10-08 18:04:20.464156: Epoch 176 
2025-10-08 18:04:20.464259: Current learning rate: 0.00148 
2025-10-08 18:06:59.978305: train_loss -0.9011 
2025-10-08 18:06:59.978617: val_loss -0.7747 
2025-10-08 18:06:59.978705: Pseudo dice [0.8397] 
2025-10-08 18:06:59.978785: Epoch time: 159.52 s 
2025-10-08 18:07:01.266321:  
2025-10-08 18:07:01.266507: Epoch 177 
2025-10-08 18:07:01.266617: Current learning rate: 0.00143 
2025-10-08 18:09:40.946490: train_loss -0.8991 
2025-10-08 18:09:40.946781: val_loss -0.778 
2025-10-08 18:09:40.946848: Pseudo dice [0.8431] 
2025-10-08 18:09:40.946929: Epoch time: 159.68 s 
2025-10-08 18:09:42.219308:  
2025-10-08 18:09:42.219497: Epoch 178 
2025-10-08 18:09:42.219598: Current learning rate: 0.00137 
2025-10-08 18:12:21.678793: train_loss -0.9007 
2025-10-08 18:12:21.679163: val_loss -0.7789 
2025-10-08 18:12:21.679236: Pseudo dice [0.8441] 
2025-10-08 18:12:21.679315: Epoch time: 159.46 s 
2025-10-08 18:12:22.965997:  
2025-10-08 18:12:22.966187: Epoch 179 
2025-10-08 18:12:22.966287: Current learning rate: 0.00132 
2025-10-08 18:15:02.456023: train_loss -0.9006 
2025-10-08 18:15:02.456320: val_loss -0.7757 
2025-10-08 18:15:02.456391: Pseudo dice [0.845] 
2025-10-08 18:15:02.456467: Epoch time: 159.49 s 
2025-10-08 18:15:04.157613:  
2025-10-08 18:15:04.157820: Epoch 180 
2025-10-08 18:15:04.157923: Current learning rate: 0.00126 
2025-10-08 18:17:43.735026: train_loss -0.9012 
2025-10-08 18:17:43.735464: val_loss -0.7837 
2025-10-08 18:17:43.735550: Pseudo dice [0.8505] 
2025-10-08 18:17:43.735662: Epoch time: 159.58 s 
2025-10-08 18:17:44.993337:  
2025-10-08 18:17:44.993529: Epoch 181 
2025-10-08 18:17:44.993698: Current learning rate: 0.0012 
2025-10-08 18:20:24.505177: train_loss -0.9031 
2025-10-08 18:20:24.505479: val_loss -0.7842 
2025-10-08 18:20:24.505545: Pseudo dice [0.8496] 
2025-10-08 18:20:24.505622: Epoch time: 159.51 s 
2025-10-08 18:20:25.775402:  
2025-10-08 18:20:25.775613: Epoch 182 
2025-10-08 18:20:25.775730: Current learning rate: 0.00115 
2025-10-08 18:23:05.322814: train_loss -0.9049 
2025-10-08 18:23:05.323139: val_loss -0.7836 
2025-10-08 18:23:05.323218: Pseudo dice [0.8477] 
2025-10-08 18:23:05.323298: Epoch time: 159.55 s 
2025-10-08 18:23:06.587313:  
2025-10-08 18:23:06.587542: Epoch 183 
2025-10-08 18:23:06.587799: Current learning rate: 0.00109 
2025-10-08 18:25:46.283131: train_loss -0.8991 
2025-10-08 18:25:46.283448: val_loss -0.7768 
2025-10-08 18:25:46.283517: Pseudo dice [0.8459] 
2025-10-08 18:25:46.283593: Epoch time: 159.7 s 
2025-10-08 18:25:47.568210:  
2025-10-08 18:25:47.568429: Epoch 184 
2025-10-08 18:25:47.568533: Current learning rate: 0.00103 
2025-10-08 18:28:27.064436: train_loss -0.9028 
2025-10-08 18:28:27.064798: val_loss -0.7709 
2025-10-08 18:28:27.064876: Pseudo dice [0.8401] 
2025-10-08 18:28:27.064965: Epoch time: 159.5 s 
2025-10-08 18:28:28.355369:  
2025-10-08 18:28:28.355572: Epoch 185 
2025-10-08 18:28:28.355684: Current learning rate: 0.00097 
2025-10-08 18:31:08.038883: train_loss -0.9006 
2025-10-08 18:31:08.039243: val_loss -0.7814 
2025-10-08 18:31:08.039373: Pseudo dice [0.8476] 
2025-10-08 18:31:08.039521: Epoch time: 159.68 s 
2025-10-08 18:31:09.315841:  
2025-10-08 18:31:09.316113: Epoch 186 
2025-10-08 18:31:09.316250: Current learning rate: 0.00091 
2025-10-08 18:33:49.350874: train_loss -0.9 
2025-10-08 18:33:49.351186: val_loss -0.7845 
2025-10-08 18:33:49.351258: Pseudo dice [0.8513] 
2025-10-08 18:33:49.351333: Epoch time: 160.04 s 
2025-10-08 18:33:50.623831:  
2025-10-08 18:33:50.624120: Epoch 187 
2025-10-08 18:33:50.624262: Current learning rate: 0.00085 
2025-10-08 18:36:30.346440: train_loss -0.9009 
2025-10-08 18:36:30.346806: val_loss -0.779 
2025-10-08 18:36:30.346876: Pseudo dice [0.8445] 
2025-10-08 18:36:30.346956: Epoch time: 159.72 s 
2025-10-08 18:36:31.624041:  
2025-10-08 18:36:31.624312: Epoch 188 
2025-10-08 18:36:31.624423: Current learning rate: 0.00079 
2025-10-08 18:39:11.090308: train_loss -0.9024 
2025-10-08 18:39:11.090631: val_loss -0.7759 
2025-10-08 18:39:11.090740: Pseudo dice [0.8429] 
2025-10-08 18:39:11.090853: Epoch time: 159.47 s 
2025-10-08 18:39:12.384935:  
2025-10-08 18:39:12.385174: Epoch 189 
2025-10-08 18:39:12.385303: Current learning rate: 0.00074 
2025-10-08 18:41:52.087576: train_loss -0.9048 
2025-10-08 18:41:52.087915: val_loss -0.7659 
2025-10-08 18:41:52.087984: Pseudo dice [0.8341] 
2025-10-08 18:41:52.088060: Epoch time: 159.7 s 
2025-10-08 18:41:53.394232:  
2025-10-08 18:41:53.394427: Epoch 190 
2025-10-08 18:41:53.394536: Current learning rate: 0.00067 
2025-10-08 18:44:32.879601: train_loss -0.9021 
2025-10-08 18:44:32.879971: val_loss -0.7798 
2025-10-08 18:44:32.880038: Pseudo dice [0.8473] 
2025-10-08 18:44:32.880114: Epoch time: 159.49 s 
2025-10-08 18:44:34.622852:  
2025-10-08 18:44:34.623028: Epoch 191 
2025-10-08 18:44:34.623166: Current learning rate: 0.00061 
2025-10-08 18:47:14.096453: train_loss -0.9034 
2025-10-08 18:47:14.096778: val_loss -0.7737 
2025-10-08 18:47:14.096858: Pseudo dice [0.841] 
2025-10-08 18:47:14.096936: Epoch time: 159.47 s 
2025-10-08 18:47:15.406616:  
2025-10-08 18:47:15.406914: Epoch 192 
2025-10-08 18:47:15.407036: Current learning rate: 0.00055 
2025-10-08 18:49:54.972099: train_loss -0.9046 
2025-10-08 18:49:54.972404: val_loss -0.7851 
2025-10-08 18:49:54.972471: Pseudo dice [0.85] 
2025-10-08 18:49:54.972551: Epoch time: 159.57 s 
2025-10-08 18:49:56.283122:  
2025-10-08 18:49:56.283311: Epoch 193 
2025-10-08 18:49:56.283411: Current learning rate: 0.00049 
2025-10-08 18:52:35.908352: train_loss -0.904 
2025-10-08 18:52:35.908709: val_loss -0.774 
2025-10-08 18:52:35.908783: Pseudo dice [0.8457] 
2025-10-08 18:52:35.908861: Epoch time: 159.63 s 
2025-10-08 18:52:37.235326:  
2025-10-08 18:52:37.235566: Epoch 194 
2025-10-08 18:52:37.235710: Current learning rate: 0.00043 
2025-10-08 18:55:17.019698: train_loss -0.9043 
2025-10-08 18:55:17.020004: val_loss -0.7769 
2025-10-08 18:55:17.020086: Pseudo dice [0.8449] 
2025-10-08 18:55:17.020180: Epoch time: 159.79 s 
2025-10-08 18:55:18.363434:  
2025-10-08 18:55:18.363667: Epoch 195 
2025-10-08 18:55:18.364026: Current learning rate: 0.00036 
2025-10-08 18:57:58.053540: train_loss -0.9034 
2025-10-08 18:57:58.053836: val_loss -0.7667 
2025-10-08 18:57:58.053904: Pseudo dice [0.8353] 
2025-10-08 18:57:58.053980: Epoch time: 159.69 s 
2025-10-08 18:57:59.380250:  
2025-10-08 18:57:59.380496: Epoch 196 
2025-10-08 18:57:59.380602: Current learning rate: 0.0003 
2025-10-08 19:00:39.131166: train_loss -0.9082 
2025-10-08 19:00:39.131484: val_loss -0.7691 
2025-10-08 19:00:39.131551: Pseudo dice [0.8388] 
2025-10-08 19:00:39.131632: Epoch time: 159.75 s 
2025-10-08 19:00:40.450740:  
2025-10-08 19:00:40.451001: Epoch 197 
2025-10-08 19:00:40.451106: Current learning rate: 0.00023 
2025-10-08 19:03:20.397526: train_loss -0.9035 
2025-10-08 19:03:20.397891: val_loss -0.7756 
2025-10-08 19:03:20.397961: Pseudo dice [0.845] 
2025-10-08 19:03:20.398042: Epoch time: 159.95 s 
2025-10-08 19:03:21.710256:  
2025-10-08 19:03:21.710523: Epoch 198 
2025-10-08 19:03:21.710644: Current learning rate: 0.00016 
2025-10-08 19:06:01.373090: train_loss -0.9048 
2025-10-08 19:06:01.373426: val_loss -0.7844 
2025-10-08 19:06:01.373494: Pseudo dice [0.8494] 
2025-10-08 19:06:01.373575: Epoch time: 159.66 s 
2025-10-08 19:06:02.695279:  
2025-10-08 19:06:02.695500: Epoch 199 
2025-10-08 19:06:02.695616: Current learning rate: 8e-05 
2025-10-08 19:08:42.545058: train_loss -0.9029 
2025-10-08 19:08:42.545399: val_loss -0.7736 
2025-10-08 19:08:42.545467: Pseudo dice [0.8407] 
2025-10-08 19:08:42.545544: Epoch time: 159.85 s 
2025-10-08 19:08:44.614203: Training done. 
2025-10-08 19:08:44.824043: Using splits from existing split file: /home/rnga/tsdehaan/my-scratch/Data_nnUNet/nnUnet_preprocessed/Dataset001_AAA/splits_final.json 
2025-10-08 19:08:44.826170: The split file contains 5 splits. 
2025-10-08 19:08:44.826262: Desired fold for training: 2 
2025-10-08 19:08:44.826313: This split has 62 training and 15 validation cases. 
2025-10-08 19:08:44.826579: predicting IVIM_024 
2025-10-08 19:08:44.828615: IVIM_024, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-08 19:08:53.571420: predicting IVIM_035 
2025-10-08 19:08:53.582200: IVIM_035, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-08 19:08:54.107751: predicting IVIM_043 
2025-10-08 19:08:54.109128: IVIM_043, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-08 19:08:54.625082: predicting IVIM_044 
2025-10-08 19:08:54.630353: IVIM_044, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-08 19:08:55.150788: predicting IVIM_047 
2025-10-08 19:08:55.152280: IVIM_047, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-08 19:08:55.652233: predicting IVIM_055 
2025-10-08 19:08:55.653621: IVIM_055, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-08 19:09:00.801727: predicting IVIM_091 
2025-10-08 19:09:00.803180: IVIM_091, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-08 19:09:01.318776: predicting IVIM_093 
2025-10-08 19:09:01.320550: IVIM_093, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-08 19:09:01.822985: predicting IVIM_116 
2025-10-08 19:09:01.824621: IVIM_116, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-08 19:09:02.326624: predicting IVIM_118 
2025-10-08 19:09:02.328344: IVIM_118, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-08 19:09:02.829812: predicting IVIM_123 
2025-10-08 19:09:02.831682: IVIM_123, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-08 19:09:03.336516: predicting IVIM_138 
2025-10-08 19:09:03.338959: IVIM_138, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-08 19:09:03.842035: predicting IVIM_146 
2025-10-08 19:09:03.844350: IVIM_146, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-08 19:09:04.348243: predicting IVIM_152 
2025-10-08 19:09:04.349921: IVIM_152, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-08 19:09:04.850930: predicting IVIM_157 
2025-10-08 19:09:04.852933: IVIM_157, shape torch.Size([1, 27, 168, 256]), rank 0 
2025-10-08 19:09:17.370644: Validation complete 
2025-10-08 19:09:17.370772: Mean Validation Dice:  0.8474114075241421 
